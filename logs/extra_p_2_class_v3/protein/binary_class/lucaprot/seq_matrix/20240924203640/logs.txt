{"adam_epsilon": 1e-08, "asl_gamma_neg": 4.0, "asl_gamma_pos": 1.0, "batch_with_seq_ids": false, "best_metric_type": "f1", "beta1": 0.9, "beta2": 0.99, "buffer_size": 1024, "cache_dir": null, "classifier_activate_func": "gelu", "classifier_size": 128, "codes_file": "../subword/extra_p/extra_p_50_codes_20000.txt", "config_path": "../config/lucaprot/lucaprot_config.json", "cross_atten": false, "dataset_name": "extra_p_2_class_v3", "dataset_type": "protein", "delete_old": false, "dev_data_dir": "../dataset/extra_p_2_class_v3/protein/binary_class/dev/", "device": "cuda", "do_eval": true, "do_lower_case": false, "do_metrics": true, "do_predict": true, "do_train": true, "dropout_prob": 0.1, "early_stop_epoch": -1, "emb_activate_func": "gelu", "embedding_complete": true, "embedding_complete_seg_overlap": true, "embedding_input_size": 2560, "embedding_input_size_a": null, "embedding_input_size_b": null, "eval_all_checkpoints": false, "evaluate_during_training": true, "fc_activate_func": "gelu", "focal_loss_alpha": 0.7, "focal_loss_gamma": 2.0, "focal_loss_reduce": false, "fp16": false, "fp16_opt_level": "O1", "fusion_type": "concat", "gradient_accumulation_steps": 2, "hidden_size": 1024, "ignore_index": -100, "input_mode": "single", "input_type": "seq_matrix", "intermediate_size": 4096, "label_filepath": "../dataset/extra_p_2_class_v3/protein/binary_class/label.txt", "label_size": 2, "label_type": "extra_p_2_class", "learning_rate": 0.0001, "llm_dir": null, "llm_dirpath": null, "llm_step": null, "llm_task_level": "token_level,span_level,seq_level,structure_level", "llm_time_str": null, "llm_type": "esm", "llm_version": "esm2", "local_rank": -1, "log_dir": "../logs/extra_p_2_class_v3/protein/binary_class/lucaprot/seq_matrix/20240924203640", "logging_steps": 4000, "loss_reduction": "mean", "loss_type": "bce", "lr_decay_rate": 0.95, "lr_update_strategy": "step", "matrix_add_special_token": true, "matrix_append_eos": true, "matrix_dirpath": "/mnt/sanyuan.hy/workspace/matrices/lucapcycle_positives/protein/esm/esm2/3B/#/mnt/sanyuan.hy/workspace/matrices/lucapcycle_negatives/protein/esm/esm2/3B/", "matrix_embedding_exists": true, "matrix_encoder": false, "matrix_encoder_act": false, "matrix_fc_size": "256", "matrix_max_length": 3072, "matrix_max_length_a": null, "matrix_max_length_b": null, "matrix_pooling_type": "value_attention", "matrix_prepend_bos": true, "max_grad_norm": 1.0, "max_steps": -1, "model_dirpath": null, "model_type": "lucaprot", "n_gpu": 1, "no_cuda": false, "no_position_embeddings": true, "no_token_embeddings": false, "no_token_type_embeddings": true, "non_ignore": false, "not_matrix_append_eos": false, "not_matrix_encoder_shared": false, "not_matrix_prepend_bos": false, "not_seq_append_eos": false, "not_seq_encoder_shared": false, "not_seq_prepend_bos": false, "num_attention_heads": 8, "num_hidden_layers": 4, "num_train_epochs": 50, "output_dir": "../models/extra_p_2_class_v3/protein/binary_class/lucaprot/seq_matrix/20240924203640", "output_mode": "binary_class", "overwrite_cache": false, "overwrite_output_dir": true, "per_gpu_eval_batch_size": 8, "per_gpu_train_batch_size": 8, "pos_weight": 4.0, "position_embedding_type": "absolute", "save_all": true, "save_steps": -1, "seed": 1221, "self_atten": false, "seq_append_eos": true, "seq_fc_size": "256", "seq_max_length": 3072, "seq_max_length_a": null, "seq_max_length_b": null, "seq_pooling_type": "value_attention", "seq_prepend_bos": true, "seq_subword": true, "seq_vocab_path": "../vocab/extra_p/extra_p_50_subword_vocab_20000.txt", "sigmoid": true, "task_level_type": "seq_level", "task_type": "binary_class", "tb_log_dir": "../tb-logs/extra_p_2_class_v3/protein/binary_class/lucaprot/seq_matrix/20240924203640", "test_data_dir": "../dataset/extra_p_2_class_v3\/protein/binary_class/test/", "time_str": "20240924203652", "train_data_dir": "../dataset/extra_p_2_class_v3/protein/binary_class/train/", "trunc_type": "right", "use_rotary_position_embeddings": true, "vector_dirpath": "/mnt/sanyuan.hy/workspace/vectors/lucapcycle_positives/protein/esm/esm2/3B/", "vector_fc_size": "256", "vocab_size": 19988, "warmup_steps": 8000, "weight": null, "weight_decay": 0.01, "worker_num": 1}
##################################################
n_gpu: 1
##################################################
Inputs:
Input Name List: protein,seq,embedding_matrix
##################################################
Encoder Config:
 {'llm_type': 'esm', 'llm_version': 'esm2', 'llm_step': None, 'llm_dirpath': None, 'input_type': 'seq_matrix', 'trunc_type': 'right', 'seq_max_length': 3072, 'vector_dirpath': '/mnt/sanyuan.hy/workspace/vectors/lucapcycle_positives/protein/esm/esm2/3B/', 'matrix_dirpath': '/mnt/sanyuan.hy/workspace/matrices/lucapcycle_positives/protein/esm/esm2/3B/#/mnt/sanyuan.hy/workspace/matrices/lucapcycle_negatives/protein/esm/esm2/3B/', 'matrix_add_special_token': True, 'prepend_bos': True, 'append_eos': True, 'local_rank': -1, 'embedding_complete': True, 'embedding_complete_seg_overlap': True, 'matrix_embedding_exists': True}
##################################################
Model Config:
 LucaConfig {
  "activate_func": "tanh",
  "alphabet": "gene_prot",
  "attention_probs_dropout_prob": 0.1,
  "classifier_activate_func": "gelu",
  "classifier_dropout_prob": 0.1,
  "classifier_size": 128,
  "cls_token_id": 2,
  "cross_atten": false,
  "directionality": "bidi",
  "emb_activate_func": "gelu",
  "embedding_fc_size": [
    256
  ],
  "embedding_input_size": 2560,
  "embedding_pooling_type": "value_attention",
  "embedding_weight": null,
  "fc_activate_func": "gelu",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "ignore_index": -100,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "kernel_size": 7,
  "layer_norm_eps": 1e-12,
  "loss_reduction": "mean",
  "matrix_fc_size": [
    256
  ],
  "matrix_max_length": 3072,
  "matrix_pooling_type": "value_attention",
  "max_position_embeddings": 3074,
  "no_position_embeddings": true,
  "no_token_embeddings": false,
  "no_token_type_embeddings": true,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "pooler_fc_size": 1280,
  "pooler_num_attention_heads": 4,
  "pooler_num_fc_layers": 1,
  "pooler_size_per_head": 1280,
  "pooler_type": "first_token_transform",
  "pos_weight": 4.0,
  "position_embedding_type": "absolute",
  "self_atten": false,
  "sep_token_id": 3,
  "seq_fc_size": [
    256
  ],
  "seq_max_length": 3072,
  "seq_pooling_type": "value_attention",
  "seq_weight": null,
  "struct_alpha": 0.2,
  "struct_embed_size": 1280,
  "struct_fc_size": null,
  "struct_hidden_size": null,
  "struct_nb_heads": 4,
  "struct_output_size": null,
  "struct_vocab_size": null,
  "struct_weight": null,
  "token_dropout": true,
  "transformers_version": "4.29.0",
  "type_vocab_size": 2,
  "use_luca_layer_norm_v2": true,
  "use_rotary_position_embeddings": true,
  "vector_fc_size": [
    256
  ],
  "vocab_size": 19988,
  "weight": null
}

##################################################
Mode Architecture:
 LucaProt(
  (seq_encoder): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(19988, 1024, padding_idx=0)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (RoPE): RotaryPositionEmbedding()
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (RoPE): RotaryPositionEmbedding()
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (RoPE): RotaryPositionEmbedding()
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (RoPE): RotaryPositionEmbedding()
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (seq_pooler): GlobalMaskValueAttentionPooling1D (1024 -> 1024)
  (seq_linear): ModuleList(
    (0): Linear(in_features=1024, out_features=256, bias=True)
    (1): Tanh()
  )
  (embedding_pooler): GlobalMaskValueAttentionPooling1D (2560 -> 2560)
  (embedding_linear): ModuleList(
    (0): Linear(in_features=2560, out_features=256, bias=True)
    (1): Tanh()
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=512, out_features=1, bias=True)
  (output): Sigmoid()
  (loss_fct): BCEWithLogitsLoss()
)
##################################################
Model parameters: 94579713 
##################################################
{"total_num": "90.200000M", "total_size": "360.790000MB", "param_sum": "90.200000M", "param_size": "360.790000MB", "buffer_sum": "0.000000M", "buffer_size": "0.000000MB", "trainable_num": "90.200000M", "trainable_size": "360.790000MB"}
##################################################