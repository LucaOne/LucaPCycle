{"adam_epsilon": 1e-08, "asl_gamma_neg": 4.0, "asl_gamma_pos": 1.0, "batch_with_seq_ids": false, "best_metric_type": "f1", "beta1": 0.9, "beta2": 0.99, "buffer_size": 10240, "cache_dir": null, "classifier_activate_func": "gelu", "classifier_size": 256, "codes_file": "../subword/extra_p/extra_p_50_codes_20000.txt", "config_path": "../config/lucaprot/lucaprot_config.json", "cross_atten": false, "dataset_name": "extra_p_31_class_v3", "dataset_type": "protein", "delete_old": false, "dev_data_dir": "../dataset/extra_p_31_class_v3/protein/multi_class/dev/", "device": "cuda", "do_eval": true, "do_lower_case": false, "do_metrics": true, "do_predict": true, "do_train": true, "dropout_prob": 0.1, "early_stop_epoch": -1, "emb_activate_func": "gelu", "embedding_complete": true, "embedding_complete_seg_overlap": true, "embedding_input_size": 2560, "embedding_input_size_a": null, "embedding_input_size_b": null, "eval_all_checkpoints": false, "evaluate_during_training": true, "fc_activate_func": "gelu", "focal_loss_alpha": 0.7, "focal_loss_gamma": 2.0, "focal_loss_reduce": false, "fp16": false, "fp16_opt_level": "O1", "fusion_type": "concat", "gradient_accumulation_steps": 32, "hidden_size": 1024, "ignore_index": -100, "input_mode": "single", "input_type": "seq_matrix", "intermediate_size": 4096, "label_filepath": "../dataset/extra_p_31_class_v3/protein/multi_class/label.txt", "label_size": 31, "label_type": "extra_p_31_class", "learning_rate": 0.0002, "llm_dir": null, "llm_dirpath": null, "llm_step": null, "llm_task_level": "token_level,span_level,seq_level,structure_level", "llm_time_str": null, "llm_type": "esm", "llm_version": "esm2", "local_rank": -1, "log_dir": "../logs/extra_p_31_class_v3/protein/multi_class/lucaprot/seq_matrix/20240923094428", "logging_steps": 1000, "loss_reduction": "mean", "loss_type": "cce", "lr_decay_rate": 0.95, "lr_update_strategy": "step", "matrix_add_special_token": true, "matrix_append_eos": true, "matrix_dirpath": "/mnt/sanyuan.hy/workspace/matrices/lucapcycle_positives/protein/esm/esm2/3B/#/mnt/sanyuan.hy/workspace/matrices/lucapcycle_negatives/protein/esm/esm2/3B/", "matrix_embedding_exists": true, "matrix_encoder": false, "matrix_encoder_act": false, "matrix_fc_size": "256", "matrix_max_length": 3072, "matrix_max_length_a": null, "matrix_max_length_b": null, "matrix_pooling_type": "value_attention", "matrix_prepend_bos": true, "max_grad_norm": 1.0, "max_steps": -1, "model_dirpath": null, "model_type": "lucaprot", "n_gpu": 1, "no_cuda": false, "no_position_embeddings": true, "no_token_embeddings": false, "no_token_type_embeddings": true, "non_ignore": false, "not_matrix_append_eos": false, "not_matrix_encoder_shared": false, "not_matrix_prepend_bos": false, "not_seq_append_eos": false, "not_seq_encoder_shared": false, "not_seq_prepend_bos": false, "num_attention_heads": 8, "num_hidden_layers": 4, "num_train_epochs": 50, "output_dir": "../models/extra_p_31_class_v3/protein/multi_class/lucaprot/seq_matrix/20240923094428", "output_mode": "multi_class", "overwrite_cache": false, "overwrite_output_dir": true, "per_gpu_eval_batch_size": 1, "per_gpu_train_batch_size": 1, "pos_weight": null, "position_embedding_type": "absolute", "save_all": true, "save_steps": -1, "seed": 1221, "self_atten": false, "seq_append_eos": true, "seq_fc_size": "256", "seq_max_length": 3072, "seq_max_length_a": null, "seq_max_length_b": null, "seq_pooling_type": "value_attention", "seq_prepend_bos": true, "seq_subword": true, "seq_vocab_path": "../vocab/extra_p/extra_p_50_subword_vocab_20000.txt", "sigmoid": false, "task_level_type": "seq_level", "task_type": "multi_class", "tb_log_dir": "../tb-logs/extra_p_31_class_v3/protein/multi_class/lucaprot/seq_matrix/20240923094428", "test_data_dir": "../dataset/extra_p_31_class_v3/protein/multi_class/test/", "time_str": "20240923094435", "train_data_dir": "../dataset/extra_p_31_class_v3/protein/multi_class/train/", "trunc_type": "right", "use_rotary_position_embeddings": true, "vector_dirpath": "/mnt/sanyuan.hy/workspace/vectors/lucapcycle_positives/protein/esm/esm2/3B/", "vector_fc_size": "256", "vocab_size": 19988, "warmup_steps": 2000, "weight": [13.0, 212.0, 7.0, 8.0, 21.0, 31.0, 3.0, 10.0, 4.0, 27.0, 28.0, 3.0, 363.0, 589.0, 6.0, 2.0, 20.0, 1.0, 4.0, 37.0, 32.0, 179.0, 17.0, 30.0, 14.0, 110.0, 1.2, 4.0, 12.0, 11.0, 64.0], "weight_decay": 0.01, "worker_num": 1}
##################################################
n_gpu: 1
##################################################
Inputs:
Input Name List: protein,seq,embedding_matrix
##################################################
Encoder Config:
 {'llm_type': 'esm', 'llm_version': 'esm2', 'llm_step': None, 'llm_dirpath': None, 'input_type': 'seq_matrix', 'trunc_type': 'right', 'seq_max_length': 3072, 'vector_dirpath': '/mnt/sanyuan.hy/workspace/vectors/lucapcycle_positives/protein/esm/esm2/3B/', 'matrix_dirpath': '/mnt/sanyuan.hy/workspace/matrices/lucapcycle_positives/protein/esm/esm2/3B/#/mnt/sanyuan.hy/workspace/matrices/lucapcycle_negatives/protein/esm/esm2/3B/', 'matrix_add_special_token': True, 'prepend_bos': True, 'append_eos': True, 'local_rank': -1, 'embedding_complete': True, 'embedding_complete_seg_overlap': True, 'matrix_embedding_exists': True}
##################################################
Model Config:
 LucaConfig {
  "activate_func": "tanh",
  "alphabet": "gene_prot",
  "attention_probs_dropout_prob": 0.1,
  "classifier_activate_func": "gelu",
  "classifier_dropout_prob": 0.1,
  "classifier_size": 256,
  "cls_token_id": 2,
  "cross_atten": false,
  "directionality": "bidi",
  "emb_activate_func": "gelu",
  "embedding_fc_size": [
    256
  ],
  "embedding_input_size": 2560,
  "embedding_pooling_type": "value_attention",
  "embedding_weight": null,
  "fc_activate_func": "gelu",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {},
  "ignore_index": -100,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "kernel_size": 7,
  "label2id": {},
  "layer_norm_eps": 1e-12,
  "loss_reduction": "mean",
  "matrix_fc_size": [
    256
  ],
  "matrix_max_length": 3072,
  "matrix_pooling_type": "value_attention",
  "max_position_embeddings": 3074,
  "no_position_embeddings": true,
  "no_token_embeddings": false,
  "no_token_type_embeddings": true,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "pooler_fc_size": 1280,
  "pooler_num_attention_heads": 4,
  "pooler_num_fc_layers": 1,
  "pooler_size_per_head": 1280,
  "pooler_type": "first_token_transform",
  "pos_weight": 40,
  "position_embedding_type": "absolute",
  "self_atten": false,
  "sep_token_id": 3,
  "seq_fc_size": [
    256
  ],
  "seq_max_length": 3072,
  "seq_pooling_type": "value_attention",
  "seq_weight": null,
  "struct_alpha": 0.2,
  "struct_embed_size": 1280,
  "struct_fc_size": null,
  "struct_hidden_size": null,
  "struct_nb_heads": 4,
  "struct_output_size": null,
  "struct_vocab_size": null,
  "struct_weight": null,
  "token_dropout": true,
  "transformers_version": "4.29.0",
  "type_vocab_size": 2,
  "use_luca_layer_norm_v2": true,
  "use_rotary_position_embeddings": true,
  "vector_fc_size": [
    256
  ],
  "vocab_size": 19988,
  "weight": [
    13.0,
    212.0,
    7.0,
    8.0,
    21.0,
    31.0,
    3.0,
    10.0,
    4.0,
    27.0,
    28.0,
    3.0,
    363.0,
    589.0,
    6.0,
    2.0,
    20.0,
    1.0,
    4.0,
    37.0,
    32.0,
    179.0,
    17.0,
    30.0,
    14.0,
    110.0,
    1.2,
    4.0,
    12.0,
    11.0,
    64.0
  ]
}

##################################################
Mode Architecture:
 LucaProt(
  (seq_encoder): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(19988, 1024, padding_idx=0)
      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (RoPE): RotaryPositionEmbedding()
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (RoPE): RotaryPositionEmbedding()
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (RoPE): RotaryPositionEmbedding()
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (RoPE): RotaryPositionEmbedding()
              (query): Linear(in_features=1024, out_features=1024, bias=True)
              (key): Linear(in_features=1024, out_features=1024, bias=True)
              (value): Linear(in_features=1024, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=1024, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=4096, out_features=1024, bias=True)
            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (seq_pooler): GlobalMaskValueAttentionPooling1D (1024 -> 1024)
  (seq_linear): ModuleList(
    (0): Linear(in_features=1024, out_features=256, bias=True)
    (1): Tanh()
  )
  (embedding_pooler): GlobalMaskValueAttentionPooling1D (2560 -> 2560)
  (embedding_linear): ModuleList(
    (0): Linear(in_features=2560, out_features=256, bias=True)
    (1): Tanh()
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=512, out_features=31, bias=True)
  (output): Softmax(dim=-1)
  (loss_fct): CrossEntropyLoss()
)
##################################################
Model parameters: 94595103 
##################################################
{"total_num": "90.210000M", "total_size": "360.850000MB", "param_sum": "90.210000M", "param_size": "360.850000MB", "buffer_sum": "0.000000M", "buffer_size": "0.000000MB", "trainable_num": "90.210000M", "trainable_size": "360.850000MB"}
##################################################
Train dataset len: 171385, batch num: 171385
***** Running training *****
Train Dataset Num examples = 171385
Train Dataset Num Epochs = 50
Logging Steps = 1000
Saving Steps = -1
Train Dataset Instantaneous batch size per GPU = 1
Train Dataset Total train batch size (w. parallel, distributed & accumulation) = 32
Train Dataset Gradient Accumulation steps = 32
Train Dataset Total optimization steps = 267837
##################################################
Training, Epoch: 0001, Batch: 004000, Sample Num: 4000, Cur Loss: 0.09358420, Cur Avg Loss: 0.09483497, Log Avg loss: 0.09483497, Global Avg Loss: 0.09483497, Time: 0.0179
Steps: 4000, Updated lr: 0.000013
Training, Epoch: 0001, Batch: 008000, Sample Num: 8000, Cur Loss: 0.07695135, Cur Avg Loss: 0.08781106, Log Avg loss: 0.08078714, Global Avg Loss: 0.08781106, Time: 0.0137
Steps: 8000, Updated lr: 0.000025
Training, Epoch: 0001, Batch: 012000, Sample Num: 12000, Cur Loss: 0.04943868, Cur Avg Loss: 0.07501475, Log Avg loss: 0.04942214, Global Avg Loss: 0.07501475, Time: 0.0136
Steps: 12000, Updated lr: 0.000038
Training, Epoch: 0001, Batch: 016000, Sample Num: 16000, Cur Loss: 0.00619472, Cur Avg Loss: 0.06320253, Log Avg loss: 0.02776586, Global Avg Loss: 0.06320253, Time: 0.0116
Steps: 16000, Updated lr: 0.000050
Training, Epoch: 0001, Batch: 020000, Sample Num: 20000, Cur Loss: 0.00051344, Cur Avg Loss: 0.05382168, Log Avg loss: 0.01629829, Global Avg Loss: 0.05382168, Time: 0.0115
Steps: 20000, Updated lr: 0.000063
Training, Epoch: 0001, Batch: 024000, Sample Num: 24000, Cur Loss: 0.00475705, Cur Avg Loss: 0.04666146, Log Avg loss: 0.01086034, Global Avg Loss: 0.04666146, Time: 0.0128
Steps: 24000, Updated lr: 0.000075
Training, Epoch: 0001, Batch: 028000, Sample Num: 28000, Cur Loss: 0.00038729, Cur Avg Loss: 0.04118711, Log Avg loss: 0.00834100, Global Avg Loss: 0.04118711, Time: 0.0135
Steps: 28000, Updated lr: 0.000087
Training, Epoch: 0001, Batch: 032000, Sample Num: 32000, Cur Loss: 0.00053409, Cur Avg Loss: 0.03693416, Log Avg loss: 0.00716349, Global Avg Loss: 0.03693416, Time: 0.0117
Steps: 32000, Updated lr: 0.000100
Training, Epoch: 0001, Batch: 036000, Sample Num: 36000, Cur Loss: 0.00036806, Cur Avg Loss: 0.03359913, Log Avg loss: 0.00691892, Global Avg Loss: 0.03359913, Time: 0.0117
Steps: 36000, Updated lr: 0.000113
Training, Epoch: 0001, Batch: 040000, Sample Num: 40000, Cur Loss: 0.00009533, Cur Avg Loss: 0.03085282, Log Avg loss: 0.00613600, Global Avg Loss: 0.03085282, Time: 0.0144
Steps: 40000, Updated lr: 0.000125
Training, Epoch: 0001, Batch: 044000, Sample Num: 44000, Cur Loss: 0.03516041, Cur Avg Loss: 0.02862568, Log Avg loss: 0.00635432, Global Avg Loss: 0.02862568, Time: 0.0223
Steps: 44000, Updated lr: 0.000138
Training, Epoch: 0001, Batch: 048000, Sample Num: 48000, Cur Loss: 0.02317521, Cur Avg Loss: 0.02676424, Log Avg loss: 0.00628834, Global Avg Loss: 0.02676424, Time: 0.0116
Steps: 48000, Updated lr: 0.000150
Training, Epoch: 0001, Batch: 052000, Sample Num: 52000, Cur Loss: 0.00309918, Cur Avg Loss: 0.02512596, Log Avg loss: 0.00546672, Global Avg Loss: 0.02512596, Time: 0.0118
Steps: 52000, Updated lr: 0.000163
Training, Epoch: 0001, Batch: 056000, Sample Num: 56000, Cur Loss: 0.00020339, Cur Avg Loss: 0.02374601, Log Avg loss: 0.00580653, Global Avg Loss: 0.02374601, Time: 0.0145
Steps: 56000, Updated lr: 0.000175
Training, Epoch: 0001, Batch: 060000, Sample Num: 60000, Cur Loss: 0.00005938, Cur Avg Loss: 0.02254507, Log Avg loss: 0.00573198, Global Avg Loss: 0.02254507, Time: 0.0128
Steps: 60000, Updated lr: 0.000188
Training, Epoch: 0001, Batch: 064000, Sample Num: 64000, Cur Loss: 0.00007606, Cur Avg Loss: 0.02147705, Log Avg loss: 0.00545675, Global Avg Loss: 0.02147705, Time: 0.0137
Steps: 64000, Updated lr: 0.000200
Training, Epoch: 0001, Batch: 068000, Sample Num: 68000, Cur Loss: 0.00002011, Cur Avg Loss: 0.02053236, Log Avg loss: 0.00541739, Global Avg Loss: 0.02053236, Time: 0.0160
Steps: 68000, Updated lr: 0.000200
Training, Epoch: 0001, Batch: 072000, Sample Num: 72000, Cur Loss: 0.00000678, Cur Avg Loss: 0.01968788, Log Avg loss: 0.00533168, Global Avg Loss: 0.01968788, Time: 0.0249
Steps: 72000, Updated lr: 0.000200
Training, Epoch: 0001, Batch: 076000, Sample Num: 76000, Cur Loss: 0.00119009, Cur Avg Loss: 0.01893106, Log Avg loss: 0.00530818, Global Avg Loss: 0.01893106, Time: 0.0121
Steps: 76000, Updated lr: 0.000200
Training, Epoch: 0001, Batch: 080000, Sample Num: 80000, Cur Loss: 0.00000486, Cur Avg Loss: 0.01824445, Log Avg loss: 0.00519887, Global Avg Loss: 0.01824445, Time: 0.0123
Steps: 80000, Updated lr: 0.000200
Training, Epoch: 0001, Batch: 084000, Sample Num: 84000, Cur Loss: 0.00000108, Cur Avg Loss: 0.01759492, Log Avg loss: 0.00460438, Global Avg Loss: 0.01759492, Time: 0.0115
Steps: 84000, Updated lr: 0.000200
Training, Epoch: 0001, Batch: 088000, Sample Num: 88000, Cur Loss: 0.00002313, Cur Avg Loss: 0.01703525, Log Avg loss: 0.00528228, Global Avg Loss: 0.01703525, Time: 0.0123
Steps: 88000, Updated lr: 0.000199
Training, Epoch: 0001, Batch: 092000, Sample Num: 92000, Cur Loss: 0.00000090, Cur Avg Loss: 0.01650717, Log Avg loss: 0.00488923, Global Avg Loss: 0.01650717, Time: 0.0376
Steps: 92000, Updated lr: 0.000199
Training, Epoch: 0001, Batch: 096000, Sample Num: 96000, Cur Loss: 0.00000060, Cur Avg Loss: 0.01603988, Log Avg loss: 0.00529220, Global Avg Loss: 0.01603988, Time: 0.0117
Steps: 96000, Updated lr: 0.000199
Training, Epoch: 0001, Batch: 100000, Sample Num: 100000, Cur Loss: 0.00000235, Cur Avg Loss: 0.01560197, Log Avg loss: 0.00509235, Global Avg Loss: 0.01560197, Time: 0.0156
Steps: 100000, Updated lr: 0.000199
Training, Epoch: 0001, Batch: 104000, Sample Num: 104000, Cur Loss: 0.00002310, Cur Avg Loss: 0.01521223, Log Avg loss: 0.00546855, Global Avg Loss: 0.01521223, Time: 0.0114
Steps: 104000, Updated lr: 0.000199
Training, Epoch: 0001, Batch: 108000, Sample Num: 108000, Cur Loss: 0.00343507, Cur Avg Loss: 0.01482949, Log Avg loss: 0.00487839, Global Avg Loss: 0.01482949, Time: 0.0142
Steps: 108000, Updated lr: 0.000199
Training, Epoch: 0001, Batch: 112000, Sample Num: 112000, Cur Loss: 0.00026758, Cur Avg Loss: 0.01445769, Log Avg loss: 0.00441914, Global Avg Loss: 0.01445769, Time: 0.0177
Steps: 112000, Updated lr: 0.000199
Training, Epoch: 0001, Batch: 116000, Sample Num: 116000, Cur Loss: 0.00013576, Cur Avg Loss: 0.01412652, Log Avg loss: 0.00485376, Global Avg Loss: 0.01412652, Time: 0.0118
Steps: 116000, Updated lr: 0.000199
Training, Epoch: 0001, Batch: 120000, Sample Num: 120000, Cur Loss: 0.00001970, Cur Avg Loss: 0.01380823, Log Avg loss: 0.00457777, Global Avg Loss: 0.01380823, Time: 0.0132
Steps: 120000, Updated lr: 0.000199
Training, Epoch: 0001, Batch: 124000, Sample Num: 124000, Cur Loss: 0.00030804, Cur Avg Loss: 0.01350864, Log Avg loss: 0.00452091, Global Avg Loss: 0.01350864, Time: 0.0122
Steps: 124000, Updated lr: 0.000199
Training, Epoch: 0001, Batch: 128000, Sample Num: 128000, Cur Loss: 0.00000097, Cur Avg Loss: 0.01323990, Log Avg loss: 0.00490891, Global Avg Loss: 0.01323990, Time: 0.0141
Steps: 128000, Updated lr: 0.000198
Training, Epoch: 0001, Batch: 132000, Sample Num: 132000, Cur Loss: 0.00000017, Cur Avg Loss: 0.01298299, Log Avg loss: 0.00476175, Global Avg Loss: 0.01298299, Time: 0.0121
Steps: 132000, Updated lr: 0.000198
Training, Epoch: 0001, Batch: 136000, Sample Num: 136000, Cur Loss: 0.00002655, Cur Avg Loss: 0.01275463, Log Avg loss: 0.00521876, Global Avg Loss: 0.01275463, Time: 0.0142
Steps: 136000, Updated lr: 0.000198
Training, Epoch: 0001, Batch: 140000, Sample Num: 140000, Cur Loss: 0.00000028, Cur Avg Loss: 0.01253196, Log Avg loss: 0.00496117, Global Avg Loss: 0.01253196, Time: 0.0147
Steps: 140000, Updated lr: 0.000198
Training, Epoch: 0001, Batch: 144000, Sample Num: 144000, Cur Loss: 0.00417544, Cur Avg Loss: 0.01230927, Log Avg loss: 0.00451532, Global Avg Loss: 0.01230927, Time: 0.0125
Steps: 144000, Updated lr: 0.000198
Training, Epoch: 0001, Batch: 148000, Sample Num: 148000, Cur Loss: 0.00813102, Cur Avg Loss: 0.01209841, Log Avg loss: 0.00450750, Global Avg Loss: 0.01209841, Time: 0.0118
Steps: 148000, Updated lr: 0.000198
Training, Epoch: 0001, Batch: 152000, Sample Num: 152000, Cur Loss: 0.00000193, Cur Avg Loss: 0.01189329, Log Avg loss: 0.00430379, Global Avg Loss: 0.01189329, Time: 0.0186
Steps: 152000, Updated lr: 0.000198
Training, Epoch: 0001, Batch: 156000, Sample Num: 156000, Cur Loss: 0.00046511, Cur Avg Loss: 0.01169697, Log Avg loss: 0.00423678, Global Avg Loss: 0.01169697, Time: 0.0166
Steps: 156000, Updated lr: 0.000198
Training, Epoch: 0001, Batch: 160000, Sample Num: 160000, Cur Loss: 0.03357175, Cur Avg Loss: 0.01151356, Log Avg loss: 0.00436036, Global Avg Loss: 0.01151356, Time: 0.0146
Steps: 160000, Updated lr: 0.000198
Training, Epoch: 0001, Batch: 164000, Sample Num: 164000, Cur Loss: 0.00028126, Cur Avg Loss: 0.01133862, Log Avg loss: 0.00434111, Global Avg Loss: 0.01133862, Time: 0.0124
Steps: 164000, Updated lr: 0.000198
Training, Epoch: 0001, Batch: 168000, Sample Num: 168000, Cur Loss: 0.00000911, Cur Avg Loss: 0.01117149, Log Avg loss: 0.00431945, Global Avg Loss: 0.01117149, Time: 0.0143
Steps: 168000, Updated lr: 0.000198
***** Running evaluation checkpoint-171385 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-171385 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 2692.841046, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.127454, "eval_total_loss": 2728.031038, "eval_acc": 0.950196, "eval_prec": 0.945735, "eval_recall": 0.950221, "eval_f1": 0.947555, "eval_top2_acc": 0.991964, "eval_top3_acc": 0.997664, "eval_top5_acc": 0.99986, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999237, "eval_pr_auc": 0.982842, "eval_mcc": 0.944675, "eval_sn": 0.950221, "eval_sp": 0.998075, "update_flag": true, "test_avg_loss": 0.131282, "test_total_loss": 2809.954761, "test_acc": 0.947486, "test_prec": 0.944972, "test_recall": 0.947408, "test_f1": 0.945583, "test_top2_acc": 0.990843, "test_top3_acc": 0.997991, "test_top5_acc": 0.999953, "test_top10_acc": 1.0, "test_roc_auc": 0.999176, "test_pr_auc": 0.985769, "test_mcc": 0.941671, "test_sn": 0.947408, "test_sp": 0.997964, "lr": 0.0001974758968841809, "cur_epoch_step": 171385, "train_global_avg_loss": 0.011034721376361762, "train_cur_epoch_loss": 1891.1857230877606, "train_cur_epoch_avg_loss": 0.011034721376361762, "train_cur_epoch_time": 2692.8410456180573, "train_cur_epoch_avg_time": 0.01571223295864899, "epoch": 1, "step": 171385}
##################################################
Training, Epoch: 0002, Batch: 000615, Sample Num: 615, Cur Loss: 0.00000047, Cur Avg Loss: 0.00361885, Log Avg loss: 0.00415006, Global Avg Loss: 0.01100821, Time: 0.0155
Training, Epoch: 0002, Batch: 004615, Sample Num: 4615, Cur Loss: 0.00003809, Cur Avg Loss: 0.00404844, Log Avg loss: 0.00411448, Global Avg Loss: 0.01085153, Time: 0.0125
Training, Epoch: 0002, Batch: 008615, Sample Num: 8615, Cur Loss: 0.00000074, Cur Avg Loss: 0.00424085, Log Avg loss: 0.00446285, Global Avg Loss: 0.01070956, Time: 0.0159
Training, Epoch: 0002, Batch: 012615, Sample Num: 12615, Cur Loss: 0.00000340, Cur Avg Loss: 0.00412441, Log Avg loss: 0.00387363, Global Avg Loss: 0.01056095, Time: 0.0152
Training, Epoch: 0002, Batch: 016615, Sample Num: 16615, Cur Loss: 0.00000049, Cur Avg Loss: 0.00414493, Log Avg loss: 0.00420966, Global Avg Loss: 0.01042582, Time: 0.0133
Training, Epoch: 0002, Batch: 020615, Sample Num: 20615, Cur Loss: 0.00007473, Cur Avg Loss: 0.00411347, Log Avg loss: 0.00398278, Global Avg Loss: 0.01029159, Time: 0.0131
Training, Epoch: 0002, Batch: 024615, Sample Num: 24615, Cur Loss: 0.00633470, Cur Avg Loss: 0.00412427, Log Avg loss: 0.00417995, Global Avg Loss: 0.01016686, Time: 0.0157
Training, Epoch: 0002, Batch: 028615, Sample Num: 28615, Cur Loss: 0.00089407, Cur Avg Loss: 0.00413094, Log Avg loss: 0.00417199, Global Avg Loss: 0.01004696, Time: 0.0129
Training, Epoch: 0002, Batch: 032615, Sample Num: 32615, Cur Loss: 0.00000043, Cur Avg Loss: 0.00411123, Log Avg loss: 0.00397022, Global Avg Loss: 0.00992781, Time: 0.0297
Training, Epoch: 0002, Batch: 036615, Sample Num: 36615, Cur Loss: 0.00004532, Cur Avg Loss: 0.00412101, Log Avg loss: 0.00420078, Global Avg Loss: 0.00981768, Time: 0.0125
Training, Epoch: 0002, Batch: 040615, Sample Num: 40615, Cur Loss: 0.00099751, Cur Avg Loss: 0.00412167, Log Avg loss: 0.00412765, Global Avg Loss: 0.00971032, Time: 0.0195
Training, Epoch: 0002, Batch: 044615, Sample Num: 44615, Cur Loss: 0.00005560, Cur Avg Loss: 0.00411331, Log Avg loss: 0.00402846, Global Avg Loss: 0.00960510, Time: 0.0140
Training, Epoch: 0002, Batch: 048615, Sample Num: 48615, Cur Loss: 0.00000468, Cur Avg Loss: 0.00407323, Log Avg loss: 0.00362617, Global Avg Loss: 0.00949639, Time: 0.0146
Training, Epoch: 0002, Batch: 052615, Sample Num: 52615, Cur Loss: 0.00070280, Cur Avg Loss: 0.00411695, Log Avg loss: 0.00464832, Global Avg Loss: 0.00940982, Time: 0.0137
Training, Epoch: 0002, Batch: 056615, Sample Num: 56615, Cur Loss: 0.00000006, Cur Avg Loss: 0.00408932, Log Avg loss: 0.00372591, Global Avg Loss: 0.00931010, Time: 0.0130
Training, Epoch: 0002, Batch: 060615, Sample Num: 60615, Cur Loss: 0.00000053, Cur Avg Loss: 0.00409293, Log Avg loss: 0.00414403, Global Avg Loss: 0.00922103, Time: 0.0192
Training, Epoch: 0002, Batch: 064615, Sample Num: 64615, Cur Loss: 0.00081950, Cur Avg Loss: 0.00409327, Log Avg loss: 0.00409837, Global Avg Loss: 0.00913420, Time: 0.0161
Training, Epoch: 0002, Batch: 068615, Sample Num: 68615, Cur Loss: 0.00000011, Cur Avg Loss: 0.00409767, Log Avg loss: 0.00416878, Global Avg Loss: 0.00905145, Time: 0.0137
Training, Epoch: 0002, Batch: 072615, Sample Num: 72615, Cur Loss: 0.00000021, Cur Avg Loss: 0.00409989, Log Avg loss: 0.00413796, Global Avg Loss: 0.00897090, Time: 0.0152
Training, Epoch: 0002, Batch: 076615, Sample Num: 76615, Cur Loss: 0.00006157, Cur Avg Loss: 0.00409910, Log Avg loss: 0.00408479, Global Avg Loss: 0.00889209, Time: 0.0121
Training, Epoch: 0002, Batch: 080615, Sample Num: 80615, Cur Loss: 0.00001907, Cur Avg Loss: 0.00409859, Log Avg loss: 0.00408878, Global Avg Loss: 0.00881585, Time: 0.0137
Training, Epoch: 0002, Batch: 084615, Sample Num: 84615, Cur Loss: 0.00009198, Cur Avg Loss: 0.00410511, Log Avg loss: 0.00423654, Global Avg Loss: 0.00874430, Time: 0.0181
Training, Epoch: 0002, Batch: 088615, Sample Num: 88615, Cur Loss: 0.05602040, Cur Avg Loss: 0.00409636, Log Avg loss: 0.00391115, Global Avg Loss: 0.00866994, Time: 0.0125
Training, Epoch: 0002, Batch: 092615, Sample Num: 92615, Cur Loss: 0.00000005, Cur Avg Loss: 0.00410852, Log Avg loss: 0.00437812, Global Avg Loss: 0.00860491, Time: 0.0124
Training, Epoch: 0002, Batch: 096615, Sample Num: 96615, Cur Loss: 0.00000290, Cur Avg Loss: 0.00408860, Log Avg loss: 0.00362736, Global Avg Loss: 0.00853062, Time: 0.0146
Training, Epoch: 0002, Batch: 100615, Sample Num: 100615, Cur Loss: 0.00000008, Cur Avg Loss: 0.00407070, Log Avg loss: 0.00363825, Global Avg Loss: 0.00845867, Time: 0.0184
Training, Epoch: 0002, Batch: 104615, Sample Num: 104615, Cur Loss: 0.00000494, Cur Avg Loss: 0.00405180, Log Avg loss: 0.00357634, Global Avg Loss: 0.00838792, Time: 0.0149
Training, Epoch: 0002, Batch: 108615, Sample Num: 108615, Cur Loss: 0.00000000, Cur Avg Loss: 0.00405403, Log Avg loss: 0.00411231, Global Avg Loss: 0.00832683, Time: 0.0200
Training, Epoch: 0002, Batch: 112615, Sample Num: 112615, Cur Loss: 0.00000388, Cur Avg Loss: 0.00402950, Log Avg loss: 0.00336360, Global Avg Loss: 0.00825693, Time: 0.0147
Training, Epoch: 0002, Batch: 116615, Sample Num: 116615, Cur Loss: 0.00001957, Cur Avg Loss: 0.00402121, Log Avg loss: 0.00378785, Global Avg Loss: 0.00819486, Time: 0.0136
Training, Epoch: 0002, Batch: 120615, Sample Num: 120615, Cur Loss: 0.00290831, Cur Avg Loss: 0.00400907, Log Avg loss: 0.00365506, Global Avg Loss: 0.00813267, Time: 0.0298
Training, Epoch: 0002, Batch: 124615, Sample Num: 124615, Cur Loss: 0.00001627, Cur Avg Loss: 0.00399984, Log Avg loss: 0.00372153, Global Avg Loss: 0.00807306, Time: 0.0151
Training, Epoch: 0002, Batch: 128615, Sample Num: 128615, Cur Loss: 0.00000015, Cur Avg Loss: 0.00398328, Log Avg loss: 0.00346746, Global Avg Loss: 0.00801165, Time: 0.0232
Training, Epoch: 0002, Batch: 132615, Sample Num: 132615, Cur Loss: 0.00010472, Cur Avg Loss: 0.00397380, Log Avg loss: 0.00366880, Global Avg Loss: 0.00795451, Time: 0.0219
Training, Epoch: 0002, Batch: 136615, Sample Num: 136615, Cur Loss: 0.00000023, Cur Avg Loss: 0.00396759, Log Avg loss: 0.00376176, Global Avg Loss: 0.00790006, Time: 0.0247
Training, Epoch: 0002, Batch: 140615, Sample Num: 140615, Cur Loss: 0.00001627, Cur Avg Loss: 0.00395674, Log Avg loss: 0.00358608, Global Avg Loss: 0.00784475, Time: 0.0345
Training, Epoch: 0002, Batch: 144615, Sample Num: 144615, Cur Loss: 0.00001173, Cur Avg Loss: 0.00394591, Log Avg loss: 0.00356540, Global Avg Loss: 0.00779058, Time: 0.0362
Training, Epoch: 0002, Batch: 148615, Sample Num: 148615, Cur Loss: 0.00000347, Cur Avg Loss: 0.00395089, Log Avg loss: 0.00413077, Global Avg Loss: 0.00774483, Time: 0.0126
Training, Epoch: 0002, Batch: 152615, Sample Num: 152615, Cur Loss: 0.00006762, Cur Avg Loss: 0.00394373, Log Avg loss: 0.00367762, Global Avg Loss: 0.00769462, Time: 0.0158
Training, Epoch: 0002, Batch: 156615, Sample Num: 156615, Cur Loss: 0.00222299, Cur Avg Loss: 0.00393576, Log Avg loss: 0.00363183, Global Avg Loss: 0.00764508, Time: 0.0124
Training, Epoch: 0002, Batch: 160615, Sample Num: 160615, Cur Loss: 0.00000139, Cur Avg Loss: 0.00393075, Log Avg loss: 0.00373439, Global Avg Loss: 0.00759796, Time: 0.0234
Training, Epoch: 0002, Batch: 164615, Sample Num: 164615, Cur Loss: 0.00003369, Cur Avg Loss: 0.00392744, Log Avg loss: 0.00379471, Global Avg Loss: 0.00755268, Time: 0.0153
Training, Epoch: 0002, Batch: 168615, Sample Num: 168615, Cur Loss: 0.00009063, Cur Avg Loss: 0.00392359, Log Avg loss: 0.00376514, Global Avg Loss: 0.00750812, Time: 0.0124
***** Running evaluation checkpoint-342770 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-342770 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3063.354832, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.124691, "eval_total_loss": 2668.879765, "eval_acc": 0.950663, "eval_prec": 0.983993, "eval_recall": 0.962274, "eval_f1": 0.968739, "eval_top2_acc": 0.992104, "eval_top3_acc": 0.999066, "eval_top5_acc": 0.99986, "eval_top10_acc": 1.0, "eval_roc_auc": 0.999365, "eval_pr_auc": 0.986594, "eval_mcc": 0.945531, "eval_sn": 0.962274, "eval_sp": 0.998092, "update_flag": true, "test_avg_loss": 0.126509, "test_total_loss": 2707.788633, "test_acc": 0.949168, "test_prec": 0.977639, "test_recall": 0.963676, "test_f1": 0.969338, "test_top2_acc": 0.992011, "test_top3_acc": 0.998739, "test_top5_acc": 0.999953, "test_top10_acc": 1.0, "test_roc_auc": 0.999335, "test_pr_auc": 0.988454, "test_mcc": 0.943809, "test_sn": 0.963676, "test_sp": 0.998027, "lr": 0.00019344711232823124, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0074774834282309985, "train_cur_epoch_loss": 671.8712716063152, "train_cur_epoch_avg_loss": 0.003920245480096363, "train_cur_epoch_time": 3063.354832172394, "train_cur_epoch_avg_time": 0.017874112858023712, "epoch": 2, "step": 342770}
##################################################
Training, Epoch: 0003, Batch: 001230, Sample Num: 1230, Cur Loss: 0.00000002, Cur Avg Loss: 0.00339196, Log Avg loss: 0.00361684, Global Avg Loss: 0.00746288, Time: 0.0144
Training, Epoch: 0003, Batch: 005230, Sample Num: 5230, Cur Loss: 0.00000980, Cur Avg Loss: 0.00317960, Log Avg loss: 0.00311429, Global Avg Loss: 0.00741289, Time: 0.0148
Training, Epoch: 0003, Batch: 009230, Sample Num: 9230, Cur Loss: 0.00008036, Cur Avg Loss: 0.00324002, Log Avg loss: 0.00331903, Global Avg Loss: 0.00736637, Time: 0.0179
Training, Epoch: 0003, Batch: 013230, Sample Num: 13230, Cur Loss: 0.00000073, Cur Avg Loss: 0.00332740, Log Avg loss: 0.00352903, Global Avg Loss: 0.00732325, Time: 0.0234
Training, Epoch: 0003, Batch: 017230, Sample Num: 17230, Cur Loss: 0.00001885, Cur Avg Loss: 0.00342277, Log Avg loss: 0.00373820, Global Avg Loss: 0.00728342, Time: 0.0141
Training, Epoch: 0003, Batch: 021230, Sample Num: 21230, Cur Loss: 0.00000002, Cur Avg Loss: 0.00347238, Log Avg loss: 0.00368607, Global Avg Loss: 0.00724389, Time: 0.0189
Training, Epoch: 0003, Batch: 025230, Sample Num: 25230, Cur Loss: 0.00000979, Cur Avg Loss: 0.00350058, Log Avg loss: 0.00365026, Global Avg Loss: 0.00720483, Time: 0.0141
Training, Epoch: 0003, Batch: 029230, Sample Num: 29230, Cur Loss: 0.00000000, Cur Avg Loss: 0.00345665, Log Avg loss: 0.00317955, Global Avg Loss: 0.00716155, Time: 0.0266
Training, Epoch: 0003, Batch: 033230, Sample Num: 33230, Cur Loss: 0.00000681, Cur Avg Loss: 0.00348017, Log Avg loss: 0.00365208, Global Avg Loss: 0.00712421, Time: 0.0130
Training, Epoch: 0003, Batch: 037230, Sample Num: 37230, Cur Loss: 0.00000003, Cur Avg Loss: 0.00343385, Log Avg loss: 0.00304905, Global Avg Loss: 0.00708131, Time: 0.0244
Training, Epoch: 0003, Batch: 041230, Sample Num: 41230, Cur Loss: 0.00019643, Cur Avg Loss: 0.00341844, Log Avg loss: 0.00327498, Global Avg Loss: 0.00704166, Time: 0.0138
Training, Epoch: 0003, Batch: 045230, Sample Num: 45230, Cur Loss: 0.00000000, Cur Avg Loss: 0.00340232, Log Avg loss: 0.00323615, Global Avg Loss: 0.00700243, Time: 0.0253
Training, Epoch: 0003, Batch: 049230, Sample Num: 49230, Cur Loss: 0.00006078, Cur Avg Loss: 0.00339863, Log Avg loss: 0.00335691, Global Avg Loss: 0.00696523, Time: 0.0126
Training, Epoch: 0003, Batch: 053230, Sample Num: 53230, Cur Loss: 0.00000205, Cur Avg Loss: 0.00338067, Log Avg loss: 0.00315967, Global Avg Loss: 0.00692679, Time: 0.0150
Training, Epoch: 0003, Batch: 057230, Sample Num: 57230, Cur Loss: 0.00000155, Cur Avg Loss: 0.00339459, Log Avg loss: 0.00357981, Global Avg Loss: 0.00689332, Time: 0.0151
Training, Epoch: 0003, Batch: 061230, Sample Num: 61230, Cur Loss: 0.00001378, Cur Avg Loss: 0.00337883, Log Avg loss: 0.00315324, Global Avg Loss: 0.00685629, Time: 0.0151
Training, Epoch: 0003, Batch: 065230, Sample Num: 65230, Cur Loss: 0.00023934, Cur Avg Loss: 0.00336361, Log Avg loss: 0.00313072, Global Avg Loss: 0.00681977, Time: 0.0139
Training, Epoch: 0003, Batch: 069230, Sample Num: 69230, Cur Loss: 0.00000979, Cur Avg Loss: 0.00336767, Log Avg loss: 0.00343391, Global Avg Loss: 0.00678690, Time: 0.0117
Training, Epoch: 0003, Batch: 073230, Sample Num: 73230, Cur Loss: 0.00000025, Cur Avg Loss: 0.00336182, Log Avg loss: 0.00326058, Global Avg Loss: 0.00675299, Time: 0.0320
Training, Epoch: 0003, Batch: 077230, Sample Num: 77230, Cur Loss: 0.00001089, Cur Avg Loss: 0.00337338, Log Avg loss: 0.00358488, Global Avg Loss: 0.00672282, Time: 0.0147
Training, Epoch: 0003, Batch: 081230, Sample Num: 81230, Cur Loss: 0.01666742, Cur Avg Loss: 0.00336710, Log Avg loss: 0.00324594, Global Avg Loss: 0.00669002, Time: 0.0177
Training, Epoch: 0003, Batch: 085230, Sample Num: 85230, Cur Loss: 0.00000608, Cur Avg Loss: 0.00336218, Log Avg loss: 0.00326216, Global Avg Loss: 0.00665798, Time: 0.0125
Training, Epoch: 0003, Batch: 089230, Sample Num: 89230, Cur Loss: 0.00000009, Cur Avg Loss: 0.00335430, Log Avg loss: 0.00318640, Global Avg Loss: 0.00662584, Time: 0.0331
Training, Epoch: 0003, Batch: 093230, Sample Num: 93230, Cur Loss: 0.00390012, Cur Avg Loss: 0.00336246, Log Avg loss: 0.00354448, Global Avg Loss: 0.00659757, Time: 0.0132
Training, Epoch: 0003, Batch: 097230, Sample Num: 97230, Cur Loss: 0.00000004, Cur Avg Loss: 0.00337435, Log Avg loss: 0.00365167, Global Avg Loss: 0.00657079, Time: 0.0155
Training, Epoch: 0003, Batch: 101230, Sample Num: 101230, Cur Loss: 0.00000332, Cur Avg Loss: 0.00337912, Log Avg loss: 0.00349488, Global Avg Loss: 0.00654307, Time: 0.0230
Training, Epoch: 0003, Batch: 105230, Sample Num: 105230, Cur Loss: 0.00000203, Cur Avg Loss: 0.00337870, Log Avg loss: 0.00336815, Global Avg Loss: 0.00651473, Time: 0.0130
Training, Epoch: 0003, Batch: 109230, Sample Num: 109230, Cur Loss: 0.00001248, Cur Avg Loss: 0.00337773, Log Avg loss: 0.00335222, Global Avg Loss: 0.00648674, Time: 0.0213
Training, Epoch: 0003, Batch: 113230, Sample Num: 113230, Cur Loss: 0.00363666, Cur Avg Loss: 0.00336809, Log Avg loss: 0.00310496, Global Avg Loss: 0.00645708, Time: 0.0125
Training, Epoch: 0003, Batch: 117230, Sample Num: 117230, Cur Loss: 0.00000000, Cur Avg Loss: 0.00336446, Log Avg loss: 0.00326149, Global Avg Loss: 0.00642929, Time: 0.0129
Training, Epoch: 0003, Batch: 121230, Sample Num: 121230, Cur Loss: 0.00684851, Cur Avg Loss: 0.00336270, Log Avg loss: 0.00331128, Global Avg Loss: 0.00640241, Time: 0.0169
Training, Epoch: 0003, Batch: 125230, Sample Num: 125230, Cur Loss: 0.00000005, Cur Avg Loss: 0.00335229, Log Avg loss: 0.00303676, Global Avg Loss: 0.00637364, Time: 0.0141
Training, Epoch: 0003, Batch: 129230, Sample Num: 129230, Cur Loss: 0.00001830, Cur Avg Loss: 0.00335854, Log Avg loss: 0.00355414, Global Avg Loss: 0.00634975, Time: 0.0234
Training, Epoch: 0003, Batch: 133230, Sample Num: 133230, Cur Loss: 0.02911783, Cur Avg Loss: 0.00334598, Log Avg loss: 0.00294040, Global Avg Loss: 0.00632110, Time: 0.0126
Training, Epoch: 0003, Batch: 137230, Sample Num: 137230, Cur Loss: 0.00000009, Cur Avg Loss: 0.00334441, Log Avg loss: 0.00329201, Global Avg Loss: 0.00629586, Time: 0.0142
Training, Epoch: 0003, Batch: 141230, Sample Num: 141230, Cur Loss: 0.00007429, Cur Avg Loss: 0.00334247, Log Avg loss: 0.00327581, Global Avg Loss: 0.00627090, Time: 0.0178
Training, Epoch: 0003, Batch: 145230, Sample Num: 145230, Cur Loss: 0.00000086, Cur Avg Loss: 0.00333783, Log Avg loss: 0.00317403, Global Avg Loss: 0.00624551, Time: 0.0195
Training, Epoch: 0003, Batch: 149230, Sample Num: 149230, Cur Loss: 0.00001264, Cur Avg Loss: 0.00334241, Log Avg loss: 0.00350865, Global Avg Loss: 0.00622326, Time: 0.0141
Training, Epoch: 0003, Batch: 153230, Sample Num: 153230, Cur Loss: 0.00000000, Cur Avg Loss: 0.00333747, Log Avg loss: 0.00315323, Global Avg Loss: 0.00619850, Time: 0.0129
Training, Epoch: 0003, Batch: 157230, Sample Num: 157230, Cur Loss: 0.00000003, Cur Avg Loss: 0.00334637, Log Avg loss: 0.00368741, Global Avg Loss: 0.00617841, Time: 0.0154
Training, Epoch: 0003, Batch: 161230, Sample Num: 161230, Cur Loss: 0.00000732, Cur Avg Loss: 0.00334744, Log Avg loss: 0.00338950, Global Avg Loss: 0.00615628, Time: 0.0130
Training, Epoch: 0003, Batch: 165230, Sample Num: 165230, Cur Loss: 0.00005405, Cur Avg Loss: 0.00334147, Log Avg loss: 0.00310060, Global Avg Loss: 0.00613222, Time: 0.0150
Training, Epoch: 0003, Batch: 169230, Sample Num: 169230, Cur Loss: 0.00000000, Cur Avg Loss: 0.00334689, Log Avg loss: 0.00357079, Global Avg Loss: 0.00611221, Time: 0.0130
***** Running evaluation checkpoint-514155 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-514155 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3033.619356, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.106218, "eval_total_loss": 2273.49141, "eval_acc": 0.958326, "eval_prec": 0.984278, "eval_recall": 0.965557, "eval_f1": 0.970931, "eval_top2_acc": 0.994534, "eval_top3_acc": 0.999206, "eval_top5_acc": 0.999813, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999459, "eval_pr_auc": 0.984755, "eval_mcc": 0.953717, "eval_sn": 0.965557, "eval_sp": 0.998396, "update_flag": true, "test_avg_loss": 0.106946, "test_total_loss": 2289.068481, "test_acc": 0.957905, "test_prec": 0.984399, "test_recall": 0.969137, "test_f1": 0.974573, "test_top2_acc": 0.993786, "test_top3_acc": 0.999299, "test_top5_acc": 0.99986, "test_top10_acc": 1.0, "test_roc_auc": 0.999456, "test_pr_auc": 0.989769, "test_mcc": 0.953244, "test_sn": 0.969137, "test_sp": 0.998374, "lr": 0.00018941832777228152, "cur_epoch_step": 171385, "train_global_avg_loss": 0.006100665983696112, "train_cur_epoch_loss": 573.6309241517702, "train_cur_epoch_avg_loss": 0.0033470310946218758, "train_cur_epoch_time": 3033.6193556785583, "train_cur_epoch_avg_time": 0.017700611813627556, "epoch": 3, "step": 514155}
##################################################
Training, Epoch: 0004, Batch: 001845, Sample Num: 1845, Cur Loss: 0.00063606, Cur Avg Loss: 0.00293824, Log Avg loss: 0.00316459, Global Avg Loss: 0.00608936, Time: 0.0123
Training, Epoch: 0004, Batch: 005845, Sample Num: 5845, Cur Loss: 0.00000006, Cur Avg Loss: 0.00296125, Log Avg loss: 0.00297186, Global Avg Loss: 0.00606538, Time: 0.0153
Training, Epoch: 0004, Batch: 009845, Sample Num: 9845, Cur Loss: 0.00000000, Cur Avg Loss: 0.00297639, Log Avg loss: 0.00299851, Global Avg Loss: 0.00604197, Time: 0.0241
Training, Epoch: 0004, Batch: 013845, Sample Num: 13845, Cur Loss: 0.00012485, Cur Avg Loss: 0.00304521, Log Avg loss: 0.00321459, Global Avg Loss: 0.00602055, Time: 0.0118
Training, Epoch: 0004, Batch: 017845, Sample Num: 17845, Cur Loss: 0.00001006, Cur Avg Loss: 0.00304650, Log Avg loss: 0.00305098, Global Avg Loss: 0.00599822, Time: 0.0124
Training, Epoch: 0004, Batch: 021845, Sample Num: 21845, Cur Loss: 0.00184537, Cur Avg Loss: 0.00294592, Log Avg loss: 0.00249721, Global Avg Loss: 0.00597209, Time: 0.0122
Training, Epoch: 0004, Batch: 025845, Sample Num: 25845, Cur Loss: 0.00000450, Cur Avg Loss: 0.00292343, Log Avg loss: 0.00280062, Global Avg Loss: 0.00594860, Time: 0.0135
Training, Epoch: 0004, Batch: 029845, Sample Num: 29845, Cur Loss: 0.00000004, Cur Avg Loss: 0.00293849, Log Avg loss: 0.00303579, Global Avg Loss: 0.00592718, Time: 0.0155
Training, Epoch: 0004, Batch: 033845, Sample Num: 33845, Cur Loss: 0.00000059, Cur Avg Loss: 0.00293242, Log Avg loss: 0.00288711, Global Avg Loss: 0.00590499, Time: 0.0189
Training, Epoch: 0004, Batch: 037845, Sample Num: 37845, Cur Loss: 0.00005347, Cur Avg Loss: 0.00294449, Log Avg loss: 0.00304667, Global Avg Loss: 0.00588428, Time: 0.0140
Training, Epoch: 0004, Batch: 041845, Sample Num: 41845, Cur Loss: 0.01430833, Cur Avg Loss: 0.00291422, Log Avg loss: 0.00262781, Global Avg Loss: 0.00586085, Time: 0.0178
Training, Epoch: 0004, Batch: 045845, Sample Num: 45845, Cur Loss: 0.00001601, Cur Avg Loss: 0.00290434, Log Avg loss: 0.00280097, Global Avg Loss: 0.00583900, Time: 0.0131
Training, Epoch: 0004, Batch: 049845, Sample Num: 49845, Cur Loss: 0.00517286, Cur Avg Loss: 0.00291405, Log Avg loss: 0.00302534, Global Avg Loss: 0.00581904, Time: 0.0237
Training, Epoch: 0004, Batch: 053845, Sample Num: 53845, Cur Loss: 0.00000164, Cur Avg Loss: 0.00290126, Log Avg loss: 0.00274192, Global Avg Loss: 0.00579737, Time: 0.0148
Training, Epoch: 0004, Batch: 057845, Sample Num: 57845, Cur Loss: 0.00000000, Cur Avg Loss: 0.00288809, Log Avg loss: 0.00271071, Global Avg Loss: 0.00577579, Time: 0.0161
Training, Epoch: 0004, Batch: 061845, Sample Num: 61845, Cur Loss: 0.00000418, Cur Avg Loss: 0.00289118, Log Avg loss: 0.00293591, Global Avg Loss: 0.00575606, Time: 0.0145
Training, Epoch: 0004, Batch: 065845, Sample Num: 65845, Cur Loss: 0.00014595, Cur Avg Loss: 0.00287398, Log Avg loss: 0.00260812, Global Avg Loss: 0.00573435, Time: 0.0130
Training, Epoch: 0004, Batch: 069845, Sample Num: 69845, Cur Loss: 0.00000000, Cur Avg Loss: 0.00289519, Log Avg loss: 0.00324434, Global Avg Loss: 0.00571730, Time: 0.0220
Training, Epoch: 0004, Batch: 073845, Sample Num: 73845, Cur Loss: 0.00004184, Cur Avg Loss: 0.00289718, Log Avg loss: 0.00293192, Global Avg Loss: 0.00569835, Time: 0.0125
Training, Epoch: 0004, Batch: 077845, Sample Num: 77845, Cur Loss: 0.00000310, Cur Avg Loss: 0.00289582, Log Avg loss: 0.00287073, Global Avg Loss: 0.00567925, Time: 0.0148
Training, Epoch: 0004, Batch: 081845, Sample Num: 81845, Cur Loss: 0.00000000, Cur Avg Loss: 0.00290303, Log Avg loss: 0.00304320, Global Avg Loss: 0.00566155, Time: 0.0137
Training, Epoch: 0004, Batch: 085845, Sample Num: 85845, Cur Loss: 0.00059657, Cur Avg Loss: 0.00289157, Log Avg loss: 0.00265708, Global Avg Loss: 0.00564152, Time: 0.0125
Training, Epoch: 0004, Batch: 089845, Sample Num: 89845, Cur Loss: 0.00005590, Cur Avg Loss: 0.00288382, Log Avg loss: 0.00271763, Global Avg Loss: 0.00562216, Time: 0.0186
Training, Epoch: 0004, Batch: 093845, Sample Num: 93845, Cur Loss: 0.00001262, Cur Avg Loss: 0.00288020, Log Avg loss: 0.00279878, Global Avg Loss: 0.00560359, Time: 0.0130
Training, Epoch: 0004, Batch: 097845, Sample Num: 97845, Cur Loss: 0.00000000, Cur Avg Loss: 0.00287844, Log Avg loss: 0.00283719, Global Avg Loss: 0.00558550, Time: 0.0123
Training, Epoch: 0004, Batch: 101845, Sample Num: 101845, Cur Loss: 0.00000817, Cur Avg Loss: 0.00287281, Log Avg loss: 0.00273499, Global Avg Loss: 0.00556699, Time: 0.0142
Training, Epoch: 0004, Batch: 105845, Sample Num: 105845, Cur Loss: 0.00002454, Cur Avg Loss: 0.00287307, Log Avg loss: 0.00287990, Global Avg Loss: 0.00554966, Time: 0.0124
Training, Epoch: 0004, Batch: 109845, Sample Num: 109845, Cur Loss: 0.10802461, Cur Avg Loss: 0.00286833, Log Avg loss: 0.00274281, Global Avg Loss: 0.00553167, Time: 0.0125
Training, Epoch: 0004, Batch: 113845, Sample Num: 113845, Cur Loss: 0.00000357, Cur Avg Loss: 0.00289080, Log Avg loss: 0.00350788, Global Avg Loss: 0.00551878, Time: 0.0127
Training, Epoch: 0004, Batch: 117845, Sample Num: 117845, Cur Loss: 0.00000002, Cur Avg Loss: 0.00288594, Log Avg loss: 0.00274762, Global Avg Loss: 0.00550124, Time: 0.0274
Training, Epoch: 0004, Batch: 121845, Sample Num: 121845, Cur Loss: 0.00003514, Cur Avg Loss: 0.00289347, Log Avg loss: 0.00311532, Global Avg Loss: 0.00548623, Time: 0.0179
Training, Epoch: 0004, Batch: 125845, Sample Num: 125845, Cur Loss: 0.00000000, Cur Avg Loss: 0.00289114, Log Avg loss: 0.00281997, Global Avg Loss: 0.00546957, Time: 0.0129
Training, Epoch: 0004, Batch: 129845, Sample Num: 129845, Cur Loss: 0.00000005, Cur Avg Loss: 0.00289712, Log Avg loss: 0.00308537, Global Avg Loss: 0.00545476, Time: 0.0262
Training, Epoch: 0004, Batch: 133845, Sample Num: 133845, Cur Loss: 0.00131495, Cur Avg Loss: 0.00289840, Log Avg loss: 0.00293989, Global Avg Loss: 0.00543923, Time: 0.0129
Training, Epoch: 0004, Batch: 137845, Sample Num: 137845, Cur Loss: 0.00001129, Cur Avg Loss: 0.00290118, Log Avg loss: 0.00299443, Global Avg Loss: 0.00542424, Time: 0.0126
Training, Epoch: 0004, Batch: 141845, Sample Num: 141845, Cur Loss: 0.00001736, Cur Avg Loss: 0.00290398, Log Avg loss: 0.00300044, Global Avg Loss: 0.00540946, Time: 0.0122
Training, Epoch: 0004, Batch: 145845, Sample Num: 145845, Cur Loss: 0.00000000, Cur Avg Loss: 0.00291615, Log Avg loss: 0.00334769, Global Avg Loss: 0.00539696, Time: 0.0124
Training, Epoch: 0004, Batch: 149845, Sample Num: 149845, Cur Loss: 0.00000007, Cur Avg Loss: 0.00292474, Log Avg loss: 0.00323791, Global Avg Loss: 0.00538395, Time: 0.0119
Training, Epoch: 0004, Batch: 153845, Sample Num: 153845, Cur Loss: 0.00000912, Cur Avg Loss: 0.00292362, Log Avg loss: 0.00288176, Global Avg Loss: 0.00536897, Time: 0.0141
Training, Epoch: 0004, Batch: 157845, Sample Num: 157845, Cur Loss: 0.00000000, Cur Avg Loss: 0.00292618, Log Avg loss: 0.00302438, Global Avg Loss: 0.00535502, Time: 0.0123
Training, Epoch: 0004, Batch: 161845, Sample Num: 161845, Cur Loss: 0.00000000, Cur Avg Loss: 0.00291781, Log Avg loss: 0.00258754, Global Avg Loss: 0.00533864, Time: 0.0148
Training, Epoch: 0004, Batch: 165845, Sample Num: 165845, Cur Loss: 0.00025413, Cur Avg Loss: 0.00291733, Log Avg loss: 0.00289802, Global Avg Loss: 0.00532428, Time: 0.0126
Training, Epoch: 0004, Batch: 169845, Sample Num: 169845, Cur Loss: 0.00000005, Cur Avg Loss: 0.00292174, Log Avg loss: 0.00310442, Global Avg Loss: 0.00531130, Time: 0.0188
***** Running evaluation checkpoint-685540 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-685540 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 2712.717369, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.109601, "eval_total_loss": 2345.899043, "eval_acc": 0.958139, "eval_prec": 0.980382, "eval_recall": 0.969291, "eval_f1": 0.970687, "eval_top2_acc": 0.99388, "eval_top3_acc": 0.998785, "eval_top5_acc": 0.99986, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999504, "eval_pr_auc": 0.987581, "eval_mcc": 0.95373, "eval_sn": 0.969291, "eval_sp": 0.99842, "update_flag": false, "test_avg_loss": 0.111743, "test_total_loss": 2391.741577, "test_acc": 0.956503, "test_prec": 0.980574, "test_recall": 0.971974, "test_f1": 0.973972, "test_top2_acc": 0.993179, "test_top3_acc": 0.998925, "test_top5_acc": 0.999953, "test_top10_acc": 0.999953, "test_roc_auc": 0.999467, "test_pr_auc": 0.990359, "test_mcc": 0.951834, "test_sn": 0.971974, "test_sp": 0.998347, "lr": 0.00018538954321633183, "cur_epoch_step": 171385, "train_global_avg_loss": 0.005306509525340976, "train_cur_epoch_loss": 501.13662115413825, "train_cur_epoch_avg_loss": 0.002924040150270667, "train_cur_epoch_time": 2712.71736907959, "train_cur_epoch_avg_time": 0.015828207655743443, "epoch": 4, "step": 685540}
##################################################
Training, Epoch: 0005, Batch: 002460, Sample Num: 2460, Cur Loss: 0.00727497, Cur Avg Loss: 0.00215873, Log Avg loss: 0.00255119, Global Avg Loss: 0.00529525, Time: 0.0144
Training, Epoch: 0005, Batch: 006460, Sample Num: 6460, Cur Loss: 0.00000132, Cur Avg Loss: 0.00246657, Log Avg loss: 0.00265590, Global Avg Loss: 0.00528000, Time: 0.0147
Training, Epoch: 0005, Batch: 010460, Sample Num: 10460, Cur Loss: 0.00000276, Cur Avg Loss: 0.00242023, Log Avg loss: 0.00234539, Global Avg Loss: 0.00526313, Time: 0.0125
Training, Epoch: 0005, Batch: 014460, Sample Num: 14460, Cur Loss: 0.00000018, Cur Avg Loss: 0.00236201, Log Avg loss: 0.00220976, Global Avg Loss: 0.00524568, Time: 0.0155
Training, Epoch: 0005, Batch: 018460, Sample Num: 18460, Cur Loss: 0.00000002, Cur Avg Loss: 0.00236392, Log Avg loss: 0.00237083, Global Avg Loss: 0.00522935, Time: 0.0189
Training, Epoch: 0005, Batch: 022460, Sample Num: 22460, Cur Loss: 0.00000000, Cur Avg Loss: 0.00246174, Log Avg loss: 0.00291319, Global Avg Loss: 0.00521626, Time: 0.0328
Training, Epoch: 0005, Batch: 026460, Sample Num: 26460, Cur Loss: 0.00000019, Cur Avg Loss: 0.00242718, Log Avg loss: 0.00223312, Global Avg Loss: 0.00519951, Time: 0.0139
Training, Epoch: 0005, Batch: 030460, Sample Num: 30460, Cur Loss: 0.00000032, Cur Avg Loss: 0.00242700, Log Avg loss: 0.00242580, Global Avg Loss: 0.00518401, Time: 0.0134
Training, Epoch: 0005, Batch: 034460, Sample Num: 34460, Cur Loss: 0.00000001, Cur Avg Loss: 0.00246612, Log Avg loss: 0.00276403, Global Avg Loss: 0.00517057, Time: 0.0142
Training, Epoch: 0005, Batch: 038460, Sample Num: 38460, Cur Loss: 0.00002246, Cur Avg Loss: 0.00247363, Log Avg loss: 0.00253829, Global Avg Loss: 0.00515602, Time: 0.0125
Training, Epoch: 0005, Batch: 042460, Sample Num: 42460, Cur Loss: 0.00000000, Cur Avg Loss: 0.00247298, Log Avg loss: 0.00246679, Global Avg Loss: 0.00514125, Time: 0.0145
Training, Epoch: 0005, Batch: 046460, Sample Num: 46460, Cur Loss: 0.00079370, Cur Avg Loss: 0.00245502, Log Avg loss: 0.00226438, Global Avg Loss: 0.00512553, Time: 0.0161
Training, Epoch: 0005, Batch: 050460, Sample Num: 50460, Cur Loss: 0.00004379, Cur Avg Loss: 0.00247314, Log Avg loss: 0.00268357, Global Avg Loss: 0.00511225, Time: 0.0124
Training, Epoch: 0005, Batch: 054460, Sample Num: 54460, Cur Loss: 0.00324962, Cur Avg Loss: 0.00247157, Log Avg loss: 0.00245176, Global Avg Loss: 0.00509787, Time: 0.0132
Training, Epoch: 0005, Batch: 058460, Sample Num: 58460, Cur Loss: 0.00000129, Cur Avg Loss: 0.00244764, Log Avg loss: 0.00212184, Global Avg Loss: 0.00508187, Time: 0.0143
Training, Epoch: 0005, Batch: 062460, Sample Num: 62460, Cur Loss: 0.00000001, Cur Avg Loss: 0.00245758, Log Avg loss: 0.00260288, Global Avg Loss: 0.00506862, Time: 0.0156
Training, Epoch: 0005, Batch: 066460, Sample Num: 66460, Cur Loss: 0.00000091, Cur Avg Loss: 0.00245412, Log Avg loss: 0.00240002, Global Avg Loss: 0.00505442, Time: 0.0155
Training, Epoch: 0005, Batch: 070460, Sample Num: 70460, Cur Loss: 0.00000001, Cur Avg Loss: 0.00246391, Log Avg loss: 0.00262661, Global Avg Loss: 0.00504158, Time: 0.0184
Training, Epoch: 0005, Batch: 074460, Sample Num: 74460, Cur Loss: 0.00000118, Cur Avg Loss: 0.00248022, Log Avg loss: 0.00276758, Global Avg Loss: 0.00502961, Time: 0.0127
Training, Epoch: 0005, Batch: 078460, Sample Num: 78460, Cur Loss: 0.00000000, Cur Avg Loss: 0.00246480, Log Avg loss: 0.00217764, Global Avg Loss: 0.00501468, Time: 0.0142
Training, Epoch: 0005, Batch: 082460, Sample Num: 82460, Cur Loss: 0.00000000, Cur Avg Loss: 0.00245786, Log Avg loss: 0.00232170, Global Avg Loss: 0.00500065, Time: 0.0126
Training, Epoch: 0005, Batch: 086460, Sample Num: 86460, Cur Loss: 0.00000002, Cur Avg Loss: 0.00247992, Log Avg loss: 0.00293488, Global Avg Loss: 0.00498995, Time: 0.0144
Training, Epoch: 0005, Batch: 090460, Sample Num: 90460, Cur Loss: 0.00000548, Cur Avg Loss: 0.00247918, Log Avg loss: 0.00246312, Global Avg Loss: 0.00497692, Time: 0.0122
Training, Epoch: 0005, Batch: 094460, Sample Num: 94460, Cur Loss: 0.00000115, Cur Avg Loss: 0.00247786, Log Avg loss: 0.00244795, Global Avg Loss: 0.00496395, Time: 0.0124
Training, Epoch: 0005, Batch: 098460, Sample Num: 98460, Cur Loss: 0.00000001, Cur Avg Loss: 0.00247666, Log Avg loss: 0.00244827, Global Avg Loss: 0.00495112, Time: 0.0137
Training, Epoch: 0005, Batch: 102460, Sample Num: 102460, Cur Loss: 0.00005338, Cur Avg Loss: 0.00248371, Log Avg loss: 0.00265727, Global Avg Loss: 0.00493947, Time: 0.0142
Training, Epoch: 0005, Batch: 106460, Sample Num: 106460, Cur Loss: 0.00000233, Cur Avg Loss: 0.00249311, Log Avg loss: 0.00273380, Global Avg Loss: 0.00492833, Time: 0.0150
Training, Epoch: 0005, Batch: 110460, Sample Num: 110460, Cur Loss: 0.00348999, Cur Avg Loss: 0.00249942, Log Avg loss: 0.00266745, Global Avg Loss: 0.00491697, Time: 0.0427
Training, Epoch: 0005, Batch: 114460, Sample Num: 114460, Cur Loss: 0.00000167, Cur Avg Loss: 0.00249270, Log Avg loss: 0.00230712, Global Avg Loss: 0.00490392, Time: 0.0235
Training, Epoch: 0005, Batch: 118460, Sample Num: 118460, Cur Loss: 0.00000007, Cur Avg Loss: 0.00249546, Log Avg loss: 0.00257441, Global Avg Loss: 0.00489233, Time: 0.0159
Training, Epoch: 0005, Batch: 122460, Sample Num: 122460, Cur Loss: 0.00320544, Cur Avg Loss: 0.00250402, Log Avg loss: 0.00275765, Global Avg Loss: 0.00488177, Time: 0.0172
Training, Epoch: 0005, Batch: 126460, Sample Num: 126460, Cur Loss: 0.00044659, Cur Avg Loss: 0.00250970, Log Avg loss: 0.00268368, Global Avg Loss: 0.00487094, Time: 0.0174
Training, Epoch: 0005, Batch: 130460, Sample Num: 130460, Cur Loss: 0.00000001, Cur Avg Loss: 0.00251453, Log Avg loss: 0.00266725, Global Avg Loss: 0.00486014, Time: 0.0335
Training, Epoch: 0005, Batch: 134460, Sample Num: 134460, Cur Loss: 0.00093415, Cur Avg Loss: 0.00251876, Log Avg loss: 0.00265652, Global Avg Loss: 0.00484939, Time: 0.0121
Training, Epoch: 0005, Batch: 138460, Sample Num: 138460, Cur Loss: 0.00000000, Cur Avg Loss: 0.00252209, Log Avg loss: 0.00263414, Global Avg Loss: 0.00483863, Time: 0.0123
Training, Epoch: 0005, Batch: 142460, Sample Num: 142460, Cur Loss: 0.00002675, Cur Avg Loss: 0.00252492, Log Avg loss: 0.00262278, Global Avg Loss: 0.00482793, Time: 0.0142
Training, Epoch: 0005, Batch: 146460, Sample Num: 146460, Cur Loss: 0.00000028, Cur Avg Loss: 0.00252340, Log Avg loss: 0.00246941, Global Avg Loss: 0.00481659, Time: 0.0136
Training, Epoch: 0005, Batch: 150460, Sample Num: 150460, Cur Loss: 0.00000575, Cur Avg Loss: 0.00253550, Log Avg loss: 0.00297861, Global Avg Loss: 0.00480779, Time: 0.0127
Training, Epoch: 0005, Batch: 154460, Sample Num: 154460, Cur Loss: 0.00430991, Cur Avg Loss: 0.00253055, Log Avg loss: 0.00234422, Global Avg Loss: 0.00479606, Time: 0.0154
Training, Epoch: 0005, Batch: 158460, Sample Num: 158460, Cur Loss: 0.00000000, Cur Avg Loss: 0.00252704, Log Avg loss: 0.00239154, Global Avg Loss: 0.00478467, Time: 0.0136
Training, Epoch: 0005, Batch: 162460, Sample Num: 162460, Cur Loss: 0.00000283, Cur Avg Loss: 0.00252733, Log Avg loss: 0.00253860, Global Avg Loss: 0.00477407, Time: 0.0383
Training, Epoch: 0005, Batch: 166460, Sample Num: 166460, Cur Loss: 0.00000875, Cur Avg Loss: 0.00253300, Log Avg loss: 0.00276356, Global Avg Loss: 0.00476463, Time: 0.0127
Training, Epoch: 0005, Batch: 170460, Sample Num: 170460, Cur Loss: 0.00002597, Cur Avg Loss: 0.00253930, Log Avg loss: 0.00280120, Global Avg Loss: 0.00475546, Time: 0.0150
***** Running evaluation checkpoint-856925 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-856925 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 2724.736483, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.101772, "eval_total_loss": 2178.327058, "eval_acc": 0.962671, "eval_prec": 0.985878, "eval_recall": 0.96699, "eval_f1": 0.972435, "eval_top2_acc": 0.994113, "eval_top3_acc": 0.999206, "eval_top5_acc": 0.99986, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999526, "eval_pr_auc": 0.990265, "eval_mcc": 0.958536, "eval_sn": 0.96699, "eval_sp": 0.998555, "update_flag": true, "test_avg_loss": 0.103492, "test_total_loss": 2215.143116, "test_acc": 0.961783, "test_prec": 0.978858, "test_recall": 0.967733, "test_f1": 0.972364, "test_top2_acc": 0.99388, "test_top3_acc": 0.998972, "test_top5_acc": 0.99986, "test_top10_acc": 0.999953, "test_roc_auc": 0.999486, "test_pr_auc": 0.990475, "test_mcc": 0.957539, "test_sn": 0.967733, "test_sp": 0.998512, "lr": 0.00018136075866038211, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0047534400364281975, "train_cur_epoch_loss": 435.5170632132458, "train_cur_epoch_avg_loss": 0.002541162080772797, "train_cur_epoch_time": 2724.7364826202393, "train_cur_epoch_avg_time": 0.015898336975932778, "epoch": 5, "step": 856925}
##################################################
Training, Epoch: 0006, Batch: 003075, Sample Num: 3075, Cur Loss: 0.00001224, Cur Avg Loss: 0.00202154, Log Avg loss: 0.00222120, Global Avg Loss: 0.00474367, Time: 0.0151
Training, Epoch: 0006, Batch: 007075, Sample Num: 7075, Cur Loss: 0.00008030, Cur Avg Loss: 0.00192245, Log Avg loss: 0.00184628, Global Avg Loss: 0.00473026, Time: 0.0153
Training, Epoch: 0006, Batch: 011075, Sample Num: 11075, Cur Loss: 0.00000000, Cur Avg Loss: 0.00196847, Log Avg loss: 0.00204987, Global Avg Loss: 0.00471791, Time: 0.0247
Training, Epoch: 0006, Batch: 015075, Sample Num: 15075, Cur Loss: 0.00019625, Cur Avg Loss: 0.00202918, Log Avg loss: 0.00219728, Global Avg Loss: 0.00470634, Time: 0.0127
Training, Epoch: 0006, Batch: 019075, Sample Num: 19075, Cur Loss: 0.00009313, Cur Avg Loss: 0.00206903, Log Avg loss: 0.00221919, Global Avg Loss: 0.00469499, Time: 0.0310
Training, Epoch: 0006, Batch: 023075, Sample Num: 23075, Cur Loss: 0.00000202, Cur Avg Loss: 0.00207659, Log Avg loss: 0.00211267, Global Avg Loss: 0.00468325, Time: 0.0145
Training, Epoch: 0006, Batch: 027075, Sample Num: 27075, Cur Loss: 0.00005247, Cur Avg Loss: 0.00207286, Log Avg loss: 0.00205135, Global Avg Loss: 0.00467134, Time: 0.0128
Training, Epoch: 0006, Batch: 031075, Sample Num: 31075, Cur Loss: 0.00043103, Cur Avg Loss: 0.00206780, Log Avg loss: 0.00203351, Global Avg Loss: 0.00465946, Time: 0.0149
Training, Epoch: 0006, Batch: 035075, Sample Num: 35075, Cur Loss: 0.00106812, Cur Avg Loss: 0.00208569, Log Avg loss: 0.00222471, Global Avg Loss: 0.00464854, Time: 0.0141
Training, Epoch: 0006, Batch: 039075, Sample Num: 39075, Cur Loss: 0.00000000, Cur Avg Loss: 0.00205300, Log Avg loss: 0.00176632, Global Avg Loss: 0.00463567, Time: 0.0156
Training, Epoch: 0006, Batch: 043075, Sample Num: 43075, Cur Loss: 0.00178985, Cur Avg Loss: 0.00204386, Log Avg loss: 0.00195460, Global Avg Loss: 0.00462376, Time: 0.0147
Training, Epoch: 0006, Batch: 047075, Sample Num: 47075, Cur Loss: 0.00000087, Cur Avg Loss: 0.00206697, Log Avg loss: 0.00231587, Global Avg Loss: 0.00461354, Time: 0.0127
Training, Epoch: 0006, Batch: 051075, Sample Num: 51075, Cur Loss: 0.00013529, Cur Avg Loss: 0.00208531, Log Avg loss: 0.00230108, Global Avg Loss: 0.00460336, Time: 0.0127
Training, Epoch: 0006, Batch: 055075, Sample Num: 55075, Cur Loss: 0.00000000, Cur Avg Loss: 0.00207840, Log Avg loss: 0.00199019, Global Avg Loss: 0.00459190, Time: 0.0145
Training, Epoch: 0006, Batch: 059075, Sample Num: 59075, Cur Loss: 0.00001475, Cur Avg Loss: 0.00209136, Log Avg loss: 0.00226980, Global Avg Loss: 0.00458176, Time: 0.0151
Training, Epoch: 0006, Batch: 063075, Sample Num: 63075, Cur Loss: 0.00046727, Cur Avg Loss: 0.00209471, Log Avg loss: 0.00214417, Global Avg Loss: 0.00457116, Time: 0.0309
Training, Epoch: 0006, Batch: 067075, Sample Num: 67075, Cur Loss: 0.04012730, Cur Avg Loss: 0.00209611, Log Avg loss: 0.00211815, Global Avg Loss: 0.00456054, Time: 0.0151
Training, Epoch: 0006, Batch: 071075, Sample Num: 71075, Cur Loss: 0.00000993, Cur Avg Loss: 0.00211043, Log Avg loss: 0.00235059, Global Avg Loss: 0.00455101, Time: 0.0142
Training, Epoch: 0006, Batch: 075075, Sample Num: 75075, Cur Loss: 0.00000001, Cur Avg Loss: 0.00212145, Log Avg loss: 0.00231722, Global Avg Loss: 0.00454143, Time: 0.0189
Training, Epoch: 0006, Batch: 079075, Sample Num: 79075, Cur Loss: 0.00015620, Cur Avg Loss: 0.00214531, Log Avg loss: 0.00259309, Global Avg Loss: 0.00453310, Time: 0.0125
Training, Epoch: 0006, Batch: 083075, Sample Num: 83075, Cur Loss: 0.00024111, Cur Avg Loss: 0.00213838, Log Avg loss: 0.00200151, Global Avg Loss: 0.00452233, Time: 0.0123
Training, Epoch: 0006, Batch: 087075, Sample Num: 87075, Cur Loss: 0.00000010, Cur Avg Loss: 0.00215314, Log Avg loss: 0.00245958, Global Avg Loss: 0.00451359, Time: 0.0150
Training, Epoch: 0006, Batch: 091075, Sample Num: 91075, Cur Loss: 0.00009465, Cur Avg Loss: 0.00215564, Log Avg loss: 0.00221006, Global Avg Loss: 0.00450387, Time: 0.0330
Training, Epoch: 0006, Batch: 095075, Sample Num: 95075, Cur Loss: 0.00000110, Cur Avg Loss: 0.00215355, Log Avg loss: 0.00210616, Global Avg Loss: 0.00449379, Time: 0.0136
Training, Epoch: 0006, Batch: 099075, Sample Num: 99075, Cur Loss: 0.00108295, Cur Avg Loss: 0.00214287, Log Avg loss: 0.00188888, Global Avg Loss: 0.00448289, Time: 0.0122
Training, Epoch: 0006, Batch: 103075, Sample Num: 103075, Cur Loss: 0.00000139, Cur Avg Loss: 0.00214397, Log Avg loss: 0.00217124, Global Avg Loss: 0.00447326, Time: 0.0150
Training, Epoch: 0006, Batch: 107075, Sample Num: 107075, Cur Loss: 0.00041720, Cur Avg Loss: 0.00214154, Log Avg loss: 0.00207882, Global Avg Loss: 0.00446333, Time: 0.0132
Training, Epoch: 0006, Batch: 111075, Sample Num: 111075, Cur Loss: 0.00000088, Cur Avg Loss: 0.00214406, Log Avg loss: 0.00221174, Global Avg Loss: 0.00445402, Time: 0.0128
Training, Epoch: 0006, Batch: 115075, Sample Num: 115075, Cur Loss: 0.00000000, Cur Avg Loss: 0.00215510, Log Avg loss: 0.00246158, Global Avg Loss: 0.00444582, Time: 0.0196
Training, Epoch: 0006, Batch: 119075, Sample Num: 119075, Cur Loss: 0.00016804, Cur Avg Loss: 0.00216475, Log Avg loss: 0.00244243, Global Avg Loss: 0.00443761, Time: 0.0126
Training, Epoch: 0006, Batch: 123075, Sample Num: 123075, Cur Loss: 0.00000001, Cur Avg Loss: 0.00217200, Log Avg loss: 0.00238767, Global Avg Loss: 0.00442925, Time: 0.0138
Training, Epoch: 0006, Batch: 127075, Sample Num: 127075, Cur Loss: 0.00000000, Cur Avg Loss: 0.00217728, Log Avg loss: 0.00233971, Global Avg Loss: 0.00442075, Time: 0.0187
Training, Epoch: 0006, Batch: 131075, Sample Num: 131075, Cur Loss: 0.00000000, Cur Avg Loss: 0.00217684, Log Avg loss: 0.00216290, Global Avg Loss: 0.00441161, Time: 0.0144
Training, Epoch: 0006, Batch: 135075, Sample Num: 135075, Cur Loss: 0.00000073, Cur Avg Loss: 0.00218467, Log Avg loss: 0.00244129, Global Avg Loss: 0.00440367, Time: 0.0142
Training, Epoch: 0006, Batch: 139075, Sample Num: 139075, Cur Loss: 0.00000066, Cur Avg Loss: 0.00218647, Log Avg loss: 0.00224717, Global Avg Loss: 0.00439500, Time: 0.0127
Training, Epoch: 0006, Batch: 143075, Sample Num: 143075, Cur Loss: 0.00000000, Cur Avg Loss: 0.00219487, Log Avg loss: 0.00248705, Global Avg Loss: 0.00438737, Time: 0.0124
Training, Epoch: 0006, Batch: 147075, Sample Num: 147075, Cur Loss: 0.00000165, Cur Avg Loss: 0.00220047, Log Avg loss: 0.00240080, Global Avg Loss: 0.00437946, Time: 0.0148
Training, Epoch: 0006, Batch: 151075, Sample Num: 151075, Cur Loss: 0.00347198, Cur Avg Loss: 0.00220373, Log Avg loss: 0.00232344, Global Avg Loss: 0.00437130, Time: 0.0132
Training, Epoch: 0006, Batch: 155075, Sample Num: 155075, Cur Loss: 0.00022820, Cur Avg Loss: 0.00220682, Log Avg loss: 0.00232376, Global Avg Loss: 0.00436321, Time: 0.0139
Training, Epoch: 0006, Batch: 159075, Sample Num: 159075, Cur Loss: 0.00000006, Cur Avg Loss: 0.00220799, Log Avg loss: 0.00225325, Global Avg Loss: 0.00435490, Time: 0.0246
Training, Epoch: 0006, Batch: 163075, Sample Num: 163075, Cur Loss: 0.00000305, Cur Avg Loss: 0.00220507, Log Avg loss: 0.00208893, Global Avg Loss: 0.00434601, Time: 0.0122
Training, Epoch: 0006, Batch: 167075, Sample Num: 167075, Cur Loss: 0.00000039, Cur Avg Loss: 0.00220263, Log Avg loss: 0.00210322, Global Avg Loss: 0.00433725, Time: 0.0282
Training, Epoch: 0006, Batch: 171075, Sample Num: 171075, Cur Loss: 0.00000002, Cur Avg Loss: 0.00221187, Log Avg loss: 0.00259792, Global Avg Loss: 0.00433048, Time: 0.0235
***** Running evaluation checkpoint-1028310 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-1028310 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 2863.961398, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.112779, "eval_total_loss": 2413.911507, "eval_acc": 0.960101, "eval_prec": 0.971386, "eval_recall": 0.971479, "eval_f1": 0.970813, "eval_top2_acc": 0.994394, "eval_top3_acc": 0.998925, "eval_top5_acc": 0.99986, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999534, "eval_pr_auc": 0.988289, "eval_mcc": 0.956015, "eval_sn": 0.971479, "eval_sp": 0.998454, "update_flag": false, "test_avg_loss": 0.119652, "test_total_loss": 2561.039243, "test_acc": 0.958045, "test_prec": 0.977605, "test_recall": 0.976996, "test_f1": 0.97691, "test_top2_acc": 0.993553, "test_top3_acc": 0.998879, "test_top5_acc": 0.999953, "test_top10_acc": 1.0, "test_roc_auc": 0.999472, "test_pr_auc": 0.991578, "test_mcc": 0.953892, "test_sn": 0.976996, "test_sp": 0.998364, "lr": 0.00017733197410443242, "cur_epoch_step": 171385, "train_global_avg_loss": 0.004329824208286894, "train_cur_epoch_loss": 379.05992840636674, "train_cur_epoch_avg_loss": 0.002211745067575148, "train_cur_epoch_time": 2863.9613978862762, "train_cur_epoch_avg_time": 0.01671068878773683, "epoch": 6, "step": 1028310}
##################################################
Training, Epoch: 0007, Batch: 003690, Sample Num: 3690, Cur Loss: 0.00001230, Cur Avg Loss: 0.00166579, Log Avg loss: 0.00170255, Global Avg Loss: 0.00432030, Time: 0.0140
Training, Epoch: 0007, Batch: 007690, Sample Num: 7690, Cur Loss: 0.00000415, Cur Avg Loss: 0.00163980, Log Avg loss: 0.00161583, Global Avg Loss: 0.00430986, Time: 0.0342
Training, Epoch: 0007, Batch: 011690, Sample Num: 11690, Cur Loss: 0.00000051, Cur Avg Loss: 0.00169749, Log Avg loss: 0.00180840, Global Avg Loss: 0.00430024, Time: 0.0213
Training, Epoch: 0007, Batch: 015690, Sample Num: 15690, Cur Loss: 0.00000025, Cur Avg Loss: 0.00174222, Log Avg loss: 0.00187292, Global Avg Loss: 0.00429094, Time: 0.0129
Training, Epoch: 0007, Batch: 019690, Sample Num: 19690, Cur Loss: 0.00000000, Cur Avg Loss: 0.00174572, Log Avg loss: 0.00175945, Global Avg Loss: 0.00428127, Time: 0.0122
Training, Epoch: 0007, Batch: 023690, Sample Num: 23690, Cur Loss: 0.00000000, Cur Avg Loss: 0.00172799, Log Avg loss: 0.00164074, Global Avg Loss: 0.00427123, Time: 0.0191
Training, Epoch: 0007, Batch: 027690, Sample Num: 27690, Cur Loss: 0.00000165, Cur Avg Loss: 0.00174081, Log Avg loss: 0.00181671, Global Avg Loss: 0.00426194, Time: 0.0127
Training, Epoch: 0007, Batch: 031690, Sample Num: 31690, Cur Loss: 0.00000000, Cur Avg Loss: 0.00177951, Log Avg loss: 0.00204739, Global Avg Loss: 0.00425358, Time: 0.0146
Training, Epoch: 0007, Batch: 035690, Sample Num: 35690, Cur Loss: 0.00000000, Cur Avg Loss: 0.00180407, Log Avg loss: 0.00199866, Global Avg Loss: 0.00424510, Time: 0.0187
Training, Epoch: 0007, Batch: 039690, Sample Num: 39690, Cur Loss: 0.00000000, Cur Avg Loss: 0.00180236, Log Avg loss: 0.00178709, Global Avg Loss: 0.00423590, Time: 0.0137
Training, Epoch: 0007, Batch: 043690, Sample Num: 43690, Cur Loss: 0.00000000, Cur Avg Loss: 0.00180487, Log Avg loss: 0.00182978, Global Avg Loss: 0.00422692, Time: 0.0161
Training, Epoch: 0007, Batch: 047690, Sample Num: 47690, Cur Loss: 0.00601008, Cur Avg Loss: 0.00184383, Log Avg loss: 0.00226940, Global Avg Loss: 0.00421964, Time: 0.0123
Training, Epoch: 0007, Batch: 051690, Sample Num: 51690, Cur Loss: 0.00000942, Cur Avg Loss: 0.00183099, Log Avg loss: 0.00167785, Global Avg Loss: 0.00421023, Time: 0.0186
Training, Epoch: 0007, Batch: 055690, Sample Num: 55690, Cur Loss: 0.00000022, Cur Avg Loss: 0.00183656, Log Avg loss: 0.00190859, Global Avg Loss: 0.00420173, Time: 0.0129
Training, Epoch: 0007, Batch: 059690, Sample Num: 59690, Cur Loss: 0.00099451, Cur Avg Loss: 0.00183224, Log Avg loss: 0.00177217, Global Avg Loss: 0.00419280, Time: 0.0127
Training, Epoch: 0007, Batch: 063690, Sample Num: 63690, Cur Loss: 0.00000150, Cur Avg Loss: 0.00184934, Log Avg loss: 0.00210448, Global Avg Loss: 0.00418515, Time: 0.0126
Training, Epoch: 0007, Batch: 067690, Sample Num: 67690, Cur Loss: 0.00886471, Cur Avg Loss: 0.00184654, Log Avg loss: 0.00180192, Global Avg Loss: 0.00417645, Time: 0.0130
Training, Epoch: 0007, Batch: 071690, Sample Num: 71690, Cur Loss: 0.00000111, Cur Avg Loss: 0.00186158, Log Avg loss: 0.00211617, Global Avg Loss: 0.00416896, Time: 0.0168
Training, Epoch: 0007, Batch: 075690, Sample Num: 75690, Cur Loss: 0.00000256, Cur Avg Loss: 0.00186060, Log Avg loss: 0.00184305, Global Avg Loss: 0.00416053, Time: 0.0274
Training, Epoch: 0007, Batch: 079690, Sample Num: 79690, Cur Loss: 0.00000000, Cur Avg Loss: 0.00188519, Log Avg loss: 0.00235035, Global Avg Loss: 0.00415400, Time: 0.0141
Training, Epoch: 0007, Batch: 083690, Sample Num: 83690, Cur Loss: 0.00000000, Cur Avg Loss: 0.00187872, Log Avg loss: 0.00174993, Global Avg Loss: 0.00414535, Time: 0.0149
Training, Epoch: 0007, Batch: 087690, Sample Num: 87690, Cur Loss: 0.00000000, Cur Avg Loss: 0.00187751, Log Avg loss: 0.00185221, Global Avg Loss: 0.00413713, Time: 0.0127
Training, Epoch: 0007, Batch: 091690, Sample Num: 91690, Cur Loss: 0.00677606, Cur Avg Loss: 0.00188799, Log Avg loss: 0.00211763, Global Avg Loss: 0.00412992, Time: 0.0120
Training, Epoch: 0007, Batch: 095690, Sample Num: 95690, Cur Loss: 0.00007153, Cur Avg Loss: 0.00189508, Log Avg loss: 0.00205760, Global Avg Loss: 0.00412255, Time: 0.0124
Training, Epoch: 0007, Batch: 099690, Sample Num: 99690, Cur Loss: 0.00000118, Cur Avg Loss: 0.00189608, Log Avg loss: 0.00191996, Global Avg Loss: 0.00411474, Time: 0.0127
Training, Epoch: 0007, Batch: 103690, Sample Num: 103690, Cur Loss: 0.00000000, Cur Avg Loss: 0.00189503, Log Avg loss: 0.00186898, Global Avg Loss: 0.00410680, Time: 0.0363
Training, Epoch: 0007, Batch: 107690, Sample Num: 107690, Cur Loss: 0.00000000, Cur Avg Loss: 0.00189166, Log Avg loss: 0.00180440, Global Avg Loss: 0.00409869, Time: 0.0148
Training, Epoch: 0007, Batch: 111690, Sample Num: 111690, Cur Loss: 0.00000101, Cur Avg Loss: 0.00188908, Log Avg loss: 0.00181947, Global Avg Loss: 0.00409070, Time: 0.0139
Training, Epoch: 0007, Batch: 115690, Sample Num: 115690, Cur Loss: 0.00000103, Cur Avg Loss: 0.00188809, Log Avg loss: 0.00186055, Global Avg Loss: 0.00408290, Time: 0.0230
Training, Epoch: 0007, Batch: 119690, Sample Num: 119690, Cur Loss: 0.00000000, Cur Avg Loss: 0.00188263, Log Avg loss: 0.00172462, Global Avg Loss: 0.00407468, Time: 0.0123
Training, Epoch: 0007, Batch: 123690, Sample Num: 123690, Cur Loss: 0.00000051, Cur Avg Loss: 0.00188851, Log Avg loss: 0.00206447, Global Avg Loss: 0.00406770, Time: 0.0258
Training, Epoch: 0007, Batch: 127690, Sample Num: 127690, Cur Loss: 0.00000000, Cur Avg Loss: 0.00189197, Log Avg loss: 0.00199897, Global Avg Loss: 0.00406054, Time: 0.0198
Training, Epoch: 0007, Batch: 131690, Sample Num: 131690, Cur Loss: 0.00000000, Cur Avg Loss: 0.00189807, Log Avg loss: 0.00209292, Global Avg Loss: 0.00405376, Time: 0.0143
Training, Epoch: 0007, Batch: 135690, Sample Num: 135690, Cur Loss: 0.00000000, Cur Avg Loss: 0.00189738, Log Avg loss: 0.00187448, Global Avg Loss: 0.00404627, Time: 0.0160
Training, Epoch: 0007, Batch: 139690, Sample Num: 139690, Cur Loss: 0.00000082, Cur Avg Loss: 0.00190292, Log Avg loss: 0.00209075, Global Avg Loss: 0.00403957, Time: 0.0144
Training, Epoch: 0007, Batch: 143690, Sample Num: 143690, Cur Loss: 0.02889205, Cur Avg Loss: 0.00189964, Log Avg loss: 0.00178526, Global Avg Loss: 0.00403188, Time: 0.0147
Training, Epoch: 0007, Batch: 147690, Sample Num: 147690, Cur Loss: 0.00000000, Cur Avg Loss: 0.00190028, Log Avg loss: 0.00192317, Global Avg Loss: 0.00402471, Time: 0.0152
Training, Epoch: 0007, Batch: 151690, Sample Num: 151690, Cur Loss: 0.00002861, Cur Avg Loss: 0.00189746, Log Avg loss: 0.00179351, Global Avg Loss: 0.00401714, Time: 0.0124
Training, Epoch: 0007, Batch: 155690, Sample Num: 155690, Cur Loss: 0.00000002, Cur Avg Loss: 0.00189878, Log Avg loss: 0.00194888, Global Avg Loss: 0.00401015, Time: 0.0186
Training, Epoch: 0007, Batch: 159690, Sample Num: 159690, Cur Loss: 0.00000000, Cur Avg Loss: 0.00190715, Log Avg loss: 0.00223274, Global Avg Loss: 0.00400417, Time: 0.0464
Training, Epoch: 0007, Batch: 163690, Sample Num: 163690, Cur Loss: 0.00000000, Cur Avg Loss: 0.00190522, Log Avg loss: 0.00182803, Global Avg Loss: 0.00399687, Time: 0.0172
Training, Epoch: 0007, Batch: 167690, Sample Num: 167690, Cur Loss: 0.00008523, Cur Avg Loss: 0.00190327, Log Avg loss: 0.00182368, Global Avg Loss: 0.00398960, Time: 0.0132
***** Running evaluation checkpoint-1199695 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-1199695 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 2846.109966, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.109937, "eval_total_loss": 2353.081949, "eval_acc": 0.962764, "eval_prec": 0.982486, "eval_recall": 0.969185, "eval_f1": 0.971972, "eval_top2_acc": 0.99458, "eval_top3_acc": 0.999066, "eval_top5_acc": 0.999813, "eval_top10_acc": 1.0, "eval_roc_auc": 0.999514, "eval_pr_auc": 0.990319, "eval_mcc": 0.958674, "eval_sn": 0.969185, "eval_sp": 0.998581, "update_flag": false, "test_avg_loss": 0.11122, "test_total_loss": 2380.542222, "test_acc": 0.961222, "test_prec": 0.981727, "test_recall": 0.967328, "test_f1": 0.970575, "test_top2_acc": 0.99458, "test_top3_acc": 0.998972, "test_top5_acc": 0.99986, "test_top10_acc": 0.999953, "test_roc_auc": 0.999495, "test_pr_auc": 0.9906, "test_mcc": 0.956989, "test_sn": 0.967328, "test_sp": 0.998512, "lr": 0.00017330318954848274, "cur_epoch_step": 171385, "train_global_avg_loss": 0.003982913658981955, "train_cur_epoch_loss": 325.88007048838347, "train_cur_epoch_avg_loss": 0.0019014503631495374, "train_cur_epoch_time": 2846.1099660396576, "train_cur_epoch_avg_time": 0.016606528961342345, "epoch": 7, "step": 1199695}
##################################################
Training, Epoch: 0008, Batch: 000305, Sample Num: 305, Cur Loss: 0.00000000, Cur Avg Loss: 0.00137902, Log Avg loss: 0.00178533, Global Avg Loss: 0.00398225, Time: 0.0197
Training, Epoch: 0008, Batch: 004305, Sample Num: 4305, Cur Loss: 0.00000107, Cur Avg Loss: 0.00143594, Log Avg loss: 0.00144028, Global Avg Loss: 0.00397381, Time: 0.0126
Training, Epoch: 0008, Batch: 008305, Sample Num: 8305, Cur Loss: 0.00166664, Cur Avg Loss: 0.00144162, Log Avg loss: 0.00144773, Global Avg Loss: 0.00396544, Time: 0.0126
Training, Epoch: 0008, Batch: 012305, Sample Num: 12305, Cur Loss: 0.00000017, Cur Avg Loss: 0.00144568, Log Avg loss: 0.00145412, Global Avg Loss: 0.00395715, Time: 0.0148
Training, Epoch: 0008, Batch: 016305, Sample Num: 16305, Cur Loss: 0.00010125, Cur Avg Loss: 0.00149263, Log Avg loss: 0.00163704, Global Avg Loss: 0.00394952, Time: 0.0123
Training, Epoch: 0008, Batch: 020305, Sample Num: 20305, Cur Loss: 0.00000000, Cur Avg Loss: 0.00144082, Log Avg loss: 0.00122964, Global Avg Loss: 0.00394060, Time: 0.0309
Training, Epoch: 0008, Batch: 024305, Sample Num: 24305, Cur Loss: 0.00000000, Cur Avg Loss: 0.00148872, Log Avg loss: 0.00173190, Global Avg Loss: 0.00393339, Time: 0.0125
Training, Epoch: 0008, Batch: 028305, Sample Num: 28305, Cur Loss: 0.00000027, Cur Avg Loss: 0.00150626, Log Avg loss: 0.00161285, Global Avg Loss: 0.00392583, Time: 0.0360
Training, Epoch: 0008, Batch: 032305, Sample Num: 32305, Cur Loss: 0.00000000, Cur Avg Loss: 0.00149960, Log Avg loss: 0.00145241, Global Avg Loss: 0.00391780, Time: 0.0134
Training, Epoch: 0008, Batch: 036305, Sample Num: 36305, Cur Loss: 0.00000000, Cur Avg Loss: 0.00149186, Log Avg loss: 0.00142942, Global Avg Loss: 0.00390974, Time: 0.0124
Training, Epoch: 0008, Batch: 040305, Sample Num: 40305, Cur Loss: 0.00000000, Cur Avg Loss: 0.00149409, Log Avg loss: 0.00151432, Global Avg Loss: 0.00390202, Time: 0.0146
Training, Epoch: 0008, Batch: 044305, Sample Num: 44305, Cur Loss: 0.00005302, Cur Avg Loss: 0.00149595, Log Avg loss: 0.00151472, Global Avg Loss: 0.00389434, Time: 0.0124
Training, Epoch: 0008, Batch: 048305, Sample Num: 48305, Cur Loss: 0.00770798, Cur Avg Loss: 0.00151011, Log Avg loss: 0.00166692, Global Avg Loss: 0.00388720, Time: 0.0127
Training, Epoch: 0008, Batch: 052305, Sample Num: 52305, Cur Loss: 0.00000000, Cur Avg Loss: 0.00149865, Log Avg loss: 0.00136026, Global Avg Loss: 0.00387913, Time: 0.0197
Training, Epoch: 0008, Batch: 056305, Sample Num: 56305, Cur Loss: 0.00000018, Cur Avg Loss: 0.00149631, Log Avg loss: 0.00146566, Global Avg Loss: 0.00387144, Time: 0.0124
Training, Epoch: 0008, Batch: 060305, Sample Num: 60305, Cur Loss: 0.00000079, Cur Avg Loss: 0.00149952, Log Avg loss: 0.00154479, Global Avg Loss: 0.00386406, Time: 0.0141
Training, Epoch: 0008, Batch: 064305, Sample Num: 64305, Cur Loss: 0.00148633, Cur Avg Loss: 0.00150655, Log Avg loss: 0.00161252, Global Avg Loss: 0.00385693, Time: 0.0125
Training, Epoch: 0008, Batch: 068305, Sample Num: 68305, Cur Loss: 0.00000000, Cur Avg Loss: 0.00151352, Log Avg loss: 0.00162548, Global Avg Loss: 0.00384989, Time: 0.0146
Training, Epoch: 0008, Batch: 072305, Sample Num: 72305, Cur Loss: 0.00000001, Cur Avg Loss: 0.00151526, Log Avg loss: 0.00154496, Global Avg Loss: 0.00384264, Time: 0.0150
Training, Epoch: 0008, Batch: 076305, Sample Num: 76305, Cur Loss: 0.00000000, Cur Avg Loss: 0.00152584, Log Avg loss: 0.00171723, Global Avg Loss: 0.00383598, Time: 0.0131
Training, Epoch: 0008, Batch: 080305, Sample Num: 80305, Cur Loss: 0.00000811, Cur Avg Loss: 0.00154070, Log Avg loss: 0.00182415, Global Avg Loss: 0.00382969, Time: 0.0142
Training, Epoch: 0008, Batch: 084305, Sample Num: 84305, Cur Loss: 0.00001482, Cur Avg Loss: 0.00153348, Log Avg loss: 0.00138842, Global Avg Loss: 0.00382209, Time: 0.0129
Training, Epoch: 0008, Batch: 088305, Sample Num: 88305, Cur Loss: 0.00007112, Cur Avg Loss: 0.00153626, Log Avg loss: 0.00159493, Global Avg Loss: 0.00381517, Time: 0.0126
Training, Epoch: 0008, Batch: 092305, Sample Num: 92305, Cur Loss: 0.00000000, Cur Avg Loss: 0.00154608, Log Avg loss: 0.00176293, Global Avg Loss: 0.00380882, Time: 0.0218
Training, Epoch: 0008, Batch: 096305, Sample Num: 96305, Cur Loss: 0.00000000, Cur Avg Loss: 0.00154769, Log Avg loss: 0.00158473, Global Avg Loss: 0.00380195, Time: 0.0123
Training, Epoch: 0008, Batch: 100305, Sample Num: 100305, Cur Loss: 0.00348082, Cur Avg Loss: 0.00155649, Log Avg loss: 0.00176846, Global Avg Loss: 0.00379570, Time: 0.0127
Training, Epoch: 0008, Batch: 104305, Sample Num: 104305, Cur Loss: 0.00000516, Cur Avg Loss: 0.00156434, Log Avg loss: 0.00176102, Global Avg Loss: 0.00378946, Time: 0.0125
Training, Epoch: 0008, Batch: 108305, Sample Num: 108305, Cur Loss: 0.00004507, Cur Avg Loss: 0.00156477, Log Avg loss: 0.00157621, Global Avg Loss: 0.00378269, Time: 0.0122
Training, Epoch: 0008, Batch: 112305, Sample Num: 112305, Cur Loss: 0.00000000, Cur Avg Loss: 0.00157552, Log Avg loss: 0.00186638, Global Avg Loss: 0.00377684, Time: 0.0121
Training, Epoch: 0008, Batch: 116305, Sample Num: 116305, Cur Loss: 0.00006040, Cur Avg Loss: 0.00158529, Log Avg loss: 0.00185964, Global Avg Loss: 0.00377102, Time: 0.0124
Training, Epoch: 0008, Batch: 120305, Sample Num: 120305, Cur Loss: 0.00002415, Cur Avg Loss: 0.00159066, Log Avg loss: 0.00174671, Global Avg Loss: 0.00376488, Time: 0.0143
Training, Epoch: 0008, Batch: 124305, Sample Num: 124305, Cur Loss: 0.00000000, Cur Avg Loss: 0.00160502, Log Avg loss: 0.00203706, Global Avg Loss: 0.00375966, Time: 0.0144
Training, Epoch: 0008, Batch: 128305, Sample Num: 128305, Cur Loss: 0.00000000, Cur Avg Loss: 0.00160686, Log Avg loss: 0.00166388, Global Avg Loss: 0.00375335, Time: 0.0152
Training, Epoch: 0008, Batch: 132305, Sample Num: 132305, Cur Loss: 0.00000049, Cur Avg Loss: 0.00161518, Log Avg loss: 0.00188234, Global Avg Loss: 0.00374773, Time: 0.0142
Training, Epoch: 0008, Batch: 136305, Sample Num: 136305, Cur Loss: 0.00000000, Cur Avg Loss: 0.00161714, Log Avg loss: 0.00168184, Global Avg Loss: 0.00374155, Time: 0.0145
Training, Epoch: 0008, Batch: 140305, Sample Num: 140305, Cur Loss: 0.00000001, Cur Avg Loss: 0.00162350, Log Avg loss: 0.00184019, Global Avg Loss: 0.00373587, Time: 0.0193
Training, Epoch: 0008, Batch: 144305, Sample Num: 144305, Cur Loss: 0.00002550, Cur Avg Loss: 0.00163500, Log Avg loss: 0.00203830, Global Avg Loss: 0.00373082, Time: 0.0123
Training, Epoch: 0008, Batch: 148305, Sample Num: 148305, Cur Loss: 0.00000042, Cur Avg Loss: 0.00163511, Log Avg loss: 0.00163920, Global Avg Loss: 0.00372461, Time: 0.0138
Training, Epoch: 0008, Batch: 152305, Sample Num: 152305, Cur Loss: 0.00100135, Cur Avg Loss: 0.00163694, Log Avg loss: 0.00170479, Global Avg Loss: 0.00371864, Time: 0.0156
Training, Epoch: 0008, Batch: 156305, Sample Num: 156305, Cur Loss: 0.00000000, Cur Avg Loss: 0.00163752, Log Avg loss: 0.00165954, Global Avg Loss: 0.00371256, Time: 0.0141
Training, Epoch: 0008, Batch: 160305, Sample Num: 160305, Cur Loss: 0.04018227, Cur Avg Loss: 0.00163560, Log Avg loss: 0.00156044, Global Avg Loss: 0.00370623, Time: 0.0176
Training, Epoch: 0008, Batch: 164305, Sample Num: 164305, Cur Loss: 0.00000015, Cur Avg Loss: 0.00163639, Log Avg loss: 0.00166841, Global Avg Loss: 0.00370026, Time: 0.0270
Training, Epoch: 0008, Batch: 168305, Sample Num: 168305, Cur Loss: 0.00000009, Cur Avg Loss: 0.00164150, Log Avg loss: 0.00185121, Global Avg Loss: 0.00369485, Time: 0.0149
***** Running evaluation checkpoint-1371080 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-1371080 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 2694.776695, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.11249, "eval_total_loss": 2407.737409, "eval_acc": 0.963418, "eval_prec": 0.983819, "eval_recall": 0.969153, "eval_f1": 0.972673, "eval_top2_acc": 0.994113, "eval_top3_acc": 0.998879, "eval_top5_acc": 0.999813, "eval_top10_acc": 1.0, "eval_roc_auc": 0.99952, "eval_pr_auc": 0.991902, "eval_mcc": 0.959378, "eval_sn": 0.969153, "eval_sp": 0.9986, "update_flag": true, "test_avg_loss": 0.112234, "test_total_loss": 2402.253486, "test_acc": 0.964259, "test_prec": 0.982996, "test_recall": 0.967878, "test_f1": 0.97152, "test_top2_acc": 0.994767, "test_top3_acc": 0.999019, "test_top5_acc": 0.999907, "test_top10_acc": 1.0, "test_roc_auc": 0.999484, "test_pr_auc": 0.991026, "test_mcc": 0.96032, "test_sn": 0.967878, "test_sp": 0.998628, "lr": 0.00016927440499253302, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0036905115395806823, "train_cur_epoch_loss": 281.7049595747725, "train_cur_epoch_avg_loss": 0.0016436967037650465, "train_cur_epoch_time": 2694.7766954898834, "train_cur_epoch_avg_time": 0.015723527120167363, "epoch": 8, "step": 1371080}
##################################################
Training, Epoch: 0009, Batch: 000920, Sample Num: 920, Cur Loss: 0.00000083, Cur Avg Loss: 0.00074489, Log Avg loss: 0.00152939, Global Avg Loss: 0.00368854, Time: 0.0126
Steps: 1373000, Updated lr: 0.000169
Training, Epoch: 0009, Batch: 004920, Sample Num: 4920, Cur Loss: 0.00000000, Cur Avg Loss: 0.00105522, Log Avg loss: 0.00112660, Global Avg Loss: 0.00368109, Time: 0.0136
Steps: 1377000, Updated lr: 0.000169
Training, Epoch: 0009, Batch: 008920, Sample Num: 8920, Cur Loss: 0.00032286, Cur Avg Loss: 0.00105754, Log Avg loss: 0.00106038, Global Avg Loss: 0.00367349, Time: 0.0128
Steps: 1381000, Updated lr: 0.000169
Training, Epoch: 0009, Batch: 012920, Sample Num: 12920, Cur Loss: 0.00000206, Cur Avg Loss: 0.00108387, Log Avg loss: 0.00114258, Global Avg Loss: 0.00366618, Time: 0.0126
Steps: 1385000, Updated lr: 0.000169
Training, Epoch: 0009, Batch: 016920, Sample Num: 16920, Cur Loss: 0.00000000, Cur Avg Loss: 0.00109786, Log Avg loss: 0.00114306, Global Avg Loss: 0.00365891, Time: 0.0182
Steps: 1389000, Updated lr: 0.000169
Training, Epoch: 0009, Batch: 020920, Sample Num: 20920, Cur Loss: 0.00000001, Cur Avg Loss: 0.00115429, Log Avg loss: 0.00139301, Global Avg Loss: 0.00365240, Time: 0.0126
Steps: 1393000, Updated lr: 0.000169
Training, Epoch: 0009, Batch: 024920, Sample Num: 24920, Cur Loss: 0.00001406, Cur Avg Loss: 0.00119800, Log Avg loss: 0.00142656, Global Avg Loss: 0.00364602, Time: 0.0185
Steps: 1397000, Updated lr: 0.000169
Training, Epoch: 0009, Batch: 028920, Sample Num: 28920, Cur Loss: 0.00000010, Cur Avg Loss: 0.00119097, Log Avg loss: 0.00114722, Global Avg Loss: 0.00363888, Time: 0.0128
Steps: 1401000, Updated lr: 0.000169
Training, Epoch: 0009, Batch: 032920, Sample Num: 32920, Cur Loss: 0.00000000, Cur Avg Loss: 0.00122515, Log Avg loss: 0.00147226, Global Avg Loss: 0.00363271, Time: 0.0123
Steps: 1405000, Updated lr: 0.000168
Training, Epoch: 0009, Batch: 036920, Sample Num: 36920, Cur Loss: 0.00000000, Cur Avg Loss: 0.00122161, Log Avg loss: 0.00119245, Global Avg Loss: 0.00362577, Time: 0.0148
Steps: 1409000, Updated lr: 0.000168
Training, Epoch: 0009, Batch: 040920, Sample Num: 40920, Cur Loss: 0.00000048, Cur Avg Loss: 0.00122981, Log Avg loss: 0.00130552, Global Avg Loss: 0.00361920, Time: 0.0123
Steps: 1413000, Updated lr: 0.000168
Training, Epoch: 0009, Batch: 044920, Sample Num: 44920, Cur Loss: 0.00051066, Cur Avg Loss: 0.00125058, Log Avg loss: 0.00146303, Global Avg Loss: 0.00361311, Time: 0.0146
Steps: 1417000, Updated lr: 0.000168
Training, Epoch: 0009, Batch: 048920, Sample Num: 48920, Cur Loss: 0.00000067, Cur Avg Loss: 0.00126069, Log Avg loss: 0.00137430, Global Avg Loss: 0.00360680, Time: 0.0157
Steps: 1421000, Updated lr: 0.000168
Training, Epoch: 0009, Batch: 052920, Sample Num: 52920, Cur Loss: 0.00000017, Cur Avg Loss: 0.00125500, Log Avg loss: 0.00118539, Global Avg Loss: 0.00360000, Time: 0.0142
Steps: 1425000, Updated lr: 0.000168
Training, Epoch: 0009, Batch: 056920, Sample Num: 56920, Cur Loss: 0.00067844, Cur Avg Loss: 0.00128098, Log Avg loss: 0.00162468, Global Avg Loss: 0.00359447, Time: 0.0121
Steps: 1429000, Updated lr: 0.000168
Training, Epoch: 0009, Batch: 060920, Sample Num: 60920, Cur Loss: 0.00000001, Cur Avg Loss: 0.00128761, Log Avg loss: 0.00138199, Global Avg Loss: 0.00358829, Time: 0.0162
Steps: 1433000, Updated lr: 0.000168
Training, Epoch: 0009, Batch: 064920, Sample Num: 64920, Cur Loss: 0.06871039, Cur Avg Loss: 0.00129701, Log Avg loss: 0.00144015, Global Avg Loss: 0.00358230, Time: 0.0139
Steps: 1437000, Updated lr: 0.000168
Training, Epoch: 0009, Batch: 068920, Sample Num: 68920, Cur Loss: 0.00000006, Cur Avg Loss: 0.00131486, Log Avg loss: 0.00160452, Global Avg Loss: 0.00357681, Time: 0.0129
Steps: 1441000, Updated lr: 0.000168
Training, Epoch: 0009, Batch: 072920, Sample Num: 72920, Cur Loss: 0.00000000, Cur Avg Loss: 0.00132184, Log Avg loss: 0.00144209, Global Avg Loss: 0.00357090, Time: 0.0124
Steps: 1445000, Updated lr: 0.000168
Training, Epoch: 0009, Batch: 076920, Sample Num: 76920, Cur Loss: 0.00001232, Cur Avg Loss: 0.00132749, Log Avg loss: 0.00143043, Global Avg Loss: 0.00356498, Time: 0.0194
Steps: 1449000, Updated lr: 0.000167
Training, Epoch: 0009, Batch: 080920, Sample Num: 80920, Cur Loss: 0.00000009, Cur Avg Loss: 0.00133503, Log Avg loss: 0.00148004, Global Avg Loss: 0.00355924, Time: 0.0144
Steps: 1453000, Updated lr: 0.000167
Training, Epoch: 0009, Batch: 084920, Sample Num: 84920, Cur Loss: 0.00013640, Cur Avg Loss: 0.00135309, Log Avg loss: 0.00171857, Global Avg Loss: 0.00355418, Time: 0.0125
Steps: 1457000, Updated lr: 0.000167
Training, Epoch: 0009, Batch: 088920, Sample Num: 88920, Cur Loss: 0.00000000, Cur Avg Loss: 0.00135199, Log Avg loss: 0.00132857, Global Avg Loss: 0.00354809, Time: 0.0136
Steps: 1461000, Updated lr: 0.000167
Training, Epoch: 0009, Batch: 092920, Sample Num: 92920, Cur Loss: 0.00000000, Cur Avg Loss: 0.00136244, Log Avg loss: 0.00159484, Global Avg Loss: 0.00354275, Time: 0.0275
Steps: 1465000, Updated lr: 0.000167
Training, Epoch: 0009, Batch: 096920, Sample Num: 96920, Cur Loss: 0.00000014, Cur Avg Loss: 0.00135578, Log Avg loss: 0.00120101, Global Avg Loss: 0.00353637, Time: 0.0365
Steps: 1469000, Updated lr: 0.000167
Training, Epoch: 0009, Batch: 100920, Sample Num: 100920, Cur Loss: 0.00000000, Cur Avg Loss: 0.00135722, Log Avg loss: 0.00139216, Global Avg Loss: 0.00353054, Time: 0.0173
Steps: 1473000, Updated lr: 0.000167
Training, Epoch: 0009, Batch: 104920, Sample Num: 104920, Cur Loss: 0.00000009, Cur Avg Loss: 0.00137878, Log Avg loss: 0.00192276, Global Avg Loss: 0.00352618, Time: 0.0146
Steps: 1477000, Updated lr: 0.000167
Training, Epoch: 0009, Batch: 108920, Sample Num: 108920, Cur Loss: 0.00000005, Cur Avg Loss: 0.00137596, Log Avg loss: 0.00130196, Global Avg Loss: 0.00352017, Time: 0.0127
Steps: 1481000, Updated lr: 0.000167
Training, Epoch: 0009, Batch: 112920, Sample Num: 112920, Cur Loss: 0.00000000, Cur Avg Loss: 0.00137733, Log Avg loss: 0.00141449, Global Avg Loss: 0.00351450, Time: 0.0144
Steps: 1485000, Updated lr: 0.000167
Training, Epoch: 0009, Batch: 116920, Sample Num: 116920, Cur Loss: 0.00000000, Cur Avg Loss: 0.00137880, Log Avg loss: 0.00142023, Global Avg Loss: 0.00350887, Time: 0.0161
Steps: 1489000, Updated lr: 0.000167
Training, Epoch: 0009, Batch: 120920, Sample Num: 120920, Cur Loss: 0.00000001, Cur Avg Loss: 0.00138301, Log Avg loss: 0.00150616, Global Avg Loss: 0.00350350, Time: 0.0154
Steps: 1493000, Updated lr: 0.000166
Training, Epoch: 0009, Batch: 124920, Sample Num: 124920, Cur Loss: 0.00000000, Cur Avg Loss: 0.00138063, Log Avg loss: 0.00130868, Global Avg Loss: 0.00349763, Time: 0.0148
Steps: 1497000, Updated lr: 0.000166
Training, Epoch: 0009, Batch: 128920, Sample Num: 128920, Cur Loss: 0.00019466, Cur Avg Loss: 0.00138428, Log Avg loss: 0.00149816, Global Avg Loss: 0.00349230, Time: 0.0139
Steps: 1501000, Updated lr: 0.000166
Training, Epoch: 0009, Batch: 132920, Sample Num: 132920, Cur Loss: 0.00000704, Cur Avg Loss: 0.00138514, Log Avg loss: 0.00141287, Global Avg Loss: 0.00348677, Time: 0.0149
Steps: 1505000, Updated lr: 0.000166
Training, Epoch: 0009, Batch: 136920, Sample Num: 136920, Cur Loss: 0.00000003, Cur Avg Loss: 0.00137791, Log Avg loss: 0.00113794, Global Avg Loss: 0.00348054, Time: 0.0123
Steps: 1509000, Updated lr: 0.000166
Training, Epoch: 0009, Batch: 140920, Sample Num: 140920, Cur Loss: 0.00000000, Cur Avg Loss: 0.00137510, Log Avg loss: 0.00127871, Global Avg Loss: 0.00347471, Time: 0.0192
Steps: 1513000, Updated lr: 0.000166
Training, Epoch: 0009, Batch: 144920, Sample Num: 144920, Cur Loss: 0.00013056, Cur Avg Loss: 0.00138248, Log Avg loss: 0.00164241, Global Avg Loss: 0.00346988, Time: 0.0123
Steps: 1517000, Updated lr: 0.000166
Training, Epoch: 0009, Batch: 148920, Sample Num: 148920, Cur Loss: 0.00000000, Cur Avg Loss: 0.00138824, Log Avg loss: 0.00159691, Global Avg Loss: 0.00346495, Time: 0.0141
Steps: 1521000, Updated lr: 0.000166
Training, Epoch: 0009, Batch: 152920, Sample Num: 152920, Cur Loss: 0.00000000, Cur Avg Loss: 0.00138464, Log Avg loss: 0.00125084, Global Avg Loss: 0.00345914, Time: 0.0191
Steps: 1525000, Updated lr: 0.000166
Training, Epoch: 0009, Batch: 156920, Sample Num: 156920, Cur Loss: 0.00000000, Cur Avg Loss: 0.00138941, Log Avg loss: 0.00157171, Global Avg Loss: 0.00345420, Time: 0.0121
Steps: 1529000, Updated lr: 0.000166
Training, Epoch: 0009, Batch: 160920, Sample Num: 160920, Cur Loss: 0.00000001, Cur Avg Loss: 0.00138826, Log Avg loss: 0.00134302, Global Avg Loss: 0.00344868, Time: 0.0150
Steps: 1533000, Updated lr: 0.000165
Training, Epoch: 0009, Batch: 164920, Sample Num: 164920, Cur Loss: 0.00032247, Cur Avg Loss: 0.00139353, Log Avg loss: 0.00160552, Global Avg Loss: 0.00344388, Time: 0.0127
Steps: 1537000, Updated lr: 0.000165
Training, Epoch: 0009, Batch: 168920, Sample Num: 168920, Cur Loss: 0.00000000, Cur Avg Loss: 0.00140608, Log Avg loss: 0.00192372, Global Avg Loss: 0.00343994, Time: 0.0132
Steps: 1541000, Updated lr: 0.000165
***** Running evaluation checkpoint-1542465 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-1542465 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 2649.254956, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.117426, "eval_total_loss": 2513.395874, "eval_acc": 0.963184, "eval_prec": 0.979686, "eval_recall": 0.970511, "eval_f1": 0.974381, "eval_top2_acc": 0.994767, "eval_top3_acc": 0.998972, "eval_top5_acc": 0.999813, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999509, "eval_pr_auc": 0.988521, "eval_mcc": 0.959118, "eval_sn": 0.970511, "eval_sp": 0.998576, "update_flag": true, "test_avg_loss": 0.118414, "test_total_loss": 2534.52473, "test_acc": 0.961269, "test_prec": 0.980933, "test_recall": 0.973722, "test_f1": 0.977054, "test_top2_acc": 0.994487, "test_top3_acc": 0.998879, "test_top5_acc": 0.999907, "test_top10_acc": 1.0, "test_roc_auc": 0.999483, "test_pr_auc": 0.992355, "test_mcc": 0.956996, "test_sn": 0.973722, "test_sp": 0.998496, "lr": 0.00016524562043658333, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0034365097552744688, "train_cur_epoch_loss": 240.70945797973923, "train_cur_epoch_avg_loss": 0.0014044954808165196, "train_cur_epoch_time": 2649.254955768585, "train_cur_epoch_avg_time": 0.015457916128999534, "epoch": 9, "step": 1542465}
##################################################
Training, Epoch: 0010, Batch: 001535, Sample Num: 1535, Cur Loss: 0.00000006, Cur Avg Loss: 0.00085577, Log Avg loss: 0.00112693, Global Avg Loss: 0.00343394, Time: 0.0259
Training, Epoch: 0010, Batch: 005535, Sample Num: 5535, Cur Loss: 0.00000000, Cur Avg Loss: 0.00098888, Log Avg loss: 0.00103996, Global Avg Loss: 0.00342776, Time: 0.0192
Training, Epoch: 0010, Batch: 009535, Sample Num: 9535, Cur Loss: 0.00000067, Cur Avg Loss: 0.00108650, Log Avg loss: 0.00122158, Global Avg Loss: 0.00342207, Time: 0.0143
Training, Epoch: 0010, Batch: 013535, Sample Num: 13535, Cur Loss: 0.00059605, Cur Avg Loss: 0.00107163, Log Avg loss: 0.00103618, Global Avg Loss: 0.00341594, Time: 0.0152
Training, Epoch: 0010, Batch: 017535, Sample Num: 17535, Cur Loss: 0.00000000, Cur Avg Loss: 0.00105755, Log Avg loss: 0.00100992, Global Avg Loss: 0.00340977, Time: 0.0134
Training, Epoch: 0010, Batch: 021535, Sample Num: 21535, Cur Loss: 0.00010974, Cur Avg Loss: 0.00104697, Log Avg loss: 0.00100059, Global Avg Loss: 0.00340361, Time: 0.0220
Training, Epoch: 0010, Batch: 025535, Sample Num: 25535, Cur Loss: 0.00000086, Cur Avg Loss: 0.00107556, Log Avg loss: 0.00122945, Global Avg Loss: 0.00339806, Time: 0.0209
Training, Epoch: 0010, Batch: 029535, Sample Num: 29535, Cur Loss: 0.00000000, Cur Avg Loss: 0.00108125, Log Avg loss: 0.00111759, Global Avg Loss: 0.00339226, Time: 0.0144
Training, Epoch: 0010, Batch: 033535, Sample Num: 33535, Cur Loss: 0.00000885, Cur Avg Loss: 0.00107640, Log Avg loss: 0.00104060, Global Avg Loss: 0.00338629, Time: 0.0386
Training, Epoch: 0010, Batch: 037535, Sample Num: 37535, Cur Loss: 0.00000000, Cur Avg Loss: 0.00109531, Log Avg loss: 0.00125385, Global Avg Loss: 0.00338089, Time: 0.0489
Training, Epoch: 0010, Batch: 041535, Sample Num: 41535, Cur Loss: 0.00007063, Cur Avg Loss: 0.00111283, Log Avg loss: 0.00127719, Global Avg Loss: 0.00337558, Time: 0.0330
Training, Epoch: 0010, Batch: 045535, Sample Num: 45535, Cur Loss: 0.00000003, Cur Avg Loss: 0.00110984, Log Avg loss: 0.00107880, Global Avg Loss: 0.00336979, Time: 0.0166
Training, Epoch: 0010, Batch: 049535, Sample Num: 49535, Cur Loss: 0.00000000, Cur Avg Loss: 0.00111877, Log Avg loss: 0.00122049, Global Avg Loss: 0.00336439, Time: 0.0277
Training, Epoch: 0010, Batch: 053535, Sample Num: 53535, Cur Loss: 0.00005376, Cur Avg Loss: 0.00113560, Log Avg loss: 0.00134398, Global Avg Loss: 0.00335933, Time: 0.0289
Training, Epoch: 0010, Batch: 057535, Sample Num: 57535, Cur Loss: 0.00000028, Cur Avg Loss: 0.00114779, Log Avg loss: 0.00131094, Global Avg Loss: 0.00335421, Time: 0.0146
Training, Epoch: 0010, Batch: 061535, Sample Num: 61535, Cur Loss: 0.00002510, Cur Avg Loss: 0.00115021, Log Avg loss: 0.00118496, Global Avg Loss: 0.00334880, Time: 0.0599
Training, Epoch: 0010, Batch: 065535, Sample Num: 65535, Cur Loss: 0.00003859, Cur Avg Loss: 0.00115482, Log Avg loss: 0.00122579, Global Avg Loss: 0.00334352, Time: 0.0159
Training, Epoch: 0010, Batch: 069535, Sample Num: 69535, Cur Loss: 0.00001773, Cur Avg Loss: 0.00117303, Log Avg loss: 0.00147133, Global Avg Loss: 0.00333887, Time: 0.0416
Training, Epoch: 0010, Batch: 073535, Sample Num: 73535, Cur Loss: 0.00000397, Cur Avg Loss: 0.00117711, Log Avg loss: 0.00124807, Global Avg Loss: 0.00333370, Time: 0.0179
Training, Epoch: 0010, Batch: 077535, Sample Num: 77535, Cur Loss: 0.00015786, Cur Avg Loss: 0.00118240, Log Avg loss: 0.00127958, Global Avg Loss: 0.00332863, Time: 0.0124
Training, Epoch: 0010, Batch: 081535, Sample Num: 81535, Cur Loss: 0.00000120, Cur Avg Loss: 0.00119370, Log Avg loss: 0.00141273, Global Avg Loss: 0.00332391, Time: 0.0152
Training, Epoch: 0010, Batch: 085535, Sample Num: 85535, Cur Loss: 0.00000000, Cur Avg Loss: 0.00119038, Log Avg loss: 0.00112276, Global Avg Loss: 0.00331850, Time: 0.0283
Training, Epoch: 0010, Batch: 089535, Sample Num: 89535, Cur Loss: 0.00000240, Cur Avg Loss: 0.00120322, Log Avg loss: 0.00147791, Global Avg Loss: 0.00331399, Time: 0.0127
Training, Epoch: 0010, Batch: 093535, Sample Num: 93535, Cur Loss: 0.00015679, Cur Avg Loss: 0.00121095, Log Avg loss: 0.00138393, Global Avg Loss: 0.00330927, Time: 0.0286
Training, Epoch: 0010, Batch: 097535, Sample Num: 97535, Cur Loss: 0.00000000, Cur Avg Loss: 0.00120578, Log Avg loss: 0.00108481, Global Avg Loss: 0.00330384, Time: 0.0203
Training, Epoch: 0010, Batch: 101535, Sample Num: 101535, Cur Loss: 0.00020944, Cur Avg Loss: 0.00120501, Log Avg loss: 0.00118632, Global Avg Loss: 0.00329869, Time: 0.0124
Training, Epoch: 0010, Batch: 105535, Sample Num: 105535, Cur Loss: 0.00000065, Cur Avg Loss: 0.00119936, Log Avg loss: 0.00105584, Global Avg Loss: 0.00329325, Time: 0.0129
Training, Epoch: 0010, Batch: 109535, Sample Num: 109535, Cur Loss: 0.00000000, Cur Avg Loss: 0.00119486, Log Avg loss: 0.00107611, Global Avg Loss: 0.00328788, Time: 0.0156
Training, Epoch: 0010, Batch: 113535, Sample Num: 113535, Cur Loss: 0.00000000, Cur Avg Loss: 0.00119868, Log Avg loss: 0.00130324, Global Avg Loss: 0.00328308, Time: 0.0268
Training, Epoch: 0010, Batch: 117535, Sample Num: 117535, Cur Loss: 0.00000067, Cur Avg Loss: 0.00120364, Log Avg loss: 0.00134464, Global Avg Loss: 0.00327841, Time: 0.0381
Training, Epoch: 0010, Batch: 121535, Sample Num: 121535, Cur Loss: 0.00000010, Cur Avg Loss: 0.00120903, Log Avg loss: 0.00136719, Global Avg Loss: 0.00327382, Time: 0.0147
Training, Epoch: 0010, Batch: 125535, Sample Num: 125535, Cur Loss: 0.00000000, Cur Avg Loss: 0.00120732, Log Avg loss: 0.00115562, Global Avg Loss: 0.00326874, Time: 0.0125
Training, Epoch: 0010, Batch: 129535, Sample Num: 129535, Cur Loss: 0.00000000, Cur Avg Loss: 0.00120997, Log Avg loss: 0.00129314, Global Avg Loss: 0.00326401, Time: 0.0220
Training, Epoch: 0010, Batch: 133535, Sample Num: 133535, Cur Loss: 0.00000000, Cur Avg Loss: 0.00122094, Log Avg loss: 0.00157612, Global Avg Loss: 0.00325998, Time: 0.0449
Training, Epoch: 0010, Batch: 137535, Sample Num: 137535, Cur Loss: 0.00000000, Cur Avg Loss: 0.00122666, Log Avg loss: 0.00141771, Global Avg Loss: 0.00325560, Time: 0.0155
Training, Epoch: 0010, Batch: 141535, Sample Num: 141535, Cur Loss: 0.00000010, Cur Avg Loss: 0.00122785, Log Avg loss: 0.00126874, Global Avg Loss: 0.00325088, Time: 0.0133
Training, Epoch: 0010, Batch: 145535, Sample Num: 145535, Cur Loss: 0.00000000, Cur Avg Loss: 0.00122646, Log Avg loss: 0.00117723, Global Avg Loss: 0.00324597, Time: 0.0123
Training, Epoch: 0010, Batch: 149535, Sample Num: 149535, Cur Loss: 0.00000001, Cur Avg Loss: 0.00122800, Log Avg loss: 0.00128379, Global Avg Loss: 0.00324133, Time: 0.0185
Training, Epoch: 0010, Batch: 153535, Sample Num: 153535, Cur Loss: 0.00000000, Cur Avg Loss: 0.00122844, Log Avg loss: 0.00124518, Global Avg Loss: 0.00323662, Time: 0.0143
Training, Epoch: 0010, Batch: 157535, Sample Num: 157535, Cur Loss: 0.00000014, Cur Avg Loss: 0.00123605, Log Avg loss: 0.00152789, Global Avg Loss: 0.00323260, Time: 0.0147
Training, Epoch: 0010, Batch: 161535, Sample Num: 161535, Cur Loss: 0.00030161, Cur Avg Loss: 0.00124115, Log Avg loss: 0.00144222, Global Avg Loss: 0.00322840, Time: 0.0137
Training, Epoch: 0010, Batch: 165535, Sample Num: 165535, Cur Loss: 0.00050654, Cur Avg Loss: 0.00124032, Log Avg loss: 0.00120674, Global Avg Loss: 0.00322366, Time: 0.0124
Training, Epoch: 0010, Batch: 169535, Sample Num: 169535, Cur Loss: 0.00000000, Cur Avg Loss: 0.00123913, Log Avg loss: 0.00118991, Global Avg Loss: 0.00321891, Time: 0.0141
***** Running evaluation checkpoint-1713850 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-1713850 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3570.676289, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.126404, "eval_total_loss": 2705.550894, "eval_acc": 0.963091, "eval_prec": 0.973216, "eval_recall": 0.970865, "eval_f1": 0.971899, "eval_top2_acc": 0.994767, "eval_top3_acc": 0.998832, "eval_top5_acc": 0.99986, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999514, "eval_pr_auc": 0.986968, "eval_mcc": 0.959039, "eval_sn": 0.970865, "eval_sp": 0.99859, "update_flag": false, "test_avg_loss": 0.125831, "test_total_loss": 2693.281891, "test_acc": 0.961549, "test_prec": 0.979385, "test_recall": 0.975431, "test_f1": 0.977196, "test_top2_acc": 0.994113, "test_top3_acc": 0.998832, "test_top5_acc": 1.0, "test_top10_acc": 1.0, "test_roc_auc": 0.999492, "test_pr_auc": 0.991453, "test_mcc": 0.957345, "test_sn": 0.975431, "test_sp": 0.99852, "lr": 0.00016121683588063364, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0032166547535309683, "train_cur_epoch_loss": 212.16772966840583, "train_cur_epoch_avg_loss": 0.0012379597378323996, "train_cur_epoch_time": 3570.676289319992, "train_cur_epoch_avg_time": 0.020834240390465865, "epoch": 10, "step": 1713850}
##################################################
Training, Epoch: 0011, Batch: 002150, Sample Num: 2150, Cur Loss: 0.00001237, Cur Avg Loss: 0.00106553, Log Avg loss: 0.00109563, Global Avg Loss: 0.00321396, Time: 0.0161
Training, Epoch: 0011, Batch: 006150, Sample Num: 6150, Cur Loss: 0.00000012, Cur Avg Loss: 0.00110647, Log Avg loss: 0.00112848, Global Avg Loss: 0.00320911, Time: 0.0184
Training, Epoch: 0011, Batch: 010150, Sample Num: 10150, Cur Loss: 0.00000000, Cur Avg Loss: 0.00096918, Log Avg loss: 0.00075809, Global Avg Loss: 0.00320342, Time: 0.0281
Training, Epoch: 0011, Batch: 014150, Sample Num: 14150, Cur Loss: 0.00000028, Cur Avg Loss: 0.00095638, Log Avg loss: 0.00092391, Global Avg Loss: 0.00319815, Time: 0.0176
Training, Epoch: 0011, Batch: 018150, Sample Num: 18150, Cur Loss: 0.00000019, Cur Avg Loss: 0.00091045, Log Avg loss: 0.00074797, Global Avg Loss: 0.00319249, Time: 0.0515
Training, Epoch: 0011, Batch: 022150, Sample Num: 22150, Cur Loss: 0.00000000, Cur Avg Loss: 0.00093589, Log Avg loss: 0.00105132, Global Avg Loss: 0.00318755, Time: 0.0139
Training, Epoch: 0011, Batch: 026150, Sample Num: 26150, Cur Loss: 0.00030324, Cur Avg Loss: 0.00094485, Log Avg loss: 0.00099447, Global Avg Loss: 0.00318251, Time: 0.0208
Training, Epoch: 0011, Batch: 030150, Sample Num: 30150, Cur Loss: 0.00001454, Cur Avg Loss: 0.00097917, Log Avg loss: 0.00120350, Global Avg Loss: 0.00317797, Time: 0.0288
Training, Epoch: 0011, Batch: 034150, Sample Num: 34150, Cur Loss: 0.00000001, Cur Avg Loss: 0.00096860, Log Avg loss: 0.00088893, Global Avg Loss: 0.00317274, Time: 0.0126
Training, Epoch: 0011, Batch: 038150, Sample Num: 38150, Cur Loss: 0.00000408, Cur Avg Loss: 0.00097089, Log Avg loss: 0.00099048, Global Avg Loss: 0.00316775, Time: 0.0247
Training, Epoch: 0011, Batch: 042150, Sample Num: 42150, Cur Loss: 0.00000000, Cur Avg Loss: 0.00097408, Log Avg loss: 0.00100450, Global Avg Loss: 0.00316283, Time: 0.0128
Training, Epoch: 0011, Batch: 046150, Sample Num: 46150, Cur Loss: 0.00021707, Cur Avg Loss: 0.00098453, Log Avg loss: 0.00109469, Global Avg Loss: 0.00315813, Time: 0.0127
Training, Epoch: 0011, Batch: 050150, Sample Num: 50150, Cur Loss: 0.00119313, Cur Avg Loss: 0.00098337, Log Avg loss: 0.00096998, Global Avg Loss: 0.00315316, Time: 0.0289
Training, Epoch: 0011, Batch: 054150, Sample Num: 54150, Cur Loss: 0.00000236, Cur Avg Loss: 0.00098730, Log Avg loss: 0.00103649, Global Avg Loss: 0.00314837, Time: 0.0138
Training, Epoch: 0011, Batch: 058150, Sample Num: 58150, Cur Loss: 0.00000000, Cur Avg Loss: 0.00098664, Log Avg loss: 0.00097780, Global Avg Loss: 0.00314347, Time: 0.0161
Training, Epoch: 0011, Batch: 062150, Sample Num: 62150, Cur Loss: 0.00000000, Cur Avg Loss: 0.00097917, Log Avg loss: 0.00087048, Global Avg Loss: 0.00313836, Time: 0.0190
Training, Epoch: 0011, Batch: 066150, Sample Num: 66150, Cur Loss: 0.01688495, Cur Avg Loss: 0.00099072, Log Avg loss: 0.00117026, Global Avg Loss: 0.00313393, Time: 0.0126
Training, Epoch: 0011, Batch: 070150, Sample Num: 70150, Cur Loss: 0.00000313, Cur Avg Loss: 0.00098624, Log Avg loss: 0.00091215, Global Avg Loss: 0.00312895, Time: 0.0150
Training, Epoch: 0011, Batch: 074150, Sample Num: 74150, Cur Loss: 0.00005204, Cur Avg Loss: 0.00098759, Log Avg loss: 0.00101119, Global Avg Loss: 0.00312421, Time: 0.0130
Training, Epoch: 0011, Batch: 078150, Sample Num: 78150, Cur Loss: 0.00000003, Cur Avg Loss: 0.00100120, Log Avg loss: 0.00125353, Global Avg Loss: 0.00312004, Time: 0.0320
Training, Epoch: 0011, Batch: 082150, Sample Num: 82150, Cur Loss: 0.00000000, Cur Avg Loss: 0.00100245, Log Avg loss: 0.00102696, Global Avg Loss: 0.00311538, Time: 0.0567
Training, Epoch: 0011, Batch: 086150, Sample Num: 86150, Cur Loss: 0.00000000, Cur Avg Loss: 0.00100407, Log Avg loss: 0.00103725, Global Avg Loss: 0.00311076, Time: 0.0186
Training, Epoch: 0011, Batch: 090150, Sample Num: 90150, Cur Loss: 0.00003569, Cur Avg Loss: 0.00100523, Log Avg loss: 0.00103022, Global Avg Loss: 0.00310614, Time: 0.0138
Training, Epoch: 0011, Batch: 094150, Sample Num: 94150, Cur Loss: 0.00000040, Cur Avg Loss: 0.00101396, Log Avg loss: 0.00121065, Global Avg Loss: 0.00310195, Time: 0.0133
Training, Epoch: 0011, Batch: 098150, Sample Num: 98150, Cur Loss: 0.00184279, Cur Avg Loss: 0.00102476, Log Avg loss: 0.00127903, Global Avg Loss: 0.00309793, Time: 0.0126
Training, Epoch: 0011, Batch: 102150, Sample Num: 102150, Cur Loss: 0.00147683, Cur Avg Loss: 0.00102659, Log Avg loss: 0.00107150, Global Avg Loss: 0.00309346, Time: 0.0178
Training, Epoch: 0011, Batch: 106150, Sample Num: 106150, Cur Loss: 0.00000002, Cur Avg Loss: 0.00103391, Log Avg loss: 0.00122079, Global Avg Loss: 0.00308935, Time: 0.0125
Training, Epoch: 0011, Batch: 110150, Sample Num: 110150, Cur Loss: 0.01059110, Cur Avg Loss: 0.00104033, Log Avg loss: 0.00121072, Global Avg Loss: 0.00308523, Time: 0.0136
Training, Epoch: 0011, Batch: 114150, Sample Num: 114150, Cur Loss: 0.00000004, Cur Avg Loss: 0.00104658, Log Avg loss: 0.00121857, Global Avg Loss: 0.00308114, Time: 0.0142
Training, Epoch: 0011, Batch: 118150, Sample Num: 118150, Cur Loss: 0.00000010, Cur Avg Loss: 0.00105229, Log Avg loss: 0.00121544, Global Avg Loss: 0.00307707, Time: 0.0341
Training, Epoch: 0011, Batch: 122150, Sample Num: 122150, Cur Loss: 0.00000000, Cur Avg Loss: 0.00105231, Log Avg loss: 0.00105279, Global Avg Loss: 0.00307266, Time: 0.0126
Training, Epoch: 0011, Batch: 126150, Sample Num: 126150, Cur Loss: 0.00000099, Cur Avg Loss: 0.00104973, Log Avg loss: 0.00097095, Global Avg Loss: 0.00306809, Time: 0.0126
Training, Epoch: 0011, Batch: 130150, Sample Num: 130150, Cur Loss: 0.00000000, Cur Avg Loss: 0.00105615, Log Avg loss: 0.00125851, Global Avg Loss: 0.00306417, Time: 0.0232
Training, Epoch: 0011, Batch: 134150, Sample Num: 134150, Cur Loss: 0.00000000, Cur Avg Loss: 0.00106534, Log Avg loss: 0.00136439, Global Avg Loss: 0.00306049, Time: 0.0146
Training, Epoch: 0011, Batch: 138150, Sample Num: 138150, Cur Loss: 0.00000020, Cur Avg Loss: 0.00107177, Log Avg loss: 0.00128744, Global Avg Loss: 0.00305666, Time: 0.0153
Training, Epoch: 0011, Batch: 142150, Sample Num: 142150, Cur Loss: 0.00000000, Cur Avg Loss: 0.00107688, Log Avg loss: 0.00125330, Global Avg Loss: 0.00305277, Time: 0.0131
Training, Epoch: 0011, Batch: 146150, Sample Num: 146150, Cur Loss: 0.00000159, Cur Avg Loss: 0.00108126, Log Avg loss: 0.00123721, Global Avg Loss: 0.00304887, Time: 0.0126
Training, Epoch: 0011, Batch: 150150, Sample Num: 150150, Cur Loss: 0.00000009, Cur Avg Loss: 0.00108551, Log Avg loss: 0.00124053, Global Avg Loss: 0.00304499, Time: 0.0143
Training, Epoch: 0011, Batch: 154150, Sample Num: 154150, Cur Loss: 0.00000000, Cur Avg Loss: 0.00108722, Log Avg loss: 0.00115160, Global Avg Loss: 0.00304093, Time: 0.0331
Training, Epoch: 0011, Batch: 158150, Sample Num: 158150, Cur Loss: 0.00000003, Cur Avg Loss: 0.00108783, Log Avg loss: 0.00111116, Global Avg Loss: 0.00303681, Time: 0.0130
Training, Epoch: 0011, Batch: 162150, Sample Num: 162150, Cur Loss: 0.00409927, Cur Avg Loss: 0.00109004, Log Avg loss: 0.00117771, Global Avg Loss: 0.00303284, Time: 0.0223
Training, Epoch: 0011, Batch: 166150, Sample Num: 166150, Cur Loss: 0.00000136, Cur Avg Loss: 0.00109186, Log Avg loss: 0.00116540, Global Avg Loss: 0.00302887, Time: 0.0174
Training, Epoch: 0011, Batch: 170150, Sample Num: 170150, Cur Loss: 0.00000000, Cur Avg Loss: 0.00109299, Log Avg loss: 0.00113992, Global Avg Loss: 0.00302486, Time: 0.0293
***** Running evaluation checkpoint-1885235 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-1885235 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3492.813158, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.124148, "eval_total_loss": 2657.262582, "eval_acc": 0.964679, "eval_prec": 0.972618, "eval_recall": 0.972443, "eval_f1": 0.972058, "eval_top2_acc": 0.994907, "eval_top3_acc": 0.998879, "eval_top5_acc": 0.999813, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999521, "eval_pr_auc": 0.990178, "eval_mcc": 0.960769, "eval_sn": 0.972443, "eval_sp": 0.998642, "update_flag": false, "test_avg_loss": 0.122007, "test_total_loss": 2611.441674, "test_acc": 0.964913, "test_prec": 0.979169, "test_recall": 0.978733, "test_f1": 0.978777, "test_top2_acc": 0.994487, "test_top3_acc": 0.998552, "test_top5_acc": 0.999907, "test_top10_acc": 1.0, "test_roc_auc": 0.999496, "test_pr_auc": 0.994012, "test_mcc": 0.961061, "test_sn": 0.978733, "test_sp": 0.998647, "lr": 0.00015718805132468392, "cur_epoch_step": 171385, "train_global_avg_loss": 0.003023602394454163, "train_cur_epoch_loss": 187.33731076802346, "train_cur_epoch_avg_loss": 0.0010930788036760712, "train_cur_epoch_time": 3492.8131577968597, "train_cur_epoch_avg_time": 0.02037992331765825, "epoch": 11, "step": 1885235}
##################################################
Training, Epoch: 0012, Batch: 002765, Sample Num: 2765, Cur Loss: 0.06289952, Cur Avg Loss: 0.00079071, Log Avg loss: 0.00088792, Global Avg Loss: 0.00302033, Time: 0.0126
Training, Epoch: 0012, Batch: 006765, Sample Num: 6765, Cur Loss: 0.00000000, Cur Avg Loss: 0.00077049, Log Avg loss: 0.00075652, Global Avg Loss: 0.00301555, Time: 0.0126
Training, Epoch: 0012, Batch: 010765, Sample Num: 10765, Cur Loss: 0.00000000, Cur Avg Loss: 0.00072559, Log Avg loss: 0.00064966, Global Avg Loss: 0.00301055, Time: 0.0142
Training, Epoch: 0012, Batch: 014765, Sample Num: 14765, Cur Loss: 0.00000000, Cur Avg Loss: 0.00074600, Log Avg loss: 0.00080090, Global Avg Loss: 0.00300590, Time: 0.0372
Training, Epoch: 0012, Batch: 018765, Sample Num: 18765, Cur Loss: 0.00007202, Cur Avg Loss: 0.00076656, Log Avg loss: 0.00084246, Global Avg Loss: 0.00300136, Time: 0.0117
Training, Epoch: 0012, Batch: 022765, Sample Num: 22765, Cur Loss: 0.00000031, Cur Avg Loss: 0.00078095, Log Avg loss: 0.00084844, Global Avg Loss: 0.00299684, Time: 0.0143
Training, Epoch: 0012, Batch: 026765, Sample Num: 26765, Cur Loss: 0.00000000, Cur Avg Loss: 0.00079271, Log Avg loss: 0.00085965, Global Avg Loss: 0.00299237, Time: 0.0199
Training, Epoch: 0012, Batch: 030765, Sample Num: 30765, Cur Loss: 0.00000089, Cur Avg Loss: 0.00078079, Log Avg loss: 0.00070104, Global Avg Loss: 0.00298759, Time: 0.0484
Training, Epoch: 0012, Batch: 034765, Sample Num: 34765, Cur Loss: 0.00000587, Cur Avg Loss: 0.00078605, Log Avg loss: 0.00082648, Global Avg Loss: 0.00298309, Time: 0.0205
Training, Epoch: 0012, Batch: 038765, Sample Num: 38765, Cur Loss: 0.00000001, Cur Avg Loss: 0.00080548, Log Avg loss: 0.00097441, Global Avg Loss: 0.00297891, Time: 0.0180
Training, Epoch: 0012, Batch: 042765, Sample Num: 42765, Cur Loss: 0.00000059, Cur Avg Loss: 0.00080639, Log Avg loss: 0.00081520, Global Avg Loss: 0.00297442, Time: 0.0242
Training, Epoch: 0012, Batch: 046765, Sample Num: 46765, Cur Loss: 0.00000000, Cur Avg Loss: 0.00083419, Log Avg loss: 0.00113139, Global Avg Loss: 0.00297061, Time: 0.0252
Training, Epoch: 0012, Batch: 050765, Sample Num: 50765, Cur Loss: 0.00000927, Cur Avg Loss: 0.00086205, Log Avg loss: 0.00118771, Global Avg Loss: 0.00296692, Time: 0.0336
Training, Epoch: 0012, Batch: 054765, Sample Num: 54765, Cur Loss: 0.00000000, Cur Avg Loss: 0.00087131, Log Avg loss: 0.00098884, Global Avg Loss: 0.00296284, Time: 0.0316
Training, Epoch: 0012, Batch: 058765, Sample Num: 58765, Cur Loss: 0.00000002, Cur Avg Loss: 0.00087714, Log Avg loss: 0.00095698, Global Avg Loss: 0.00295872, Time: 0.0152
Training, Epoch: 0012, Batch: 062765, Sample Num: 62765, Cur Loss: 0.00000000, Cur Avg Loss: 0.00087954, Log Avg loss: 0.00091481, Global Avg Loss: 0.00295452, Time: 0.0599
Training, Epoch: 0012, Batch: 066765, Sample Num: 66765, Cur Loss: 0.00000371, Cur Avg Loss: 0.00087683, Log Avg loss: 0.00083439, Global Avg Loss: 0.00295018, Time: 0.0418
Training, Epoch: 0012, Batch: 070765, Sample Num: 70765, Cur Loss: 0.00000000, Cur Avg Loss: 0.00088119, Log Avg loss: 0.00095388, Global Avg Loss: 0.00294609, Time: 0.0387
Training, Epoch: 0012, Batch: 074765, Sample Num: 74765, Cur Loss: 0.00000000, Cur Avg Loss: 0.00087456, Log Avg loss: 0.00075728, Global Avg Loss: 0.00294163, Time: 0.0152
Training, Epoch: 0012, Batch: 078765, Sample Num: 78765, Cur Loss: 0.00967037, Cur Avg Loss: 0.00088253, Log Avg loss: 0.00103156, Global Avg Loss: 0.00293774, Time: 0.0214
Training, Epoch: 0012, Batch: 082765, Sample Num: 82765, Cur Loss: 0.00000000, Cur Avg Loss: 0.00088739, Log Avg loss: 0.00098296, Global Avg Loss: 0.00293376, Time: 0.0610
Training, Epoch: 0012, Batch: 086765, Sample Num: 86765, Cur Loss: 0.00000000, Cur Avg Loss: 0.00088917, Log Avg loss: 0.00092599, Global Avg Loss: 0.00292969, Time: 0.0140
Training, Epoch: 0012, Batch: 090765, Sample Num: 90765, Cur Loss: 0.00000016, Cur Avg Loss: 0.00089411, Log Avg loss: 0.00100134, Global Avg Loss: 0.00292579, Time: 0.0361
Training, Epoch: 0012, Batch: 094765, Sample Num: 94765, Cur Loss: 0.00000060, Cur Avg Loss: 0.00090455, Log Avg loss: 0.00114151, Global Avg Loss: 0.00292218, Time: 0.0123
Training, Epoch: 0012, Batch: 098765, Sample Num: 98765, Cur Loss: 0.00000003, Cur Avg Loss: 0.00090319, Log Avg loss: 0.00087094, Global Avg Loss: 0.00291805, Time: 0.0151
Training, Epoch: 0012, Batch: 102765, Sample Num: 102765, Cur Loss: 0.00000001, Cur Avg Loss: 0.00090552, Log Avg loss: 0.00096315, Global Avg Loss: 0.00291411, Time: 0.0127
Training, Epoch: 0012, Batch: 106765, Sample Num: 106765, Cur Loss: 0.00000042, Cur Avg Loss: 0.00091464, Log Avg loss: 0.00114882, Global Avg Loss: 0.00291057, Time: 0.0132
Training, Epoch: 0012, Batch: 110765, Sample Num: 110765, Cur Loss: 0.00000038, Cur Avg Loss: 0.00092387, Log Avg loss: 0.00117016, Global Avg Loss: 0.00290708, Time: 0.0150
Training, Epoch: 0012, Batch: 114765, Sample Num: 114765, Cur Loss: 0.00000000, Cur Avg Loss: 0.00092443, Log Avg loss: 0.00094005, Global Avg Loss: 0.00290315, Time: 0.0196
Training, Epoch: 0012, Batch: 118765, Sample Num: 118765, Cur Loss: 0.01256015, Cur Avg Loss: 0.00092198, Log Avg loss: 0.00085158, Global Avg Loss: 0.00289905, Time: 0.0124
Training, Epoch: 0012, Batch: 122765, Sample Num: 122765, Cur Loss: 0.00000009, Cur Avg Loss: 0.00092481, Log Avg loss: 0.00100878, Global Avg Loss: 0.00289529, Time: 0.0129
Training, Epoch: 0012, Batch: 126765, Sample Num: 126765, Cur Loss: 0.00000000, Cur Avg Loss: 0.00093169, Log Avg loss: 0.00114300, Global Avg Loss: 0.00289180, Time: 0.0164
Training, Epoch: 0012, Batch: 130765, Sample Num: 130765, Cur Loss: 0.00000000, Cur Avg Loss: 0.00093915, Log Avg loss: 0.00117566, Global Avg Loss: 0.00288840, Time: 0.0135
Training, Epoch: 0012, Batch: 134765, Sample Num: 134765, Cur Loss: 0.00000193, Cur Avg Loss: 0.00094295, Log Avg loss: 0.00106718, Global Avg Loss: 0.00288479, Time: 0.0227
Training, Epoch: 0012, Batch: 138765, Sample Num: 138765, Cur Loss: 0.01753885, Cur Avg Loss: 0.00095097, Log Avg loss: 0.00122092, Global Avg Loss: 0.00288150, Time: 0.0125
Training, Epoch: 0012, Batch: 142765, Sample Num: 142765, Cur Loss: 0.00000002, Cur Avg Loss: 0.00095596, Log Avg loss: 0.00112905, Global Avg Loss: 0.00287805, Time: 0.0146
Training, Epoch: 0012, Batch: 146765, Sample Num: 146765, Cur Loss: 0.00000031, Cur Avg Loss: 0.00095551, Log Avg loss: 0.00093972, Global Avg Loss: 0.00287423, Time: 0.0351
Training, Epoch: 0012, Batch: 150765, Sample Num: 150765, Cur Loss: 0.00000033, Cur Avg Loss: 0.00096401, Log Avg loss: 0.00127579, Global Avg Loss: 0.00287109, Time: 0.0141
Training, Epoch: 0012, Batch: 154765, Sample Num: 154765, Cur Loss: 0.00000208, Cur Avg Loss: 0.00096410, Log Avg loss: 0.00096734, Global Avg Loss: 0.00286736, Time: 0.0126
Training, Epoch: 0012, Batch: 158765, Sample Num: 158765, Cur Loss: 0.00000004, Cur Avg Loss: 0.00096333, Log Avg loss: 0.00093358, Global Avg Loss: 0.00286357, Time: 0.0149
Training, Epoch: 0012, Batch: 162765, Sample Num: 162765, Cur Loss: 0.04816968, Cur Avg Loss: 0.00096414, Log Avg loss: 0.00099652, Global Avg Loss: 0.00285993, Time: 0.0125
Training, Epoch: 0012, Batch: 166765, Sample Num: 166765, Cur Loss: 0.00000026, Cur Avg Loss: 0.00096294, Log Avg loss: 0.00091387, Global Avg Loss: 0.00285613, Time: 0.0370
Training, Epoch: 0012, Batch: 170765, Sample Num: 170765, Cur Loss: 0.00000004, Cur Avg Loss: 0.00096125, Log Avg loss: 0.00089103, Global Avg Loss: 0.00285231, Time: 0.0178
***** Running evaluation checkpoint-2056620 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-2056620 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3662.634398, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.135126, "eval_total_loss": 2892.230686, "eval_acc": 0.96239, "eval_prec": 0.985145, "eval_recall": 0.972914, "eval_f1": 0.977074, "eval_top2_acc": 0.994207, "eval_top3_acc": 0.998832, "eval_top5_acc": 0.999766, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999503, "eval_pr_auc": 0.990452, "eval_mcc": 0.95826, "eval_sn": 0.972914, "eval_sp": 0.998561, "update_flag": true, "test_avg_loss": 0.130735, "test_total_loss": 2798.242979, "test_acc": 0.962811, "test_prec": 0.987142, "test_recall": 0.981564, "test_f1": 0.984136, "test_top2_acc": 0.994253, "test_top3_acc": 0.998785, "test_top5_acc": 0.999813, "test_top10_acc": 1.0, "test_roc_auc": 0.999507, "test_pr_auc": 0.99287, "test_mcc": 0.958695, "test_sn": 0.981564, "test_sp": 0.998567, "lr": 0.00015315926676873423, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0028517074438820073, "train_cur_epoch_loss": 164.67750312576246, "train_cur_epoch_avg_loss": 0.00096086298757629, "train_cur_epoch_time": 3662.634397506714, "train_cur_epoch_avg_time": 0.02137079906355115, "epoch": 12, "step": 2056620}
##################################################
Training, Epoch: 0013, Batch: 003380, Sample Num: 3380, Cur Loss: 0.00000867, Cur Avg Loss: 0.00058913, Log Avg loss: 0.00063008, Global Avg Loss: 0.00284800, Time: 0.0126
Training, Epoch: 0013, Batch: 007380, Sample Num: 7380, Cur Loss: 0.00000002, Cur Avg Loss: 0.00069482, Log Avg loss: 0.00078413, Global Avg Loss: 0.00284400, Time: 0.0146
Training, Epoch: 0013, Batch: 011380, Sample Num: 11380, Cur Loss: 0.00000028, Cur Avg Loss: 0.00072195, Log Avg loss: 0.00077200, Global Avg Loss: 0.00283999, Time: 0.0125
Training, Epoch: 0013, Batch: 015380, Sample Num: 15380, Cur Loss: 0.00010459, Cur Avg Loss: 0.00070212, Log Avg loss: 0.00064570, Global Avg Loss: 0.00283575, Time: 0.0163
Training, Epoch: 0013, Batch: 019380, Sample Num: 19380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00072843, Log Avg loss: 0.00082962, Global Avg Loss: 0.00283189, Time: 0.0125
Training, Epoch: 0013, Batch: 023380, Sample Num: 23380, Cur Loss: 0.00000001, Cur Avg Loss: 0.00073562, Log Avg loss: 0.00077045, Global Avg Loss: 0.00282792, Time: 0.0142
Training, Epoch: 0013, Batch: 027380, Sample Num: 27380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00074757, Log Avg loss: 0.00081737, Global Avg Loss: 0.00282406, Time: 0.0128
Training, Epoch: 0013, Batch: 031380, Sample Num: 31380, Cur Loss: 0.00483271, Cur Avg Loss: 0.00076096, Log Avg loss: 0.00085266, Global Avg Loss: 0.00282029, Time: 0.0132
Training, Epoch: 0013, Batch: 035380, Sample Num: 35380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00075120, Log Avg loss: 0.00067458, Global Avg Loss: 0.00281618, Time: 0.0125
Training, Epoch: 0013, Batch: 039380, Sample Num: 39380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00075565, Log Avg loss: 0.00079507, Global Avg Loss: 0.00281233, Time: 0.0373
Training, Epoch: 0013, Batch: 043380, Sample Num: 43380, Cur Loss: 0.00000513, Cur Avg Loss: 0.00078036, Log Avg loss: 0.00102360, Global Avg Loss: 0.00280892, Time: 0.0522
Training, Epoch: 0013, Batch: 047380, Sample Num: 47380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00077410, Log Avg loss: 0.00070621, Global Avg Loss: 0.00280492, Time: 0.0191
Training, Epoch: 0013, Batch: 051380, Sample Num: 51380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00078798, Log Avg loss: 0.00095234, Global Avg Loss: 0.00280141, Time: 0.0125
Training, Epoch: 0013, Batch: 055380, Sample Num: 55380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00079455, Log Avg loss: 0.00087899, Global Avg Loss: 0.00279777, Time: 0.0267
Training, Epoch: 0013, Batch: 059380, Sample Num: 59380, Cur Loss: 0.00000016, Cur Avg Loss: 0.00079852, Log Avg loss: 0.00085346, Global Avg Loss: 0.00279409, Time: 0.0189
Training, Epoch: 0013, Batch: 063380, Sample Num: 63380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00080486, Log Avg loss: 0.00089906, Global Avg Loss: 0.00279051, Time: 0.0310
Training, Epoch: 0013, Batch: 067380, Sample Num: 67380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00079326, Log Avg loss: 0.00060941, Global Avg Loss: 0.00278641, Time: 0.0141
Training, Epoch: 0013, Batch: 071380, Sample Num: 71380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00080126, Log Avg loss: 0.00093594, Global Avg Loss: 0.00278293, Time: 0.0140
Training, Epoch: 0013, Batch: 075380, Sample Num: 75380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00080881, Log Avg loss: 0.00094364, Global Avg Loss: 0.00277948, Time: 0.0126
Training, Epoch: 0013, Batch: 079380, Sample Num: 79380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00080550, Log Avg loss: 0.00074319, Global Avg Loss: 0.00277566, Time: 0.0139
Training, Epoch: 0013, Batch: 083380, Sample Num: 83380, Cur Loss: 0.00012322, Cur Avg Loss: 0.00081432, Log Avg loss: 0.00098933, Global Avg Loss: 0.00277233, Time: 0.0163
Training, Epoch: 0013, Batch: 087380, Sample Num: 87380, Cur Loss: 0.00000080, Cur Avg Loss: 0.00080912, Log Avg loss: 0.00070060, Global Avg Loss: 0.00276846, Time: 0.0126
Training, Epoch: 0013, Batch: 091380, Sample Num: 91380, Cur Loss: 0.00000021, Cur Avg Loss: 0.00080227, Log Avg loss: 0.00065280, Global Avg Loss: 0.00276452, Time: 0.0150
Training, Epoch: 0013, Batch: 095380, Sample Num: 95380, Cur Loss: 0.00002917, Cur Avg Loss: 0.00080440, Log Avg loss: 0.00085306, Global Avg Loss: 0.00276097, Time: 0.0149
Training, Epoch: 0013, Batch: 099380, Sample Num: 99380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00080698, Log Avg loss: 0.00086836, Global Avg Loss: 0.00275746, Time: 0.0154
Training, Epoch: 0013, Batch: 103380, Sample Num: 103380, Cur Loss: 0.00000001, Cur Avg Loss: 0.00080846, Log Avg loss: 0.00084517, Global Avg Loss: 0.00275392, Time: 0.0171
Training, Epoch: 0013, Batch: 107380, Sample Num: 107380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00080922, Log Avg loss: 0.00082886, Global Avg Loss: 0.00275036, Time: 0.0130
Training, Epoch: 0013, Batch: 111380, Sample Num: 111380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00080708, Log Avg loss: 0.00074965, Global Avg Loss: 0.00274667, Time: 0.0144
Training, Epoch: 0013, Batch: 115380, Sample Num: 115380, Cur Loss: 0.00108679, Cur Avg Loss: 0.00080242, Log Avg loss: 0.00067264, Global Avg Loss: 0.00274285, Time: 0.0390
Training, Epoch: 0013, Batch: 119380, Sample Num: 119380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00080036, Log Avg loss: 0.00074112, Global Avg Loss: 0.00273917, Time: 0.0464
Training, Epoch: 0013, Batch: 123380, Sample Num: 123380, Cur Loss: 0.00000010, Cur Avg Loss: 0.00080080, Log Avg loss: 0.00081385, Global Avg Loss: 0.00273563, Time: 0.0311
Training, Epoch: 0013, Batch: 127380, Sample Num: 127380, Cur Loss: 0.00000210, Cur Avg Loss: 0.00079770, Log Avg loss: 0.00070198, Global Avg Loss: 0.00273191, Time: 0.0125
Training, Epoch: 0013, Batch: 131380, Sample Num: 131380, Cur Loss: 0.00000005, Cur Avg Loss: 0.00080549, Log Avg loss: 0.00105358, Global Avg Loss: 0.00272884, Time: 0.0126
Training, Epoch: 0013, Batch: 135380, Sample Num: 135380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00080600, Log Avg loss: 0.00082295, Global Avg Loss: 0.00272536, Time: 0.0160
Training, Epoch: 0013, Batch: 139380, Sample Num: 139380, Cur Loss: 0.00000262, Cur Avg Loss: 0.00081578, Log Avg loss: 0.00114684, Global Avg Loss: 0.00272249, Time: 0.0153
Training, Epoch: 0013, Batch: 143380, Sample Num: 143380, Cur Loss: 0.00000001, Cur Avg Loss: 0.00082473, Log Avg loss: 0.00113641, Global Avg Loss: 0.00271960, Time: 0.0225
Training, Epoch: 0013, Batch: 147380, Sample Num: 147380, Cur Loss: 0.00000205, Cur Avg Loss: 0.00082582, Log Avg loss: 0.00086494, Global Avg Loss: 0.00271624, Time: 0.0239
Training, Epoch: 0013, Batch: 151380, Sample Num: 151380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00082602, Log Avg loss: 0.00083340, Global Avg Loss: 0.00271283, Time: 0.0124
Training, Epoch: 0013, Batch: 155380, Sample Num: 155380, Cur Loss: 0.00000075, Cur Avg Loss: 0.00082399, Log Avg loss: 0.00074712, Global Avg Loss: 0.00270927, Time: 0.0286
Training, Epoch: 0013, Batch: 159380, Sample Num: 159380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00083092, Log Avg loss: 0.00110001, Global Avg Loss: 0.00270637, Time: 0.0122
Training, Epoch: 0013, Batch: 163380, Sample Num: 163380, Cur Loss: 0.00000000, Cur Avg Loss: 0.00083430, Log Avg loss: 0.00096895, Global Avg Loss: 0.00270324, Time: 0.0189
Training, Epoch: 0013, Batch: 167380, Sample Num: 167380, Cur Loss: 0.00000004, Cur Avg Loss: 0.00083667, Log Avg loss: 0.00093357, Global Avg Loss: 0.00270005, Time: 0.0123
Training, Epoch: 0013, Batch: 171380, Sample Num: 171380, Cur Loss: 0.00000107, Cur Avg Loss: 0.00083840, Log Avg loss: 0.00091091, Global Avg Loss: 0.00269684, Time: 0.0123
***** Running evaluation checkpoint-2228005 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-2228005 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3326.537357, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.1389, "eval_total_loss": 2973.022814, "eval_acc": 0.963418, "eval_prec": 0.974276, "eval_recall": 0.973472, "eval_f1": 0.973692, "eval_top2_acc": 0.994394, "eval_top3_acc": 0.998785, "eval_top5_acc": 0.99986, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999502, "eval_pr_auc": 0.989047, "eval_mcc": 0.959366, "eval_sn": 0.973472, "eval_sp": 0.998593, "update_flag": false, "test_avg_loss": 0.139892, "test_total_loss": 2994.25743, "test_acc": 0.963184, "test_prec": 0.982734, "test_recall": 0.97935, "test_f1": 0.980991, "test_top2_acc": 0.993833, "test_top3_acc": 0.998598, "test_top5_acc": 0.999907, "test_top10_acc": 0.999953, "test_roc_auc": 0.999482, "test_pr_auc": 0.992854, "test_mcc": 0.959112, "test_sn": 0.97935, "test_sp": 0.998574, "lr": 0.00014913048221278454, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0026968359203665975, "train_cur_epoch_loss": 143.68535151797857, "train_cur_epoch_avg_loss": 0.0008383776381712436, "train_cur_epoch_time": 3326.5373570919037, "train_cur_epoch_avg_time": 0.019409734557236068, "epoch": 13, "step": 2228005}
##################################################
Training, Epoch: 0014, Batch: 003995, Sample Num: 3995, Cur Loss: 0.00000307, Cur Avg Loss: 0.00075686, Log Avg loss: 0.00075592, Global Avg Loss: 0.00269336, Time: 0.0122
Training, Epoch: 0014, Batch: 007995, Sample Num: 7995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00081174, Log Avg loss: 0.00086654, Global Avg Loss: 0.00269010, Time: 0.0486
Training, Epoch: 0014, Batch: 011995, Sample Num: 11995, Cur Loss: 0.00038957, Cur Avg Loss: 0.00074157, Log Avg loss: 0.00060133, Global Avg Loss: 0.00268637, Time: 0.0192
Training, Epoch: 0014, Batch: 015995, Sample Num: 15995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00069837, Log Avg loss: 0.00056882, Global Avg Loss: 0.00268259, Time: 0.0199
Training, Epoch: 0014, Batch: 019995, Sample Num: 19995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00071698, Log Avg loss: 0.00079140, Global Avg Loss: 0.00267923, Time: 0.0248
Training, Epoch: 0014, Batch: 023995, Sample Num: 23995, Cur Loss: 0.00029214, Cur Avg Loss: 0.00072412, Log Avg loss: 0.00075981, Global Avg Loss: 0.00267582, Time: 0.0202
Training, Epoch: 0014, Batch: 027995, Sample Num: 27995, Cur Loss: 0.00000086, Cur Avg Loss: 0.00072604, Log Avg loss: 0.00073757, Global Avg Loss: 0.00267238, Time: 0.0188
Training, Epoch: 0014, Batch: 031995, Sample Num: 31995, Cur Loss: 0.00352187, Cur Avg Loss: 0.00074530, Log Avg loss: 0.00088006, Global Avg Loss: 0.00266921, Time: 0.0156
Training, Epoch: 0014, Batch: 035995, Sample Num: 35995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00074719, Log Avg loss: 0.00076235, Global Avg Loss: 0.00266584, Time: 0.0479
Training, Epoch: 0014, Batch: 039995, Sample Num: 39995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00073183, Log Avg loss: 0.00059363, Global Avg Loss: 0.00266218, Time: 0.0210
Training, Epoch: 0014, Batch: 043995, Sample Num: 43995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00072630, Log Avg loss: 0.00067095, Global Avg Loss: 0.00265868, Time: 0.0271
Training, Epoch: 0014, Batch: 047995, Sample Num: 47995, Cur Loss: 0.00002538, Cur Avg Loss: 0.00071916, Log Avg loss: 0.00064068, Global Avg Loss: 0.00265513, Time: 0.0182
Training, Epoch: 0014, Batch: 051995, Sample Num: 51995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00073432, Log Avg loss: 0.00091617, Global Avg Loss: 0.00265208, Time: 0.0160
Training, Epoch: 0014, Batch: 055995, Sample Num: 55995, Cur Loss: 0.00041988, Cur Avg Loss: 0.00073094, Log Avg loss: 0.00068703, Global Avg Loss: 0.00264864, Time: 0.0200
Training, Epoch: 0014, Batch: 059995, Sample Num: 59995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00072743, Log Avg loss: 0.00067823, Global Avg Loss: 0.00264519, Time: 0.0177
Training, Epoch: 0014, Batch: 063995, Sample Num: 63995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00072612, Log Avg loss: 0.00070658, Global Avg Loss: 0.00264181, Time: 0.0190
Training, Epoch: 0014, Batch: 067995, Sample Num: 67995, Cur Loss: 0.00008816, Cur Avg Loss: 0.00072936, Log Avg loss: 0.00078119, Global Avg Loss: 0.00263857, Time: 0.0220
Training, Epoch: 0014, Batch: 071995, Sample Num: 71995, Cur Loss: 0.00000016, Cur Avg Loss: 0.00072190, Log Avg loss: 0.00059507, Global Avg Loss: 0.00263502, Time: 0.0154
Training, Epoch: 0014, Batch: 075995, Sample Num: 75995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00072268, Log Avg loss: 0.00073677, Global Avg Loss: 0.00263172, Time: 0.0196
Training, Epoch: 0014, Batch: 079995, Sample Num: 79995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00072493, Log Avg loss: 0.00076756, Global Avg Loss: 0.00262849, Time: 0.0584
Training, Epoch: 0014, Batch: 083995, Sample Num: 83995, Cur Loss: 0.00000006, Cur Avg Loss: 0.00072551, Log Avg loss: 0.00073712, Global Avg Loss: 0.00262522, Time: 0.0194
Training, Epoch: 0014, Batch: 087995, Sample Num: 87995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00072237, Log Avg loss: 0.00065634, Global Avg Loss: 0.00262182, Time: 0.0222
Training, Epoch: 0014, Batch: 091995, Sample Num: 91995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00072497, Log Avg loss: 0.00078216, Global Avg Loss: 0.00261865, Time: 0.0147
Training, Epoch: 0014, Batch: 095995, Sample Num: 95995, Cur Loss: 0.00000014, Cur Avg Loss: 0.00072296, Log Avg loss: 0.00067675, Global Avg Loss: 0.00261530, Time: 0.0443
Training, Epoch: 0014, Batch: 099995, Sample Num: 99995, Cur Loss: 0.00000375, Cur Avg Loss: 0.00072460, Log Avg loss: 0.00076396, Global Avg Loss: 0.00261212, Time: 0.0232
Training, Epoch: 0014, Batch: 103995, Sample Num: 103995, Cur Loss: 0.00000013, Cur Avg Loss: 0.00073052, Log Avg loss: 0.00087860, Global Avg Loss: 0.00260915, Time: 0.0124
Training, Epoch: 0014, Batch: 107995, Sample Num: 107995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00073890, Log Avg loss: 0.00095690, Global Avg Loss: 0.00260632, Time: 0.0303
Training, Epoch: 0014, Batch: 111995, Sample Num: 111995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00074332, Log Avg loss: 0.00086240, Global Avg Loss: 0.00260334, Time: 0.0529
Training, Epoch: 0014, Batch: 115995, Sample Num: 115995, Cur Loss: 0.00000134, Cur Avg Loss: 0.00075049, Log Avg loss: 0.00095139, Global Avg Loss: 0.00260052, Time: 0.0263
Training, Epoch: 0014, Batch: 119995, Sample Num: 119995, Cur Loss: 0.00000024, Cur Avg Loss: 0.00075136, Log Avg loss: 0.00077661, Global Avg Loss: 0.00259741, Time: 0.0244
Training, Epoch: 0014, Batch: 123995, Sample Num: 123995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00075335, Log Avg loss: 0.00081313, Global Avg Loss: 0.00259438, Time: 0.0394
Training, Epoch: 0014, Batch: 127995, Sample Num: 127995, Cur Loss: 0.00000012, Cur Avg Loss: 0.00075232, Log Avg loss: 0.00072035, Global Avg Loss: 0.00259120, Time: 0.0233
Training, Epoch: 0014, Batch: 131995, Sample Num: 131995, Cur Loss: 0.00000007, Cur Avg Loss: 0.00074633, Log Avg loss: 0.00055469, Global Avg Loss: 0.00258774, Time: 0.0457
Training, Epoch: 0014, Batch: 135995, Sample Num: 135995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00074622, Log Avg loss: 0.00074263, Global Avg Loss: 0.00258462, Time: 0.0546
Training, Epoch: 0014, Batch: 139995, Sample Num: 139995, Cur Loss: 0.00176463, Cur Avg Loss: 0.00074227, Log Avg loss: 0.00060773, Global Avg Loss: 0.00258128, Time: 0.0280
Training, Epoch: 0014, Batch: 143995, Sample Num: 143995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00074100, Log Avg loss: 0.00069667, Global Avg Loss: 0.00257810, Time: 0.0272
Training, Epoch: 0014, Batch: 147995, Sample Num: 147995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00074210, Log Avg loss: 0.00078185, Global Avg Loss: 0.00257508, Time: 0.0262
Training, Epoch: 0014, Batch: 151995, Sample Num: 151995, Cur Loss: 0.00000001, Cur Avg Loss: 0.00074029, Log Avg loss: 0.00067296, Global Avg Loss: 0.00257188, Time: 0.0351
Training, Epoch: 0014, Batch: 155995, Sample Num: 155995, Cur Loss: 0.00085841, Cur Avg Loss: 0.00074152, Log Avg loss: 0.00078842, Global Avg Loss: 0.00256889, Time: 0.0318
Training, Epoch: 0014, Batch: 159995, Sample Num: 159995, Cur Loss: 0.00000002, Cur Avg Loss: 0.00074843, Log Avg loss: 0.00101789, Global Avg Loss: 0.00256629, Time: 0.0218
Training, Epoch: 0014, Batch: 163995, Sample Num: 163995, Cur Loss: 0.00000474, Cur Avg Loss: 0.00074791, Log Avg loss: 0.00072700, Global Avg Loss: 0.00256322, Time: 0.0124
Training, Epoch: 0014, Batch: 167995, Sample Num: 167995, Cur Loss: 0.00000000, Cur Avg Loss: 0.00075191, Log Avg loss: 0.00091625, Global Avg Loss: 0.00256047, Time: 0.0128
***** Running evaluation checkpoint-2399390 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-2399390 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 4804.294867, Avg time per batch (s): 0.030000
{"eval_avg_loss": 0.139951, "eval_total_loss": 2995.507225, "eval_acc": 0.963511, "eval_prec": 0.985594, "eval_recall": 0.972785, "eval_f1": 0.977239, "eval_top2_acc": 0.994487, "eval_top3_acc": 0.998925, "eval_top5_acc": 0.99986, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999494, "eval_pr_auc": 0.988228, "eval_mcc": 0.959469, "eval_sn": 0.972785, "eval_sp": 0.998596, "update_flag": true, "test_avg_loss": 0.141192, "test_total_loss": 3022.06716, "test_acc": 0.964539, "test_prec": 0.980423, "test_recall": 0.974845, "test_f1": 0.977437, "test_top2_acc": 0.994394, "test_top3_acc": 0.998832, "test_top5_acc": 0.99986, "test_top10_acc": 0.999953, "test_roc_auc": 0.999461, "test_pr_auc": 0.991927, "test_mcc": 0.96062, "test_sn": 0.974845, "test_sp": 0.998633, "lr": 0.00014510169765683483, "cur_epoch_step": 171385, "train_global_avg_loss": 0.002558045858872563, "train_cur_epoch_loss": 129.18573856204148, "train_cur_epoch_avg_loss": 0.0007537750594395162, "train_cur_epoch_time": 4804.294867038727, "train_cur_epoch_avg_time": 0.02803217823636098, "epoch": 14, "step": 2399390}
##################################################
Training, Epoch: 0015, Batch: 000610, Sample Num: 610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00057156, Log Avg loss: 0.00080413, Global Avg Loss: 0.00255754, Time: 0.0233
Training, Epoch: 0015, Batch: 004610, Sample Num: 4610, Cur Loss: 0.00000555, Cur Avg Loss: 0.00064628, Log Avg loss: 0.00065768, Global Avg Loss: 0.00255438, Time: 0.0168
Training, Epoch: 0015, Batch: 008610, Sample Num: 8610, Cur Loss: 0.00000006, Cur Avg Loss: 0.00061235, Log Avg loss: 0.00057324, Global Avg Loss: 0.00255109, Time: 0.0218
Training, Epoch: 0015, Batch: 012610, Sample Num: 12610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00057056, Log Avg loss: 0.00048061, Global Avg Loss: 0.00254766, Time: 0.0199
Training, Epoch: 0015, Batch: 016610, Sample Num: 16610, Cur Loss: 0.00000091, Cur Avg Loss: 0.00055671, Log Avg loss: 0.00051305, Global Avg Loss: 0.00254429, Time: 0.0182
Training, Epoch: 0015, Batch: 020610, Sample Num: 20610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00055398, Log Avg loss: 0.00054263, Global Avg Loss: 0.00254098, Time: 0.0582
Training, Epoch: 0015, Batch: 024610, Sample Num: 24610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00053241, Log Avg loss: 0.00042131, Global Avg Loss: 0.00253748, Time: 0.0266
Training, Epoch: 0015, Batch: 028610, Sample Num: 28610, Cur Loss: 0.00121690, Cur Avg Loss: 0.00056503, Log Avg loss: 0.00076570, Global Avg Loss: 0.00253456, Time: 0.0242
Training, Epoch: 0015, Batch: 032610, Sample Num: 32610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00056189, Log Avg loss: 0.00053943, Global Avg Loss: 0.00253128, Time: 0.0310
Training, Epoch: 0015, Batch: 036610, Sample Num: 36610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00058482, Log Avg loss: 0.00077172, Global Avg Loss: 0.00252839, Time: 0.0512
Training, Epoch: 0015, Batch: 040610, Sample Num: 40610, Cur Loss: 0.00002568, Cur Avg Loss: 0.00058091, Log Avg loss: 0.00054513, Global Avg Loss: 0.00252514, Time: 0.0330
Training, Epoch: 0015, Batch: 044610, Sample Num: 44610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00059578, Log Avg loss: 0.00074681, Global Avg Loss: 0.00252223, Time: 0.0601
Training, Epoch: 0015, Batch: 048610, Sample Num: 48610, Cur Loss: 0.00085613, Cur Avg Loss: 0.00059848, Log Avg loss: 0.00062858, Global Avg Loss: 0.00251913, Time: 0.0238
Training, Epoch: 0015, Batch: 052610, Sample Num: 52610, Cur Loss: 0.00000005, Cur Avg Loss: 0.00059927, Log Avg loss: 0.00060890, Global Avg Loss: 0.00251602, Time: 0.0318
Training, Epoch: 0015, Batch: 056610, Sample Num: 56610, Cur Loss: 0.00000009, Cur Avg Loss: 0.00062469, Log Avg loss: 0.00095900, Global Avg Loss: 0.00251348, Time: 0.0487
Training, Epoch: 0015, Batch: 060610, Sample Num: 60610, Cur Loss: 0.00000017, Cur Avg Loss: 0.00062680, Log Avg loss: 0.00065672, Global Avg Loss: 0.00251046, Time: 0.0261
Training, Epoch: 0015, Batch: 064610, Sample Num: 64610, Cur Loss: 0.00000001, Cur Avg Loss: 0.00062562, Log Avg loss: 0.00060770, Global Avg Loss: 0.00250737, Time: 0.0176
Training, Epoch: 0015, Batch: 068610, Sample Num: 68610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00063255, Log Avg loss: 0.00074445, Global Avg Loss: 0.00250452, Time: 0.0419
Training, Epoch: 0015, Batch: 072610, Sample Num: 72610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00063233, Log Avg loss: 0.00062850, Global Avg Loss: 0.00250148, Time: 0.0229
Training, Epoch: 0015, Batch: 076610, Sample Num: 76610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00063357, Log Avg loss: 0.00065617, Global Avg Loss: 0.00249850, Time: 0.0231
Training, Epoch: 0015, Batch: 080610, Sample Num: 80610, Cur Loss: 0.00000080, Cur Avg Loss: 0.00063744, Log Avg loss: 0.00071144, Global Avg Loss: 0.00249562, Time: 0.0245
Training, Epoch: 0015, Batch: 084610, Sample Num: 84610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00063346, Log Avg loss: 0.00055342, Global Avg Loss: 0.00249249, Time: 0.0393
Training, Epoch: 0015, Batch: 088610, Sample Num: 88610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00063411, Log Avg loss: 0.00064784, Global Avg Loss: 0.00248953, Time: 0.0589
Training, Epoch: 0015, Batch: 092610, Sample Num: 92610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00063739, Log Avg loss: 0.00070999, Global Avg Loss: 0.00248667, Time: 0.0747
Training, Epoch: 0015, Batch: 096610, Sample Num: 96610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00064394, Log Avg loss: 0.00079548, Global Avg Loss: 0.00248396, Time: 0.0379
Training, Epoch: 0015, Batch: 100610, Sample Num: 100610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00064622, Log Avg loss: 0.00070143, Global Avg Loss: 0.00248111, Time: 0.0154
Training, Epoch: 0015, Batch: 104610, Sample Num: 104610, Cur Loss: 0.00000003, Cur Avg Loss: 0.00064360, Log Avg loss: 0.00057756, Global Avg Loss: 0.00247807, Time: 0.0242
Training, Epoch: 0015, Batch: 108610, Sample Num: 108610, Cur Loss: 0.00000020, Cur Avg Loss: 0.00065008, Log Avg loss: 0.00081975, Global Avg Loss: 0.00247542, Time: 0.0220
Training, Epoch: 0015, Batch: 112610, Sample Num: 112610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00065561, Log Avg loss: 0.00080575, Global Avg Loss: 0.00247276, Time: 0.0139
Training, Epoch: 0015, Batch: 116610, Sample Num: 116610, Cur Loss: 0.00079727, Cur Avg Loss: 0.00065317, Log Avg loss: 0.00058439, Global Avg Loss: 0.00246976, Time: 0.0126
Training, Epoch: 0015, Batch: 120610, Sample Num: 120610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00065360, Log Avg loss: 0.00066603, Global Avg Loss: 0.00246690, Time: 0.0147
Training, Epoch: 0015, Batch: 124610, Sample Num: 124610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00065369, Log Avg loss: 0.00065658, Global Avg Loss: 0.00246403, Time: 0.0570
Training, Epoch: 0015, Batch: 128610, Sample Num: 128610, Cur Loss: 0.00000002, Cur Avg Loss: 0.00065146, Log Avg loss: 0.00058187, Global Avg Loss: 0.00246105, Time: 0.0127
Training, Epoch: 0015, Batch: 132610, Sample Num: 132610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00064930, Log Avg loss: 0.00057981, Global Avg Loss: 0.00245808, Time: 0.0127
Training, Epoch: 0015, Batch: 136610, Sample Num: 136610, Cur Loss: 0.00000035, Cur Avg Loss: 0.00064777, Log Avg loss: 0.00059711, Global Avg Loss: 0.00245514, Time: 0.0222
Training, Epoch: 0015, Batch: 140610, Sample Num: 140610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00065220, Log Avg loss: 0.00080343, Global Avg Loss: 0.00245254, Time: 0.0287
Training, Epoch: 0015, Batch: 144610, Sample Num: 144610, Cur Loss: 0.00000000, Cur Avg Loss: 0.00065236, Log Avg loss: 0.00065811, Global Avg Loss: 0.00244972, Time: 0.0163
Training, Epoch: 0015, Batch: 148610, Sample Num: 148610, Cur Loss: 0.00000667, Cur Avg Loss: 0.00065967, Log Avg loss: 0.00092403, Global Avg Loss: 0.00244732, Time: 0.0125
Training, Epoch: 0015, Batch: 152610, Sample Num: 152610, Cur Loss: 0.00000006, Cur Avg Loss: 0.00065914, Log Avg loss: 0.00063945, Global Avg Loss: 0.00244449, Time: 0.0128
Training, Epoch: 0015, Batch: 156610, Sample Num: 156610, Cur Loss: 0.00000064, Cur Avg Loss: 0.00066175, Log Avg loss: 0.00076110, Global Avg Loss: 0.00244186, Time: 0.0135
Training, Epoch: 0015, Batch: 160610, Sample Num: 160610, Cur Loss: 0.00000025, Cur Avg Loss: 0.00066694, Log Avg loss: 0.00087046, Global Avg Loss: 0.00243940, Time: 0.0127
Training, Epoch: 0015, Batch: 164610, Sample Num: 164610, Cur Loss: 0.00000039, Cur Avg Loss: 0.00066957, Log Avg loss: 0.00077489, Global Avg Loss: 0.00243680, Time: 0.0496
Training, Epoch: 0015, Batch: 168610, Sample Num: 168610, Cur Loss: 0.00000750, Cur Avg Loss: 0.00067072, Log Avg loss: 0.00071835, Global Avg Loss: 0.00243413, Time: 0.0293
***** Running evaluation checkpoint-2570775 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-2570775 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 4950.372482, Avg time per batch (s): 0.030000
{"eval_avg_loss": 0.149466, "eval_total_loss": 3199.167583, "eval_acc": 0.963605, "eval_prec": 0.973027, "eval_recall": 0.971136, "eval_f1": 0.971571, "eval_top2_acc": 0.993926, "eval_top3_acc": 0.998972, "eval_top5_acc": 0.999813, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999484, "eval_pr_auc": 0.990008, "eval_mcc": 0.959569, "eval_sn": 0.971136, "eval_sp": 0.998598, "update_flag": false, "test_avg_loss": 0.150621, "test_total_loss": 3223.885761, "test_acc": 0.962764, "test_prec": 0.982911, "test_recall": 0.978686, "test_f1": 0.980741, "test_top2_acc": 0.99402, "test_top3_acc": 0.998739, "test_top5_acc": 0.99986, "test_top10_acc": 0.999953, "test_roc_auc": 0.99945, "test_pr_auc": 0.993617, "test_mcc": 0.958633, "test_sn": 0.978686, "test_sp": 0.99856, "lr": 0.00014107291310088514, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0024322081556993782, "train_cur_epoch_loss": 114.91026814599513, "train_cur_epoch_avg_loss": 0.0006704803112640846, "train_cur_epoch_time": 4950.372481584549, "train_cur_epoch_avg_time": 0.028884514289958568, "epoch": 15, "step": 2570775}
##################################################
Training, Epoch: 0016, Batch: 001225, Sample Num: 1225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00036111, Log Avg loss: 0.00056544, Global Avg Loss: 0.00243122, Time: 0.0141
Training, Epoch: 0016, Batch: 005225, Sample Num: 5225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00037147, Log Avg loss: 0.00037464, Global Avg Loss: 0.00242803, Time: 0.0151
Training, Epoch: 0016, Batch: 009225, Sample Num: 9225, Cur Loss: 0.02647825, Cur Avg Loss: 0.00039900, Log Avg loss: 0.00043497, Global Avg Loss: 0.00242494, Time: 0.0131
Training, Epoch: 0016, Batch: 013225, Sample Num: 13225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00048576, Log Avg loss: 0.00068586, Global Avg Loss: 0.00242225, Time: 0.0144
Training, Epoch: 0016, Batch: 017225, Sample Num: 17225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00051624, Log Avg loss: 0.00061701, Global Avg Loss: 0.00241946, Time: 0.0423
Training, Epoch: 0016, Batch: 021225, Sample Num: 21225, Cur Loss: 0.00001639, Cur Avg Loss: 0.00055251, Log Avg loss: 0.00070869, Global Avg Loss: 0.00241682, Time: 0.0294
Training, Epoch: 0016, Batch: 025225, Sample Num: 25225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00054784, Log Avg loss: 0.00052305, Global Avg Loss: 0.00241390, Time: 0.0334
Training, Epoch: 0016, Batch: 029225, Sample Num: 29225, Cur Loss: 0.00002778, Cur Avg Loss: 0.00056644, Log Avg loss: 0.00068374, Global Avg Loss: 0.00241124, Time: 0.0275
Training, Epoch: 0016, Batch: 033225, Sample Num: 33225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00056044, Log Avg loss: 0.00051664, Global Avg Loss: 0.00240833, Time: 0.0201
Training, Epoch: 0016, Batch: 037225, Sample Num: 37225, Cur Loss: 0.00000001, Cur Avg Loss: 0.00056634, Log Avg loss: 0.00061533, Global Avg Loss: 0.00240558, Time: 0.0166
Training, Epoch: 0016, Batch: 041225, Sample Num: 41225, Cur Loss: 0.00000001, Cur Avg Loss: 0.00056773, Log Avg loss: 0.00058067, Global Avg Loss: 0.00240278, Time: 0.0149
Training, Epoch: 0016, Batch: 045225, Sample Num: 45225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00057394, Log Avg loss: 0.00063792, Global Avg Loss: 0.00240008, Time: 0.0138
Training, Epoch: 0016, Batch: 049225, Sample Num: 49225, Cur Loss: 0.00000001, Cur Avg Loss: 0.00056687, Log Avg loss: 0.00048697, Global Avg Loss: 0.00239716, Time: 0.0183
Training, Epoch: 0016, Batch: 053225, Sample Num: 53225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00056287, Log Avg loss: 0.00051360, Global Avg Loss: 0.00239429, Time: 0.0143
Training, Epoch: 0016, Batch: 057225, Sample Num: 57225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00056315, Log Avg loss: 0.00056695, Global Avg Loss: 0.00239151, Time: 0.0167
Training, Epoch: 0016, Batch: 061225, Sample Num: 61225, Cur Loss: 0.00000189, Cur Avg Loss: 0.00056238, Log Avg loss: 0.00055122, Global Avg Loss: 0.00238871, Time: 0.0364
Training, Epoch: 0016, Batch: 065225, Sample Num: 65225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00056260, Log Avg loss: 0.00056601, Global Avg Loss: 0.00238595, Time: 0.0187
Training, Epoch: 0016, Batch: 069225, Sample Num: 69225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00057019, Log Avg loss: 0.00069394, Global Avg Loss: 0.00238338, Time: 0.0470
Training, Epoch: 0016, Batch: 073225, Sample Num: 73225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00057564, Log Avg loss: 0.00067002, Global Avg Loss: 0.00238079, Time: 0.0271
Training, Epoch: 0016, Batch: 077225, Sample Num: 77225, Cur Loss: 0.00000006, Cur Avg Loss: 0.00057721, Log Avg loss: 0.00060592, Global Avg Loss: 0.00237811, Time: 0.0252
Training, Epoch: 0016, Batch: 081225, Sample Num: 81225, Cur Loss: 0.00000005, Cur Avg Loss: 0.00057706, Log Avg loss: 0.00057414, Global Avg Loss: 0.00237539, Time: 0.0213
Training, Epoch: 0016, Batch: 085225, Sample Num: 85225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00057232, Log Avg loss: 0.00047606, Global Avg Loss: 0.00237253, Time: 0.0352
Training, Epoch: 0016, Batch: 089225, Sample Num: 89225, Cur Loss: 0.00000002, Cur Avg Loss: 0.00057193, Log Avg loss: 0.00056364, Global Avg Loss: 0.00236981, Time: 0.0395
Training, Epoch: 0016, Batch: 093225, Sample Num: 93225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00057464, Log Avg loss: 0.00063522, Global Avg Loss: 0.00236720, Time: 0.0650
Training, Epoch: 0016, Batch: 097225, Sample Num: 97225, Cur Loss: 0.00000001, Cur Avg Loss: 0.00057812, Log Avg loss: 0.00065901, Global Avg Loss: 0.00236464, Time: 0.0290
Training, Epoch: 0016, Batch: 101225, Sample Num: 101225, Cur Loss: 0.00000004, Cur Avg Loss: 0.00058539, Log Avg loss: 0.00076210, Global Avg Loss: 0.00236224, Time: 0.0312
Training, Epoch: 0016, Batch: 105225, Sample Num: 105225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00058666, Log Avg loss: 0.00061881, Global Avg Loss: 0.00235964, Time: 0.0234
Training, Epoch: 0016, Batch: 109225, Sample Num: 109225, Cur Loss: 0.00000004, Cur Avg Loss: 0.00059173, Log Avg loss: 0.00072522, Global Avg Loss: 0.00235720, Time: 0.0141
Training, Epoch: 0016, Batch: 113225, Sample Num: 113225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00059191, Log Avg loss: 0.00059681, Global Avg Loss: 0.00235457, Time: 0.0218
Training, Epoch: 0016, Batch: 117225, Sample Num: 117225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00059153, Log Avg loss: 0.00058081, Global Avg Loss: 0.00235194, Time: 0.0445
Training, Epoch: 0016, Batch: 121225, Sample Num: 121225, Cur Loss: 0.00000068, Cur Avg Loss: 0.00058831, Log Avg loss: 0.00049394, Global Avg Loss: 0.00234917, Time: 0.0125
Training, Epoch: 0016, Batch: 125225, Sample Num: 125225, Cur Loss: 0.00000008, Cur Avg Loss: 0.00058990, Log Avg loss: 0.00063818, Global Avg Loss: 0.00234664, Time: 0.0343
Training, Epoch: 0016, Batch: 129225, Sample Num: 129225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00059116, Log Avg loss: 0.00063050, Global Avg Loss: 0.00234409, Time: 0.0136
Training, Epoch: 0016, Batch: 133225, Sample Num: 133225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00059356, Log Avg loss: 0.00067116, Global Avg Loss: 0.00234162, Time: 0.0322
Training, Epoch: 0016, Batch: 137225, Sample Num: 137225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00059819, Log Avg loss: 0.00075234, Global Avg Loss: 0.00233927, Time: 0.0235
Training, Epoch: 0016, Batch: 141225, Sample Num: 141225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00059343, Log Avg loss: 0.00043008, Global Avg Loss: 0.00233646, Time: 0.0331
Training, Epoch: 0016, Batch: 145225, Sample Num: 145225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00059282, Log Avg loss: 0.00057145, Global Avg Loss: 0.00233386, Time: 0.0122
Training, Epoch: 0016, Batch: 149225, Sample Num: 149225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00059964, Log Avg loss: 0.00084704, Global Avg Loss: 0.00233167, Time: 0.0136
Training, Epoch: 0016, Batch: 153225, Sample Num: 153225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00059982, Log Avg loss: 0.00060677, Global Avg Loss: 0.00232914, Time: 0.0134
Training, Epoch: 0016, Batch: 157225, Sample Num: 157225, Cur Loss: 0.00000001, Cur Avg Loss: 0.00059883, Log Avg loss: 0.00056065, Global Avg Loss: 0.00232654, Time: 0.0127
Training, Epoch: 0016, Batch: 161225, Sample Num: 161225, Cur Loss: 0.00000438, Cur Avg Loss: 0.00059839, Log Avg loss: 0.00058138, Global Avg Loss: 0.00232399, Time: 0.0122
Training, Epoch: 0016, Batch: 165225, Sample Num: 165225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00059981, Log Avg loss: 0.00065697, Global Avg Loss: 0.00232155, Time: 0.0144
Training, Epoch: 0016, Batch: 169225, Sample Num: 169225, Cur Loss: 0.00000000, Cur Avg Loss: 0.00059999, Log Avg loss: 0.00060734, Global Avg Loss: 0.00231905, Time: 0.0145
***** Running evaluation checkpoint-2742160 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-2742160 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 4235.115061, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.15693, "eval_total_loss": 3358.927197, "eval_acc": 0.963184, "eval_prec": 0.975882, "eval_recall": 0.971309, "eval_f1": 0.973352, "eval_top2_acc": 0.994207, "eval_top3_acc": 0.998645, "eval_top5_acc": 0.999766, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999452, "eval_pr_auc": 0.9884, "eval_mcc": 0.959096, "eval_sn": 0.971309, "eval_sp": 0.998576, "update_flag": false, "test_avg_loss": 0.153902, "test_total_loss": 3294.125825, "test_acc": 0.964586, "test_prec": 0.980204, "test_recall": 0.977621, "test_f1": 0.978713, "test_top2_acc": 0.994207, "test_top3_acc": 0.999206, "test_top5_acc": 0.999813, "test_top10_acc": 0.999953, "test_roc_auc": 0.999433, "test_pr_auc": 0.992826, "test_mcc": 0.960657, "test_sn": 0.977621, "test_sp": 0.998627, "lr": 0.00013704412854493545, "cur_epoch_step": 171385, "train_global_avg_loss": 0.002317708439317962, "train_cur_epoch_loss": 102.8674524905308, "train_cur_epoch_avg_loss": 0.0006002126935877165, "train_cur_epoch_time": 4235.115060567856, "train_cur_epoch_avg_time": 0.024711118595955633, "epoch": 16, "step": 2742160}
##################################################
Training, Epoch: 0017, Batch: 001840, Sample Num: 1840, Cur Loss: 0.00000115, Cur Avg Loss: 0.00024776, Log Avg loss: 0.00044749, Global Avg Loss: 0.00231632, Time: 0.0126
Steps: 2746000, Updated lr: 0.000137
Training, Epoch: 0017, Batch: 005840, Sample Num: 5840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00038818, Log Avg loss: 0.00045277, Global Avg Loss: 0.00231361, Time: 0.0152
Steps: 2750000, Updated lr: 0.000137
Training, Epoch: 0017, Batch: 009840, Sample Num: 9840, Cur Loss: 0.00000617, Cur Avg Loss: 0.00037130, Log Avg loss: 0.00034665, Global Avg Loss: 0.00231075, Time: 0.0382
Steps: 2754000, Updated lr: 0.000137
Training, Epoch: 0017, Batch: 013840, Sample Num: 13840, Cur Loss: 0.00012622, Cur Avg Loss: 0.00044831, Log Avg loss: 0.00063777, Global Avg Loss: 0.00230832, Time: 0.0142
Steps: 2758000, Updated lr: 0.000137
Training, Epoch: 0017, Batch: 017840, Sample Num: 17840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00044613, Log Avg loss: 0.00043859, Global Avg Loss: 0.00230561, Time: 0.0151
Steps: 2762000, Updated lr: 0.000137
Training, Epoch: 0017, Batch: 021840, Sample Num: 21840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00042455, Log Avg loss: 0.00032832, Global Avg Loss: 0.00230275, Time: 0.0139
Steps: 2766000, Updated lr: 0.000136
Training, Epoch: 0017, Batch: 025840, Sample Num: 25840, Cur Loss: 0.00000002, Cur Avg Loss: 0.00045990, Log Avg loss: 0.00065286, Global Avg Loss: 0.00230037, Time: 0.0146
Steps: 2770000, Updated lr: 0.000136
Training, Epoch: 0017, Batch: 029840, Sample Num: 29840, Cur Loss: 0.00000008, Cur Avg Loss: 0.00045758, Log Avg loss: 0.00044260, Global Avg Loss: 0.00229768, Time: 0.0501
Steps: 2774000, Updated lr: 0.000136
Training, Epoch: 0017, Batch: 033840, Sample Num: 33840, Cur Loss: 0.00000499, Cur Avg Loss: 0.00046043, Log Avg loss: 0.00048172, Global Avg Loss: 0.00229507, Time: 0.0166
Steps: 2778000, Updated lr: 0.000136
Training, Epoch: 0017, Batch: 037840, Sample Num: 37840, Cur Loss: 0.00026724, Cur Avg Loss: 0.00046514, Log Avg loss: 0.00050502, Global Avg Loss: 0.00229249, Time: 0.0128
Steps: 2782000, Updated lr: 0.000136
Training, Epoch: 0017, Batch: 041840, Sample Num: 41840, Cur Loss: 0.00000001, Cur Avg Loss: 0.00046900, Log Avg loss: 0.00050551, Global Avg Loss: 0.00228992, Time: 0.0130
Steps: 2786000, Updated lr: 0.000136
Training, Epoch: 0017, Batch: 045840, Sample Num: 45840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00046292, Log Avg loss: 0.00039928, Global Avg Loss: 0.00228721, Time: 0.0463
Steps: 2790000, Updated lr: 0.000136
Training, Epoch: 0017, Batch: 049840, Sample Num: 49840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00047007, Log Avg loss: 0.00055199, Global Avg Loss: 0.00228473, Time: 0.0352
Steps: 2794000, Updated lr: 0.000136
Training, Epoch: 0017, Batch: 053840, Sample Num: 53840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00047283, Log Avg loss: 0.00050718, Global Avg Loss: 0.00228218, Time: 0.0168
Steps: 2798000, Updated lr: 0.000136
Training, Epoch: 0017, Batch: 057840, Sample Num: 57840, Cur Loss: 0.00002647, Cur Avg Loss: 0.00047652, Log Avg loss: 0.00052630, Global Avg Loss: 0.00227967, Time: 0.0141
Steps: 2802000, Updated lr: 0.000136
Training, Epoch: 0017, Batch: 061840, Sample Num: 61840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00047852, Log Avg loss: 0.00050736, Global Avg Loss: 0.00227715, Time: 0.0131
Steps: 2806000, Updated lr: 0.000136
Training, Epoch: 0017, Batch: 065840, Sample Num: 65840, Cur Loss: 0.13922143, Cur Avg Loss: 0.00048331, Log Avg loss: 0.00055735, Global Avg Loss: 0.00227470, Time: 0.0145
Steps: 2810000, Updated lr: 0.000135
Training, Epoch: 0017, Batch: 069840, Sample Num: 69840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00048729, Log Avg loss: 0.00055276, Global Avg Loss: 0.00227225, Time: 0.0132
Steps: 2814000, Updated lr: 0.000135
Training, Epoch: 0017, Batch: 073840, Sample Num: 73840, Cur Loss: 0.00000006, Cur Avg Loss: 0.00048851, Log Avg loss: 0.00050990, Global Avg Loss: 0.00226974, Time: 0.0131
Steps: 2818000, Updated lr: 0.000135
Training, Epoch: 0017, Batch: 077840, Sample Num: 77840, Cur Loss: 0.00000420, Cur Avg Loss: 0.00049128, Log Avg loss: 0.00054244, Global Avg Loss: 0.00226729, Time: 0.0170
Steps: 2822000, Updated lr: 0.000135
Training, Epoch: 0017, Batch: 081840, Sample Num: 81840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00049491, Log Avg loss: 0.00056553, Global Avg Loss: 0.00226488, Time: 0.0264
Steps: 2826000, Updated lr: 0.000135
Training, Epoch: 0017, Batch: 085840, Sample Num: 85840, Cur Loss: 0.00000003, Cur Avg Loss: 0.00049504, Log Avg loss: 0.00049778, Global Avg Loss: 0.00226238, Time: 0.0167
Steps: 2830000, Updated lr: 0.000135
Training, Epoch: 0017, Batch: 089840, Sample Num: 89840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00049295, Log Avg loss: 0.00044808, Global Avg Loss: 0.00225982, Time: 0.0118
Steps: 2834000, Updated lr: 0.000135
Training, Epoch: 0017, Batch: 093840, Sample Num: 93840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00049403, Log Avg loss: 0.00051817, Global Avg Loss: 0.00225736, Time: 0.0156
Steps: 2838000, Updated lr: 0.000135
Training, Epoch: 0017, Batch: 097840, Sample Num: 97840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00048988, Log Avg loss: 0.00039253, Global Avg Loss: 0.00225474, Time: 0.0187
Steps: 2842000, Updated lr: 0.000135
Training, Epoch: 0017, Batch: 101840, Sample Num: 101840, Cur Loss: 0.00000716, Cur Avg Loss: 0.00049156, Log Avg loss: 0.00053271, Global Avg Loss: 0.00225232, Time: 0.0123
Steps: 2846000, Updated lr: 0.000135
Training, Epoch: 0017, Batch: 105840, Sample Num: 105840, Cur Loss: 0.00000001, Cur Avg Loss: 0.00049238, Log Avg loss: 0.00051320, Global Avg Loss: 0.00224987, Time: 0.0149
Steps: 2850000, Updated lr: 0.000135
Training, Epoch: 0017, Batch: 109840, Sample Num: 109840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00049710, Log Avg loss: 0.00062195, Global Avg Loss: 0.00224759, Time: 0.0229
Steps: 2854000, Updated lr: 0.000134
Training, Epoch: 0017, Batch: 113840, Sample Num: 113840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00049918, Log Avg loss: 0.00055645, Global Avg Loss: 0.00224522, Time: 0.0149
Steps: 2858000, Updated lr: 0.000134
Training, Epoch: 0017, Batch: 117840, Sample Num: 117840, Cur Loss: 0.00000176, Cur Avg Loss: 0.00050433, Log Avg loss: 0.00065091, Global Avg Loss: 0.00224299, Time: 0.0130
Steps: 2862000, Updated lr: 0.000134
Training, Epoch: 0017, Batch: 121840, Sample Num: 121840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00050294, Log Avg loss: 0.00046196, Global Avg Loss: 0.00224050, Time: 0.0123
Steps: 2866000, Updated lr: 0.000134
Training, Epoch: 0017, Batch: 125840, Sample Num: 125840, Cur Loss: 0.00001454, Cur Avg Loss: 0.00050412, Log Avg loss: 0.00053996, Global Avg Loss: 0.00223813, Time: 0.0147
Steps: 2870000, Updated lr: 0.000134
Training, Epoch: 0017, Batch: 129840, Sample Num: 129840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00050724, Log Avg loss: 0.00060543, Global Avg Loss: 0.00223586, Time: 0.0125
Steps: 2874000, Updated lr: 0.000134
Training, Epoch: 0017, Batch: 133840, Sample Num: 133840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00050757, Log Avg loss: 0.00051841, Global Avg Loss: 0.00223347, Time: 0.0190
Steps: 2878000, Updated lr: 0.000134
Training, Epoch: 0017, Batch: 137840, Sample Num: 137840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00051215, Log Avg loss: 0.00066538, Global Avg Loss: 0.00223129, Time: 0.0124
Steps: 2882000, Updated lr: 0.000134
Training, Epoch: 0017, Batch: 141840, Sample Num: 141840, Cur Loss: 0.00000027, Cur Avg Loss: 0.00051668, Log Avg loss: 0.00067275, Global Avg Loss: 0.00222913, Time: 0.0373
Steps: 2886000, Updated lr: 0.000134
Training, Epoch: 0017, Batch: 145840, Sample Num: 145840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00051467, Log Avg loss: 0.00044325, Global Avg Loss: 0.00222666, Time: 0.0465
Steps: 2890000, Updated lr: 0.000134
Training, Epoch: 0017, Batch: 149840, Sample Num: 149840, Cur Loss: 0.00000013, Cur Avg Loss: 0.00051160, Log Avg loss: 0.00039958, Global Avg Loss: 0.00222413, Time: 0.0203
Steps: 2894000, Updated lr: 0.000133
Training, Epoch: 0017, Batch: 153840, Sample Num: 153840, Cur Loss: 0.00000003, Cur Avg Loss: 0.00051928, Log Avg loss: 0.00080695, Global Avg Loss: 0.00222217, Time: 0.0147
Steps: 2898000, Updated lr: 0.000133
Training, Epoch: 0017, Batch: 157840, Sample Num: 157840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00052534, Log Avg loss: 0.00075840, Global Avg Loss: 0.00222015, Time: 0.0204
Steps: 2902000, Updated lr: 0.000133
Training, Epoch: 0017, Batch: 161840, Sample Num: 161840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00052460, Log Avg loss: 0.00049551, Global Avg Loss: 0.00221778, Time: 0.0216
Steps: 2906000, Updated lr: 0.000133
Training, Epoch: 0017, Batch: 165840, Sample Num: 165840, Cur Loss: 0.00000000, Cur Avg Loss: 0.00052552, Log Avg loss: 0.00056271, Global Avg Loss: 0.00221550, Time: 0.0438
Steps: 2910000, Updated lr: 0.000133
Training, Epoch: 0017, Batch: 169840, Sample Num: 169840, Cur Loss: 0.01871690, Cur Avg Loss: 0.00052886, Log Avg loss: 0.00066737, Global Avg Loss: 0.00221338, Time: 0.0121
***** Running evaluation checkpoint-2913545 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-2913545 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3439.838481, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.167807, "eval_total_loss": 3591.745262, "eval_acc": 0.961362, "eval_prec": 0.984655, "eval_recall": 0.972378, "eval_f1": 0.976536, "eval_top2_acc": 0.993973, "eval_top3_acc": 0.998645, "eval_top5_acc": 0.999766, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999463, "eval_pr_auc": 0.988396, "eval_mcc": 0.957185, "eval_sn": 0.972378, "eval_sp": 0.998526, "update_flag": false, "test_avg_loss": 0.164205, "test_total_loss": 3514.651829, "test_acc": 0.962951, "test_prec": 0.982089, "test_recall": 0.980172, "test_f1": 0.981089, "test_top2_acc": 0.99388, "test_top3_acc": 0.998739, "test_top5_acc": 0.99972, "test_top10_acc": 0.999907, "test_roc_auc": 0.999435, "test_pr_auc": 0.992514, "test_mcc": 0.958901, "test_sn": 0.980172, "test_sp": 0.998581, "lr": 0.00013301534398898573, "cur_epoch_step": 171385, "train_global_avg_loss": 0.002212469484371575, "train_cur_epoch_loss": 90.60202988186916, "train_cur_epoch_avg_loss": 0.0005286462052213972, "train_cur_epoch_time": 3439.8384811878204, "train_cur_epoch_avg_time": 0.02007082580848861, "epoch": 17, "step": 2913545}
##################################################
Training, Epoch: 0018, Batch: 002455, Sample Num: 2455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00038914, Log Avg loss: 0.00043404, Global Avg Loss: 0.00221093, Time: 0.0231
Training, Epoch: 0018, Batch: 006455, Sample Num: 6455, Cur Loss: 0.00000001, Cur Avg Loss: 0.00033318, Log Avg loss: 0.00029883, Global Avg Loss: 0.00220832, Time: 0.0215
Training, Epoch: 0018, Batch: 010455, Sample Num: 10455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00035621, Log Avg loss: 0.00039339, Global Avg Loss: 0.00220583, Time: 0.0322
Training, Epoch: 0018, Batch: 014455, Sample Num: 14455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00036606, Log Avg loss: 0.00039181, Global Avg Loss: 0.00220335, Time: 0.0138
Training, Epoch: 0018, Batch: 018455, Sample Num: 18455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00039889, Log Avg loss: 0.00051752, Global Avg Loss: 0.00220105, Time: 0.0209
Training, Epoch: 0018, Batch: 022455, Sample Num: 22455, Cur Loss: 0.00013631, Cur Avg Loss: 0.00039890, Log Avg loss: 0.00039896, Global Avg Loss: 0.00219860, Time: 0.0133
Training, Epoch: 0018, Batch: 026455, Sample Num: 26455, Cur Loss: 0.00000002, Cur Avg Loss: 0.00040300, Log Avg loss: 0.00042603, Global Avg Loss: 0.00219619, Time: 0.0453
Training, Epoch: 0018, Batch: 030455, Sample Num: 30455, Cur Loss: 0.00000003, Cur Avg Loss: 0.00039799, Log Avg loss: 0.00036483, Global Avg Loss: 0.00219370, Time: 0.0622
Training, Epoch: 0018, Batch: 034455, Sample Num: 34455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00042599, Log Avg loss: 0.00063921, Global Avg Loss: 0.00219159, Time: 0.0138
Training, Epoch: 0018, Batch: 038455, Sample Num: 38455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00044192, Log Avg loss: 0.00057910, Global Avg Loss: 0.00218940, Time: 0.0588
Training, Epoch: 0018, Batch: 042455, Sample Num: 42455, Cur Loss: 0.00000194, Cur Avg Loss: 0.00044037, Log Avg loss: 0.00042549, Global Avg Loss: 0.00218702, Time: 0.0173
Training, Epoch: 0018, Batch: 046455, Sample Num: 46455, Cur Loss: 0.00000029, Cur Avg Loss: 0.00043824, Log Avg loss: 0.00041566, Global Avg Loss: 0.00218462, Time: 0.0445
Training, Epoch: 0018, Batch: 050455, Sample Num: 50455, Cur Loss: 0.00000012, Cur Avg Loss: 0.00044245, Log Avg loss: 0.00049124, Global Avg Loss: 0.00218234, Time: 0.0541
Training, Epoch: 0018, Batch: 054455, Sample Num: 54455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00045390, Log Avg loss: 0.00059832, Global Avg Loss: 0.00218020, Time: 0.0230
Training, Epoch: 0018, Batch: 058455, Sample Num: 58455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00044954, Log Avg loss: 0.00039032, Global Avg Loss: 0.00217780, Time: 0.0506
Training, Epoch: 0018, Batch: 062455, Sample Num: 62455, Cur Loss: 0.00000008, Cur Avg Loss: 0.00045192, Log Avg loss: 0.00048669, Global Avg Loss: 0.00217552, Time: 0.0124
Training, Epoch: 0018, Batch: 066455, Sample Num: 66455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00045668, Log Avg loss: 0.00053089, Global Avg Loss: 0.00217331, Time: 0.0152
Training, Epoch: 0018, Batch: 070455, Sample Num: 70455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00045794, Log Avg loss: 0.00047893, Global Avg Loss: 0.00217104, Time: 0.0255
Training, Epoch: 0018, Batch: 074455, Sample Num: 74455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00046100, Log Avg loss: 0.00051492, Global Avg Loss: 0.00216883, Time: 0.0405
Training, Epoch: 0018, Batch: 078455, Sample Num: 78455, Cur Loss: 0.00000001, Cur Avg Loss: 0.00046743, Log Avg loss: 0.00058703, Global Avg Loss: 0.00216671, Time: 0.0473
Training, Epoch: 0018, Batch: 082455, Sample Num: 82455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00048006, Log Avg loss: 0.00072790, Global Avg Loss: 0.00216479, Time: 0.0274
Training, Epoch: 0018, Batch: 086455, Sample Num: 86455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00047624, Log Avg loss: 0.00039734, Global Avg Loss: 0.00216243, Time: 0.0200
Training, Epoch: 0018, Batch: 090455, Sample Num: 90455, Cur Loss: 0.00000001, Cur Avg Loss: 0.00047476, Log Avg loss: 0.00044286, Global Avg Loss: 0.00216014, Time: 0.0490
Training, Epoch: 0018, Batch: 094455, Sample Num: 94455, Cur Loss: 0.00000003, Cur Avg Loss: 0.00047197, Log Avg loss: 0.00040891, Global Avg Loss: 0.00215782, Time: 0.0149
Training, Epoch: 0018, Batch: 098455, Sample Num: 98455, Cur Loss: 0.00000038, Cur Avg Loss: 0.00047995, Log Avg loss: 0.00066836, Global Avg Loss: 0.00215584, Time: 0.0784
Training, Epoch: 0018, Batch: 102455, Sample Num: 102455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00047767, Log Avg loss: 0.00042148, Global Avg Loss: 0.00215354, Time: 0.0259
Training, Epoch: 0018, Batch: 106455, Sample Num: 106455, Cur Loss: 0.00000047, Cur Avg Loss: 0.00048116, Log Avg loss: 0.00057052, Global Avg Loss: 0.00215144, Time: 0.0142
Training, Epoch: 0018, Batch: 110455, Sample Num: 110455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00047834, Log Avg loss: 0.00040336, Global Avg Loss: 0.00214913, Time: 0.0186
Training, Epoch: 0018, Batch: 114455, Sample Num: 114455, Cur Loss: 0.00000068, Cur Avg Loss: 0.00047507, Log Avg loss: 0.00038469, Global Avg Loss: 0.00214680, Time: 0.0169
Training, Epoch: 0018, Batch: 118455, Sample Num: 118455, Cur Loss: 0.00010763, Cur Avg Loss: 0.00047533, Log Avg loss: 0.00048300, Global Avg Loss: 0.00214460, Time: 0.0477
Training, Epoch: 0018, Batch: 122455, Sample Num: 122455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00047629, Log Avg loss: 0.00050456, Global Avg Loss: 0.00214244, Time: 0.0183
Training, Epoch: 0018, Batch: 126455, Sample Num: 126455, Cur Loss: 0.00010921, Cur Avg Loss: 0.00047222, Log Avg loss: 0.00034777, Global Avg Loss: 0.00214008, Time: 0.0458
Training, Epoch: 0018, Batch: 130455, Sample Num: 130455, Cur Loss: 0.00000005, Cur Avg Loss: 0.00047918, Log Avg loss: 0.00069918, Global Avg Loss: 0.00213819, Time: 0.0184
Training, Epoch: 0018, Batch: 134455, Sample Num: 134455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00048160, Log Avg loss: 0.00056050, Global Avg Loss: 0.00213612, Time: 0.0193
Training, Epoch: 0018, Batch: 138455, Sample Num: 138455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00048457, Log Avg loss: 0.00058433, Global Avg Loss: 0.00213408, Time: 0.0400
Training, Epoch: 0018, Batch: 142455, Sample Num: 142455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00048784, Log Avg loss: 0.00060106, Global Avg Loss: 0.00213208, Time: 0.0536
Training, Epoch: 0018, Batch: 146455, Sample Num: 146455, Cur Loss: 0.00000038, Cur Avg Loss: 0.00049300, Log Avg loss: 0.00067667, Global Avg Loss: 0.00213017, Time: 0.0659
Training, Epoch: 0018, Batch: 150455, Sample Num: 150455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00049175, Log Avg loss: 0.00044598, Global Avg Loss: 0.00212797, Time: 0.0328
Training, Epoch: 0018, Batch: 154455, Sample Num: 154455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00048781, Log Avg loss: 0.00033978, Global Avg Loss: 0.00212564, Time: 0.0508
Training, Epoch: 0018, Batch: 158455, Sample Num: 158455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00048808, Log Avg loss: 0.00049859, Global Avg Loss: 0.00212352, Time: 0.0121
Training, Epoch: 0018, Batch: 162455, Sample Num: 162455, Cur Loss: 0.00000000, Cur Avg Loss: 0.00048612, Log Avg loss: 0.00040846, Global Avg Loss: 0.00212129, Time: 0.0134
Training, Epoch: 0018, Batch: 166455, Sample Num: 166455, Cur Loss: 0.00000054, Cur Avg Loss: 0.00048449, Log Avg loss: 0.00041811, Global Avg Loss: 0.00211908, Time: 0.0487
Training, Epoch: 0018, Batch: 170455, Sample Num: 170455, Cur Loss: 0.00509351, Cur Avg Loss: 0.00048571, Log Avg loss: 0.00053634, Global Avg Loss: 0.00211703, Time: 0.0122
***** Running evaluation checkpoint-3084930 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-3084930 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 4887.328185, Avg time per batch (s): 0.030000
{"eval_avg_loss": 0.172906, "eval_total_loss": 3700.879356, "eval_acc": 0.963418, "eval_prec": 0.969762, "eval_recall": 0.966699, "eval_f1": 0.968041, "eval_top2_acc": 0.993833, "eval_top3_acc": 0.998739, "eval_top5_acc": 0.99986, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999429, "eval_pr_auc": 0.985532, "eval_mcc": 0.959419, "eval_sn": 0.966699, "eval_sp": 0.998583, "update_flag": false, "test_avg_loss": 0.172128, "test_total_loss": 3684.234058, "test_acc": 0.964025, "test_prec": 0.980222, "test_recall": 0.978044, "test_f1": 0.978897, "test_top2_acc": 0.99444, "test_top3_acc": 0.998645, "test_top5_acc": 0.99972, "test_top10_acc": 0.999953, "test_roc_auc": 0.999417, "test_pr_auc": 0.991565, "test_mcc": 0.960142, "test_sn": 0.978044, "test_sp": 0.998598, "lr": 0.00012898655943303604, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0021167054249851785, "train_cur_epoch_loss": 83.75866285476678, "train_cur_epoch_avg_loss": 0.0004887164154083892, "train_cur_epoch_time": 4887.328185081482, "train_cur_epoch_avg_time": 0.02851666239800147, "epoch": 18, "step": 3084930}
##################################################
Training, Epoch: 0019, Batch: 003070, Sample Num: 3070, Cur Loss: 0.00000001, Cur Avg Loss: 0.00041657, Log Avg loss: 0.00056164, Global Avg Loss: 0.00211502, Time: 0.0124
Training, Epoch: 0019, Batch: 007070, Sample Num: 7070, Cur Loss: 0.00000000, Cur Avg Loss: 0.00031916, Log Avg loss: 0.00024439, Global Avg Loss: 0.00211260, Time: 0.0124
Training, Epoch: 0019, Batch: 011070, Sample Num: 11070, Cur Loss: 0.00000017, Cur Avg Loss: 0.00032156, Log Avg loss: 0.00032582, Global Avg Loss: 0.00211029, Time: 0.0168
Training, Epoch: 0019, Batch: 015070, Sample Num: 15070, Cur Loss: 0.00000000, Cur Avg Loss: 0.00035901, Log Avg loss: 0.00046265, Global Avg Loss: 0.00210816, Time: 0.0369
Training, Epoch: 0019, Batch: 019070, Sample Num: 19070, Cur Loss: 0.00000000, Cur Avg Loss: 0.00032766, Log Avg loss: 0.00020953, Global Avg Loss: 0.00210571, Time: 0.0145
Training, Epoch: 0019, Batch: 023070, Sample Num: 23070, Cur Loss: 0.00000019, Cur Avg Loss: 0.00036285, Log Avg loss: 0.00053061, Global Avg Loss: 0.00210369, Time: 0.0271
Training, Epoch: 0019, Batch: 027070, Sample Num: 27070, Cur Loss: 0.00002100, Cur Avg Loss: 0.00037031, Log Avg loss: 0.00041332, Global Avg Loss: 0.00210151, Time: 0.0126
Training, Epoch: 0019, Batch: 031070, Sample Num: 31070, Cur Loss: 0.00000000, Cur Avg Loss: 0.00036275, Log Avg loss: 0.00031164, Global Avg Loss: 0.00209922, Time: 0.0128
Training, Epoch: 0019, Batch: 035070, Sample Num: 35070, Cur Loss: 0.00000000, Cur Avg Loss: 0.00035477, Log Avg loss: 0.00029276, Global Avg Loss: 0.00209690, Time: 0.0190
Training, Epoch: 0019, Batch: 039070, Sample Num: 39070, Cur Loss: 0.00000009, Cur Avg Loss: 0.00035923, Log Avg loss: 0.00039833, Global Avg Loss: 0.00209473, Time: 0.0128
Training, Epoch: 0019, Batch: 043070, Sample Num: 43070, Cur Loss: 0.00000010, Cur Avg Loss: 0.00036739, Log Avg loss: 0.00044711, Global Avg Loss: 0.00209262, Time: 0.0150
Training, Epoch: 0019, Batch: 047070, Sample Num: 47070, Cur Loss: 0.00000000, Cur Avg Loss: 0.00037933, Log Avg loss: 0.00050790, Global Avg Loss: 0.00209059, Time: 0.0188
Training, Epoch: 0019, Batch: 051070, Sample Num: 51070, Cur Loss: 0.00000010, Cur Avg Loss: 0.00036909, Log Avg loss: 0.00024854, Global Avg Loss: 0.00208825, Time: 0.0226
Training, Epoch: 0019, Batch: 055070, Sample Num: 55070, Cur Loss: 0.00029375, Cur Avg Loss: 0.00035858, Log Avg loss: 0.00022448, Global Avg Loss: 0.00208587, Time: 0.0140
Training, Epoch: 0019, Batch: 059070, Sample Num: 59070, Cur Loss: 0.00033352, Cur Avg Loss: 0.00035936, Log Avg loss: 0.00037000, Global Avg Loss: 0.00208369, Time: 0.0363
Training, Epoch: 0019, Batch: 063070, Sample Num: 63070, Cur Loss: 0.00000000, Cur Avg Loss: 0.00037041, Log Avg loss: 0.00053371, Global Avg Loss: 0.00208172, Time: 0.0154
Training, Epoch: 0019, Batch: 067070, Sample Num: 67070, Cur Loss: 0.00000009, Cur Avg Loss: 0.00038472, Log Avg loss: 0.00061031, Global Avg Loss: 0.00207985, Time: 0.0286
Training, Epoch: 0019, Batch: 071070, Sample Num: 71070, Cur Loss: 0.00000352, Cur Avg Loss: 0.00038588, Log Avg loss: 0.00040527, Global Avg Loss: 0.00207773, Time: 0.0212
Training, Epoch: 0019, Batch: 075070, Sample Num: 75070, Cur Loss: 0.00000000, Cur Avg Loss: 0.00039553, Log Avg loss: 0.00056695, Global Avg Loss: 0.00207582, Time: 0.0123
Training, Epoch: 0019, Batch: 079070, Sample Num: 79070, Cur Loss: 0.00000006, Cur Avg Loss: 0.00039333, Log Avg loss: 0.00035207, Global Avg Loss: 0.00207364, Time: 0.0182
Training, Epoch: 0019, Batch: 083070, Sample Num: 83070, Cur Loss: 0.00000004, Cur Avg Loss: 0.00039060, Log Avg loss: 0.00033668, Global Avg Loss: 0.00207144, Time: 0.0146
Training, Epoch: 0019, Batch: 087070, Sample Num: 87070, Cur Loss: 0.00000131, Cur Avg Loss: 0.00039447, Log Avg loss: 0.00047478, Global Avg Loss: 0.00206943, Time: 0.0164
Training, Epoch: 0019, Batch: 091070, Sample Num: 91070, Cur Loss: 0.00000001, Cur Avg Loss: 0.00039620, Log Avg loss: 0.00043400, Global Avg Loss: 0.00206737, Time: 0.0139
Training, Epoch: 0019, Batch: 095070, Sample Num: 95070, Cur Loss: 0.00000030, Cur Avg Loss: 0.00040885, Log Avg loss: 0.00069688, Global Avg Loss: 0.00206565, Time: 0.0122
Training, Epoch: 0019, Batch: 099070, Sample Num: 99070, Cur Loss: 0.00000003, Cur Avg Loss: 0.00040974, Log Avg loss: 0.00043074, Global Avg Loss: 0.00206359, Time: 0.0155
Training, Epoch: 0019, Batch: 103070, Sample Num: 103070, Cur Loss: 0.00000064, Cur Avg Loss: 0.00040990, Log Avg loss: 0.00041388, Global Avg Loss: 0.00206152, Time: 0.0129
Training, Epoch: 0019, Batch: 107070, Sample Num: 107070, Cur Loss: 0.00000002, Cur Avg Loss: 0.00040703, Log Avg loss: 0.00033308, Global Avg Loss: 0.00205936, Time: 0.0378
Training, Epoch: 0019, Batch: 111070, Sample Num: 111070, Cur Loss: 0.00000000, Cur Avg Loss: 0.00040913, Log Avg loss: 0.00046548, Global Avg Loss: 0.00205736, Time: 0.0275
Training, Epoch: 0019, Batch: 115070, Sample Num: 115070, Cur Loss: 0.00000000, Cur Avg Loss: 0.00041318, Log Avg loss: 0.00052549, Global Avg Loss: 0.00205545, Time: 0.0155
Training, Epoch: 0019, Batch: 119070, Sample Num: 119070, Cur Loss: 0.00000000, Cur Avg Loss: 0.00041268, Log Avg loss: 0.00039842, Global Avg Loss: 0.00205338, Time: 0.0197
Training, Epoch: 0019, Batch: 123070, Sample Num: 123070, Cur Loss: 0.00000000, Cur Avg Loss: 0.00041421, Log Avg loss: 0.00045972, Global Avg Loss: 0.00205139, Time: 0.0127
Training, Epoch: 0019, Batch: 127070, Sample Num: 127070, Cur Loss: 0.00000022, Cur Avg Loss: 0.00041600, Log Avg loss: 0.00047105, Global Avg Loss: 0.00204942, Time: 0.0140
Training, Epoch: 0019, Batch: 131070, Sample Num: 131070, Cur Loss: 0.00000033, Cur Avg Loss: 0.00041228, Log Avg loss: 0.00029411, Global Avg Loss: 0.00204724, Time: 0.0311
Training, Epoch: 0019, Batch: 135070, Sample Num: 135070, Cur Loss: 0.00000000, Cur Avg Loss: 0.00041198, Log Avg loss: 0.00040198, Global Avg Loss: 0.00204520, Time: 0.0470
Training, Epoch: 0019, Batch: 139070, Sample Num: 139070, Cur Loss: 0.00000000, Cur Avg Loss: 0.00041138, Log Avg loss: 0.00039121, Global Avg Loss: 0.00204314, Time: 0.0225
Training, Epoch: 0019, Batch: 143070, Sample Num: 143070, Cur Loss: 0.00000581, Cur Avg Loss: 0.00041348, Log Avg loss: 0.00048638, Global Avg Loss: 0.00204122, Time: 0.0289
Training, Epoch: 0019, Batch: 147070, Sample Num: 147070, Cur Loss: 0.00000000, Cur Avg Loss: 0.00041463, Log Avg loss: 0.00045600, Global Avg Loss: 0.00203925, Time: 0.0135
Training, Epoch: 0019, Batch: 151070, Sample Num: 151070, Cur Loss: 0.00000000, Cur Avg Loss: 0.00041576, Log Avg loss: 0.00045715, Global Avg Loss: 0.00203730, Time: 0.0123
Training, Epoch: 0019, Batch: 155070, Sample Num: 155070, Cur Loss: 0.00000001, Cur Avg Loss: 0.00041197, Log Avg loss: 0.00026888, Global Avg Loss: 0.00203511, Time: 0.0132
Training, Epoch: 0019, Batch: 159070, Sample Num: 159070, Cur Loss: 0.00000001, Cur Avg Loss: 0.00041260, Log Avg loss: 0.00043698, Global Avg Loss: 0.00203314, Time: 0.0143
Training, Epoch: 0019, Batch: 163070, Sample Num: 163070, Cur Loss: 0.00000000, Cur Avg Loss: 0.00041395, Log Avg loss: 0.00046761, Global Avg Loss: 0.00203122, Time: 0.0141
Training, Epoch: 0019, Batch: 167070, Sample Num: 167070, Cur Loss: 0.00000000, Cur Avg Loss: 0.00041512, Log Avg loss: 0.00046278, Global Avg Loss: 0.00202929, Time: 0.0131
Training, Epoch: 0019, Batch: 171070, Sample Num: 171070, Cur Loss: 0.00006013, Cur Avg Loss: 0.00041625, Log Avg loss: 0.00046352, Global Avg Loss: 0.00202736, Time: 0.0133
***** Running evaluation checkpoint-3256315 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-3256315 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3278.708836, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.177059, "eval_total_loss": 3789.774817, "eval_acc": 0.962904, "eval_prec": 0.980086, "eval_recall": 0.969801, "eval_f1": 0.974224, "eval_top2_acc": 0.994487, "eval_top3_acc": 0.998739, "eval_top5_acc": 0.99986, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999434, "eval_pr_auc": 0.986804, "eval_mcc": 0.958784, "eval_sn": 0.969801, "eval_sp": 0.998565, "update_flag": false, "test_avg_loss": 0.173905, "test_total_loss": 3722.254075, "test_acc": 0.963231, "test_prec": 0.987066, "test_recall": 0.97583, "test_f1": 0.980547, "test_top2_acc": 0.994253, "test_top3_acc": 0.998598, "test_top5_acc": 0.999907, "test_top10_acc": 0.999953, "test_roc_auc": 0.999408, "test_pr_auc": 0.992542, "test_mcc": 0.959155, "test_sn": 0.97583, "test_sp": 0.998571, "lr": 0.00012495777487708633, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0020271878226989595, "train_cur_epoch_loss": 71.27404817108732, "train_cur_epoch_avg_loss": 0.00041587098153915056, "train_cur_epoch_time": 3278.7088356018066, "train_cur_epoch_avg_time": 0.01913066391808972, "epoch": 19, "step": 3256315}
##################################################
Training, Epoch: 0020, Batch: 003685, Sample Num: 3685, Cur Loss: 0.00000001, Cur Avg Loss: 0.00026748, Log Avg loss: 0.00026303, Global Avg Loss: 0.00202520, Time: 0.0140
Training, Epoch: 0020, Batch: 007685, Sample Num: 7685, Cur Loss: 0.00000000, Cur Avg Loss: 0.00024839, Log Avg loss: 0.00023081, Global Avg Loss: 0.00202300, Time: 0.0126
Training, Epoch: 0020, Batch: 011685, Sample Num: 11685, Cur Loss: 0.00000000, Cur Avg Loss: 0.00029753, Log Avg loss: 0.00039195, Global Avg Loss: 0.00202100, Time: 0.0263
Training, Epoch: 0020, Batch: 015685, Sample Num: 15685, Cur Loss: 0.00000000, Cur Avg Loss: 0.00028068, Log Avg loss: 0.00023145, Global Avg Loss: 0.00201882, Time: 0.0146
Training, Epoch: 0020, Batch: 019685, Sample Num: 19685, Cur Loss: 0.00000000, Cur Avg Loss: 0.00026362, Log Avg loss: 0.00019671, Global Avg Loss: 0.00201659, Time: 0.0484
Training, Epoch: 0020, Batch: 023685, Sample Num: 23685, Cur Loss: 0.00000000, Cur Avg Loss: 0.00027633, Log Avg loss: 0.00033888, Global Avg Loss: 0.00201454, Time: 0.0122
Training, Epoch: 0020, Batch: 027685, Sample Num: 27685, Cur Loss: 0.00000000, Cur Avg Loss: 0.00029272, Log Avg loss: 0.00038977, Global Avg Loss: 0.00201257, Time: 0.0148
Training, Epoch: 0020, Batch: 031685, Sample Num: 31685, Cur Loss: 0.00000340, Cur Avg Loss: 0.00029119, Log Avg loss: 0.00028058, Global Avg Loss: 0.00201046, Time: 0.0151
Training, Epoch: 0020, Batch: 035685, Sample Num: 35685, Cur Loss: 0.00000001, Cur Avg Loss: 0.00031304, Log Avg loss: 0.00048616, Global Avg Loss: 0.00200861, Time: 0.0172
Training, Epoch: 0020, Batch: 039685, Sample Num: 39685, Cur Loss: 0.00000000, Cur Avg Loss: 0.00033345, Log Avg loss: 0.00051555, Global Avg Loss: 0.00200679, Time: 0.0150
Training, Epoch: 0020, Batch: 043685, Sample Num: 43685, Cur Loss: 0.00000000, Cur Avg Loss: 0.00032907, Log Avg loss: 0.00028554, Global Avg Loss: 0.00200471, Time: 0.0128
Training, Epoch: 0020, Batch: 047685, Sample Num: 47685, Cur Loss: 0.00000000, Cur Avg Loss: 0.00033076, Log Avg loss: 0.00034930, Global Avg Loss: 0.00200270, Time: 0.0122
Training, Epoch: 0020, Batch: 051685, Sample Num: 51685, Cur Loss: 0.00002738, Cur Avg Loss: 0.00032927, Log Avg loss: 0.00031147, Global Avg Loss: 0.00200066, Time: 0.0130
Training, Epoch: 0020, Batch: 055685, Sample Num: 55685, Cur Loss: 0.00000047, Cur Avg Loss: 0.00033444, Log Avg loss: 0.00040122, Global Avg Loss: 0.00199873, Time: 0.0138
Training, Epoch: 0020, Batch: 059685, Sample Num: 59685, Cur Loss: 0.00000001, Cur Avg Loss: 0.00034992, Log Avg loss: 0.00056537, Global Avg Loss: 0.00199700, Time: 0.0146
Training, Epoch: 0020, Batch: 063685, Sample Num: 63685, Cur Loss: 0.00000124, Cur Avg Loss: 0.00034772, Log Avg loss: 0.00031499, Global Avg Loss: 0.00199497, Time: 0.0121
Training, Epoch: 0020, Batch: 067685, Sample Num: 67685, Cur Loss: 0.00001476, Cur Avg Loss: 0.00034626, Log Avg loss: 0.00032297, Global Avg Loss: 0.00199296, Time: 0.0147
Training, Epoch: 0020, Batch: 071685, Sample Num: 71685, Cur Loss: 0.00000004, Cur Avg Loss: 0.00035294, Log Avg loss: 0.00046596, Global Avg Loss: 0.00199112, Time: 0.0132
Training, Epoch: 0020, Batch: 075685, Sample Num: 75685, Cur Loss: 0.00000000, Cur Avg Loss: 0.00034716, Log Avg loss: 0.00024355, Global Avg Loss: 0.00198903, Time: 0.0192
Training, Epoch: 0020, Batch: 079685, Sample Num: 79685, Cur Loss: 0.00000000, Cur Avg Loss: 0.00034863, Log Avg loss: 0.00037657, Global Avg Loss: 0.00198709, Time: 0.0130
Training, Epoch: 0020, Batch: 083685, Sample Num: 83685, Cur Loss: 0.00005259, Cur Avg Loss: 0.00034995, Log Avg loss: 0.00037617, Global Avg Loss: 0.00198516, Time: 0.0124
Training, Epoch: 0020, Batch: 087685, Sample Num: 87685, Cur Loss: 0.00000013, Cur Avg Loss: 0.00034843, Log Avg loss: 0.00031655, Global Avg Loss: 0.00198317, Time: 0.0125
Training, Epoch: 0020, Batch: 091685, Sample Num: 91685, Cur Loss: 0.00000229, Cur Avg Loss: 0.00035329, Log Avg loss: 0.00045987, Global Avg Loss: 0.00198135, Time: 0.0125
Training, Epoch: 0020, Batch: 095685, Sample Num: 95685, Cur Loss: 0.00000001, Cur Avg Loss: 0.00035432, Log Avg loss: 0.00037786, Global Avg Loss: 0.00197943, Time: 0.0142
Training, Epoch: 0020, Batch: 099685, Sample Num: 99685, Cur Loss: 0.00000001, Cur Avg Loss: 0.00035977, Log Avg loss: 0.00049019, Global Avg Loss: 0.00197766, Time: 0.0149
Training, Epoch: 0020, Batch: 103685, Sample Num: 103685, Cur Loss: 0.00000405, Cur Avg Loss: 0.00036150, Log Avg loss: 0.00040475, Global Avg Loss: 0.00197579, Time: 0.0187
Training, Epoch: 0020, Batch: 107685, Sample Num: 107685, Cur Loss: 0.00000007, Cur Avg Loss: 0.00036499, Log Avg loss: 0.00045530, Global Avg Loss: 0.00197398, Time: 0.0133
Training, Epoch: 0020, Batch: 111685, Sample Num: 111685, Cur Loss: 0.00004273, Cur Avg Loss: 0.00036296, Log Avg loss: 0.00030846, Global Avg Loss: 0.00197200, Time: 0.0185
Training, Epoch: 0020, Batch: 115685, Sample Num: 115685, Cur Loss: 0.00000000, Cur Avg Loss: 0.00036013, Log Avg loss: 0.00028099, Global Avg Loss: 0.00197000, Time: 0.0196
Training, Epoch: 0020, Batch: 119685, Sample Num: 119685, Cur Loss: 0.00000031, Cur Avg Loss: 0.00035885, Log Avg loss: 0.00032189, Global Avg Loss: 0.00196804, Time: 0.0394
Training, Epoch: 0020, Batch: 123685, Sample Num: 123685, Cur Loss: 0.00000000, Cur Avg Loss: 0.00036178, Log Avg loss: 0.00044937, Global Avg Loss: 0.00196625, Time: 0.0619
Training, Epoch: 0020, Batch: 127685, Sample Num: 127685, Cur Loss: 0.00000000, Cur Avg Loss: 0.00036205, Log Avg loss: 0.00037043, Global Avg Loss: 0.00196436, Time: 0.0138
Training, Epoch: 0020, Batch: 131685, Sample Num: 131685, Cur Loss: 0.00000000, Cur Avg Loss: 0.00036411, Log Avg loss: 0.00042995, Global Avg Loss: 0.00196255, Time: 0.0127
Training, Epoch: 0020, Batch: 135685, Sample Num: 135685, Cur Loss: 0.00000152, Cur Avg Loss: 0.00036520, Log Avg loss: 0.00040116, Global Avg Loss: 0.00196071, Time: 0.0123
Training, Epoch: 0020, Batch: 139685, Sample Num: 139685, Cur Loss: 0.00000000, Cur Avg Loss: 0.00036891, Log Avg loss: 0.00049450, Global Avg Loss: 0.00195898, Time: 0.0190
Training, Epoch: 0020, Batch: 143685, Sample Num: 143685, Cur Loss: 0.01220418, Cur Avg Loss: 0.00036711, Log Avg loss: 0.00030439, Global Avg Loss: 0.00195703, Time: 0.0121
Training, Epoch: 0020, Batch: 147685, Sample Num: 147685, Cur Loss: 0.00000055, Cur Avg Loss: 0.00036584, Log Avg loss: 0.00032022, Global Avg Loss: 0.00195511, Time: 0.0123
Training, Epoch: 0020, Batch: 151685, Sample Num: 151685, Cur Loss: 0.00006436, Cur Avg Loss: 0.00037052, Log Avg loss: 0.00054323, Global Avg Loss: 0.00195345, Time: 0.0174
Training, Epoch: 0020, Batch: 155685, Sample Num: 155685, Cur Loss: 0.00000000, Cur Avg Loss: 0.00037456, Log Avg loss: 0.00052777, Global Avg Loss: 0.00195178, Time: 0.0284
Training, Epoch: 0020, Batch: 159685, Sample Num: 159685, Cur Loss: 0.00000000, Cur Avg Loss: 0.00037396, Log Avg loss: 0.00035073, Global Avg Loss: 0.00194991, Time: 0.0295
Training, Epoch: 0020, Batch: 163685, Sample Num: 163685, Cur Loss: 0.00000000, Cur Avg Loss: 0.00037513, Log Avg loss: 0.00042200, Global Avg Loss: 0.00194812, Time: 0.0378
Training, Epoch: 0020, Batch: 167685, Sample Num: 167685, Cur Loss: 0.00000004, Cur Avg Loss: 0.00038022, Log Avg loss: 0.00058818, Global Avg Loss: 0.00194653, Time: 0.0142
***** Running evaluation checkpoint-3427700 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-3427700 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3288.098394, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.180255, "eval_total_loss": 3858.18633, "eval_acc": 0.962671, "eval_prec": 0.978273, "eval_recall": 0.971952, "eval_f1": 0.974477, "eval_top2_acc": 0.994253, "eval_top3_acc": 0.998785, "eval_top5_acc": 0.99972, "eval_top10_acc": 0.99986, "eval_roc_auc": 0.999421, "eval_pr_auc": 0.986768, "eval_mcc": 0.958554, "eval_sn": 0.971952, "eval_sp": 0.998572, "update_flag": false, "test_avg_loss": 0.177678, "test_total_loss": 3803.029046, "test_acc": 0.964119, "test_prec": 0.979884, "test_recall": 0.975448, "test_f1": 0.977469, "test_top2_acc": 0.993926, "test_top3_acc": 0.998832, "test_top5_acc": 0.999907, "test_top10_acc": 0.999953, "test_roc_auc": 0.999404, "test_pr_auc": 0.992355, "test_mcc": 0.960152, "test_sn": 0.975448, "test_sp": 0.998624, "lr": 0.00012092899032113665, "cur_epoch_step": 171385, "train_global_avg_loss": 0.001944794151136275, "train_cur_epoch_loss": 65.00879697667433, "train_cur_epoch_avg_loss": 0.0003793143914384242, "train_cur_epoch_time": 3288.0983941555023, "train_cur_epoch_avg_time": 0.019185450267850176, "epoch": 20, "step": 3427700}
##################################################
Training, Epoch: 0021, Batch: 000300, Sample Num: 300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00054282, Log Avg loss: 0.00035374, Global Avg Loss: 0.00194467, Time: 0.0201
Training, Epoch: 0021, Batch: 004300, Sample Num: 4300, Cur Loss: 0.00002143, Cur Avg Loss: 0.00029487, Log Avg loss: 0.00027627, Global Avg Loss: 0.00194273, Time: 0.0126
Training, Epoch: 0021, Batch: 008300, Sample Num: 8300, Cur Loss: 0.00000010, Cur Avg Loss: 0.00028351, Log Avg loss: 0.00027131, Global Avg Loss: 0.00194078, Time: 0.0151
Training, Epoch: 0021, Batch: 012300, Sample Num: 12300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00026862, Log Avg loss: 0.00023771, Global Avg Loss: 0.00193880, Time: 0.0144
Training, Epoch: 0021, Batch: 016300, Sample Num: 16300, Cur Loss: 0.00000010, Cur Avg Loss: 0.00028782, Log Avg loss: 0.00034686, Global Avg Loss: 0.00193695, Time: 0.0440
Training, Epoch: 0021, Batch: 020300, Sample Num: 20300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00028948, Log Avg loss: 0.00029628, Global Avg Loss: 0.00193505, Time: 0.0140
Training, Epoch: 0021, Batch: 024300, Sample Num: 24300, Cur Loss: 0.00005032, Cur Avg Loss: 0.00029265, Log Avg loss: 0.00030873, Global Avg Loss: 0.00193316, Time: 0.0125
Training, Epoch: 0021, Batch: 028300, Sample Num: 28300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00028435, Log Avg loss: 0.00023388, Global Avg Loss: 0.00193120, Time: 0.0164
Training, Epoch: 0021, Batch: 032300, Sample Num: 32300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00030192, Log Avg loss: 0.00042622, Global Avg Loss: 0.00192946, Time: 0.0137
Training, Epoch: 0021, Batch: 036300, Sample Num: 36300, Cur Loss: 0.00000747, Cur Avg Loss: 0.00030187, Log Avg loss: 0.00030147, Global Avg Loss: 0.00192758, Time: 0.0123
Training, Epoch: 0021, Batch: 040300, Sample Num: 40300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00029291, Log Avg loss: 0.00021164, Global Avg Loss: 0.00192560, Time: 0.0187
Training, Epoch: 0021, Batch: 044300, Sample Num: 44300, Cur Loss: 0.00000002, Cur Avg Loss: 0.00029659, Log Avg loss: 0.00033369, Global Avg Loss: 0.00192376, Time: 0.0440
Training, Epoch: 0021, Batch: 048300, Sample Num: 48300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00029788, Log Avg loss: 0.00031213, Global Avg Loss: 0.00192191, Time: 0.0141
Training, Epoch: 0021, Batch: 052300, Sample Num: 52300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00029974, Log Avg loss: 0.00032225, Global Avg Loss: 0.00192007, Time: 0.0152
Training, Epoch: 0021, Batch: 056300, Sample Num: 56300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00030052, Log Avg loss: 0.00031071, Global Avg Loss: 0.00191822, Time: 0.0143
Training, Epoch: 0021, Batch: 060300, Sample Num: 60300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00030028, Log Avg loss: 0.00029684, Global Avg Loss: 0.00191636, Time: 0.0267
Training, Epoch: 0021, Batch: 064300, Sample Num: 64300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00029855, Log Avg loss: 0.00027242, Global Avg Loss: 0.00191448, Time: 0.0128
Training, Epoch: 0021, Batch: 068300, Sample Num: 68300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00030089, Log Avg loss: 0.00033860, Global Avg Loss: 0.00191268, Time: 0.0306
Training, Epoch: 0021, Batch: 072300, Sample Num: 72300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00030582, Log Avg loss: 0.00039005, Global Avg Loss: 0.00191094, Time: 0.0156
Training, Epoch: 0021, Batch: 076300, Sample Num: 76300, Cur Loss: 0.00023271, Cur Avg Loss: 0.00031437, Log Avg loss: 0.00046883, Global Avg Loss: 0.00190929, Time: 0.0146
Training, Epoch: 0021, Batch: 080300, Sample Num: 80300, Cur Loss: 0.00003685, Cur Avg Loss: 0.00031848, Log Avg loss: 0.00039679, Global Avg Loss: 0.00190757, Time: 0.0182
Training, Epoch: 0021, Batch: 084300, Sample Num: 84300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00031772, Log Avg loss: 0.00030257, Global Avg Loss: 0.00190574, Time: 0.0358
Training, Epoch: 0021, Batch: 088300, Sample Num: 88300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00032310, Log Avg loss: 0.00043649, Global Avg Loss: 0.00190407, Time: 0.0315
Training, Epoch: 0021, Batch: 092300, Sample Num: 92300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00032305, Log Avg loss: 0.00032189, Global Avg Loss: 0.00190227, Time: 0.0139
Training, Epoch: 0021, Batch: 096300, Sample Num: 96300, Cur Loss: 0.00000009, Cur Avg Loss: 0.00032433, Log Avg loss: 0.00035377, Global Avg Loss: 0.00190051, Time: 0.0320
Training, Epoch: 0021, Batch: 100300, Sample Num: 100300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00032822, Log Avg loss: 0.00042192, Global Avg Loss: 0.00189884, Time: 0.0351
Training, Epoch: 0021, Batch: 104300, Sample Num: 104300, Cur Loss: 0.00000001, Cur Avg Loss: 0.00033289, Log Avg loss: 0.00044993, Global Avg Loss: 0.00189719, Time: 0.0169
Training, Epoch: 0021, Batch: 108300, Sample Num: 108300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00033417, Log Avg loss: 0.00036774, Global Avg Loss: 0.00189546, Time: 0.0353
Training, Epoch: 0021, Batch: 112300, Sample Num: 112300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00033364, Log Avg loss: 0.00031919, Global Avg Loss: 0.00189368, Time: 0.0267
Training, Epoch: 0021, Batch: 116300, Sample Num: 116300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00033199, Log Avg loss: 0.00028582, Global Avg Loss: 0.00189187, Time: 0.0300
Training, Epoch: 0021, Batch: 120300, Sample Num: 120300, Cur Loss: 0.00000043, Cur Avg Loss: 0.00033030, Log Avg loss: 0.00028090, Global Avg Loss: 0.00189005, Time: 0.0284
Training, Epoch: 0021, Batch: 124300, Sample Num: 124300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00032916, Log Avg loss: 0.00029506, Global Avg Loss: 0.00188826, Time: 0.0125
Training, Epoch: 0021, Batch: 128300, Sample Num: 128300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00032893, Log Avg loss: 0.00032183, Global Avg Loss: 0.00188649, Time: 0.0247
Training, Epoch: 0021, Batch: 132300, Sample Num: 132300, Cur Loss: 0.00011410, Cur Avg Loss: 0.00033371, Log Avg loss: 0.00048701, Global Avg Loss: 0.00188492, Time: 0.0175
Training, Epoch: 0021, Batch: 136300, Sample Num: 136300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00033768, Log Avg loss: 0.00046900, Global Avg Loss: 0.00188333, Time: 0.0188
Training, Epoch: 0021, Batch: 140300, Sample Num: 140300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00034169, Log Avg loss: 0.00047834, Global Avg Loss: 0.00188176, Time: 0.0518
Training, Epoch: 0021, Batch: 144300, Sample Num: 144300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00034172, Log Avg loss: 0.00034257, Global Avg Loss: 0.00188003, Time: 0.0150
Training, Epoch: 0021, Batch: 148300, Sample Num: 148300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00034050, Log Avg loss: 0.00029670, Global Avg Loss: 0.00187826, Time: 0.0147
Training, Epoch: 0021, Batch: 152300, Sample Num: 152300, Cur Loss: 0.00000001, Cur Avg Loss: 0.00033906, Log Avg loss: 0.00028560, Global Avg Loss: 0.00187648, Time: 0.0123
Training, Epoch: 0021, Batch: 156300, Sample Num: 156300, Cur Loss: 0.00156637, Cur Avg Loss: 0.00034532, Log Avg loss: 0.00058369, Global Avg Loss: 0.00187504, Time: 0.0124
Training, Epoch: 0021, Batch: 160300, Sample Num: 160300, Cur Loss: 0.00001630, Cur Avg Loss: 0.00034641, Log Avg loss: 0.00038899, Global Avg Loss: 0.00187338, Time: 0.0128
Training, Epoch: 0021, Batch: 164300, Sample Num: 164300, Cur Loss: 0.00000050, Cur Avg Loss: 0.00034684, Log Avg loss: 0.00036401, Global Avg Loss: 0.00187170, Time: 0.0129
Training, Epoch: 0021, Batch: 168300, Sample Num: 168300, Cur Loss: 0.00000000, Cur Avg Loss: 0.00034900, Log Avg loss: 0.00043754, Global Avg Loss: 0.00187011, Time: 0.0352
***** Running evaluation checkpoint-3599085 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-3599085 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 4014.412189, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.179992, "eval_total_loss": 3852.542056, "eval_acc": 0.962951, "eval_prec": 0.985046, "eval_recall": 0.969332, "eval_f1": 0.975252, "eval_top2_acc": 0.993973, "eval_top3_acc": 0.998598, "eval_top5_acc": 0.999813, "eval_top10_acc": 0.99986, "eval_roc_auc": 0.99943, "eval_pr_auc": 0.987748, "eval_mcc": 0.958867, "eval_sn": 0.969332, "eval_sp": 0.998581, "update_flag": false, "test_avg_loss": 0.175995, "test_total_loss": 3766.992503, "test_acc": 0.963465, "test_prec": 0.980157, "test_recall": 0.97554, "test_f1": 0.977658, "test_top2_acc": 0.993599, "test_top3_acc": 0.998692, "test_top5_acc": 0.999813, "test_top10_acc": 0.999907, "test_roc_auc": 0.999394, "test_pr_auc": 0.993258, "test_mcc": 0.959421, "test_sn": 0.97554, "test_sp": 0.998593, "lr": 0.00011690020576518695, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0018689588924207538, "train_cur_epoch_loss": 60.37100347719078, "train_cur_epoch_avg_loss": 0.0003522537181036309, "train_cur_epoch_time": 4014.4121894836426, "train_cur_epoch_avg_time": 0.023423357875447925, "epoch": 21, "step": 3599085}
##################################################
Training, Epoch: 0022, Batch: 000915, Sample Num: 915, Cur Loss: 0.00000237, Cur Avg Loss: 0.00040643, Log Avg loss: 0.00050175, Global Avg Loss: 0.00186859, Time: 0.0132
Training, Epoch: 0022, Batch: 004915, Sample Num: 4915, Cur Loss: 0.00000016, Cur Avg Loss: 0.00024594, Log Avg loss: 0.00020923, Global Avg Loss: 0.00186675, Time: 0.0177
Training, Epoch: 0022, Batch: 008915, Sample Num: 8915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00023465, Log Avg loss: 0.00022077, Global Avg Loss: 0.00186492, Time: 0.0268
Training, Epoch: 0022, Batch: 012915, Sample Num: 12915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00025920, Log Avg loss: 0.00031393, Global Avg Loss: 0.00186320, Time: 0.0303
Training, Epoch: 0022, Batch: 016915, Sample Num: 16915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00030845, Log Avg loss: 0.00046748, Global Avg Loss: 0.00186166, Time: 0.0279
Training, Epoch: 0022, Batch: 020915, Sample Num: 20915, Cur Loss: 0.00000035, Cur Avg Loss: 0.00031137, Log Avg loss: 0.00032372, Global Avg Loss: 0.00185996, Time: 0.0543
Training, Epoch: 0022, Batch: 024915, Sample Num: 24915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00030517, Log Avg loss: 0.00027276, Global Avg Loss: 0.00185821, Time: 0.0364
Training, Epoch: 0022, Batch: 028915, Sample Num: 28915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00030583, Log Avg loss: 0.00030992, Global Avg Loss: 0.00185650, Time: 0.0182
Training, Epoch: 0022, Batch: 032915, Sample Num: 32915, Cur Loss: 0.00000021, Cur Avg Loss: 0.00029439, Log Avg loss: 0.00021172, Global Avg Loss: 0.00185469, Time: 0.0339
Training, Epoch: 0022, Batch: 036915, Sample Num: 36915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00030173, Log Avg loss: 0.00036205, Global Avg Loss: 0.00185305, Time: 0.0146
Training, Epoch: 0022, Batch: 040915, Sample Num: 40915, Cur Loss: 0.00000010, Cur Avg Loss: 0.00028961, Log Avg loss: 0.00017776, Global Avg Loss: 0.00185121, Time: 0.0181
Training, Epoch: 0022, Batch: 044915, Sample Num: 44915, Cur Loss: 0.00000040, Cur Avg Loss: 0.00029424, Log Avg loss: 0.00034169, Global Avg Loss: 0.00184955, Time: 0.0290
Training, Epoch: 0022, Batch: 048915, Sample Num: 48915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00028931, Log Avg loss: 0.00023390, Global Avg Loss: 0.00184778, Time: 0.0166
Training, Epoch: 0022, Batch: 052915, Sample Num: 52915, Cur Loss: 0.00000001, Cur Avg Loss: 0.00029177, Log Avg loss: 0.00032184, Global Avg Loss: 0.00184611, Time: 0.0150
Training, Epoch: 0022, Batch: 056915, Sample Num: 56915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00029424, Log Avg loss: 0.00032696, Global Avg Loss: 0.00184444, Time: 0.0123
Training, Epoch: 0022, Batch: 060915, Sample Num: 60915, Cur Loss: 0.00000410, Cur Avg Loss: 0.00029686, Log Avg loss: 0.00033406, Global Avg Loss: 0.00184279, Time: 0.0146
Training, Epoch: 0022, Batch: 064915, Sample Num: 64915, Cur Loss: 0.00000056, Cur Avg Loss: 0.00028924, Log Avg loss: 0.00017319, Global Avg Loss: 0.00184097, Time: 0.0148
Training, Epoch: 0022, Batch: 068915, Sample Num: 68915, Cur Loss: 0.00005658, Cur Avg Loss: 0.00028989, Log Avg loss: 0.00030050, Global Avg Loss: 0.00183929, Time: 0.0140
Training, Epoch: 0022, Batch: 072915, Sample Num: 72915, Cur Loss: 0.00000369, Cur Avg Loss: 0.00028638, Log Avg loss: 0.00022594, Global Avg Loss: 0.00183753, Time: 0.0128
Training, Epoch: 0022, Batch: 076915, Sample Num: 76915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00028853, Log Avg loss: 0.00032760, Global Avg Loss: 0.00183589, Time: 0.0146
Training, Epoch: 0022, Batch: 080915, Sample Num: 80915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00028673, Log Avg loss: 0.00025226, Global Avg Loss: 0.00183417, Time: 0.0129
Training, Epoch: 0022, Batch: 084915, Sample Num: 84915, Cur Loss: 0.00000001, Cur Avg Loss: 0.00028262, Log Avg loss: 0.00019950, Global Avg Loss: 0.00183239, Time: 0.0147
Training, Epoch: 0022, Batch: 088915, Sample Num: 88915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00027977, Log Avg loss: 0.00021926, Global Avg Loss: 0.00183064, Time: 0.0125
Training, Epoch: 0022, Batch: 092915, Sample Num: 92915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00028006, Log Avg loss: 0.00028632, Global Avg Loss: 0.00182897, Time: 0.0492
Training, Epoch: 0022, Batch: 096915, Sample Num: 96915, Cur Loss: 0.00000005, Cur Avg Loss: 0.00027600, Log Avg loss: 0.00018179, Global Avg Loss: 0.00182719, Time: 0.0252
Training, Epoch: 0022, Batch: 100915, Sample Num: 100915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00029183, Log Avg loss: 0.00067542, Global Avg Loss: 0.00182594, Time: 0.0217
Training, Epoch: 0022, Batch: 104915, Sample Num: 104915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00029697, Log Avg loss: 0.00042672, Global Avg Loss: 0.00182443, Time: 0.0473
Training, Epoch: 0022, Batch: 108915, Sample Num: 108915, Cur Loss: 0.00000003, Cur Avg Loss: 0.00029713, Log Avg loss: 0.00030134, Global Avg Loss: 0.00182279, Time: 0.0378
Training, Epoch: 0022, Batch: 112915, Sample Num: 112915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00029885, Log Avg loss: 0.00034545, Global Avg Loss: 0.00182120, Time: 0.0137
Training, Epoch: 0022, Batch: 116915, Sample Num: 116915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00029940, Log Avg loss: 0.00031512, Global Avg Loss: 0.00181958, Time: 0.0310
Training, Epoch: 0022, Batch: 120915, Sample Num: 120915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00029856, Log Avg loss: 0.00027395, Global Avg Loss: 0.00181791, Time: 0.0212
Training, Epoch: 0022, Batch: 124915, Sample Num: 124915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00030157, Log Avg loss: 0.00039241, Global Avg Loss: 0.00181638, Time: 0.0264
Training, Epoch: 0022, Batch: 128915, Sample Num: 128915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00030878, Log Avg loss: 0.00053413, Global Avg Loss: 0.00181501, Time: 0.0447
Training, Epoch: 0022, Batch: 132915, Sample Num: 132915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00031138, Log Avg loss: 0.00039523, Global Avg Loss: 0.00181349, Time: 0.0571
Training, Epoch: 0022, Batch: 136915, Sample Num: 136915, Cur Loss: 0.00000016, Cur Avg Loss: 0.00031137, Log Avg loss: 0.00031107, Global Avg Loss: 0.00181188, Time: 0.0404
Training, Epoch: 0022, Batch: 140915, Sample Num: 140915, Cur Loss: 0.00000001, Cur Avg Loss: 0.00030979, Log Avg loss: 0.00025569, Global Avg Loss: 0.00181021, Time: 0.0178
Training, Epoch: 0022, Batch: 144915, Sample Num: 144915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00030948, Log Avg loss: 0.00029833, Global Avg Loss: 0.00180860, Time: 0.0125
Training, Epoch: 0022, Batch: 148915, Sample Num: 148915, Cur Loss: 0.00001060, Cur Avg Loss: 0.00031346, Log Avg loss: 0.00045761, Global Avg Loss: 0.00180716, Time: 0.0126
Training, Epoch: 0022, Batch: 152915, Sample Num: 152915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00031475, Log Avg loss: 0.00036296, Global Avg Loss: 0.00180562, Time: 0.0367
Training, Epoch: 0022, Batch: 156915, Sample Num: 156915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00031634, Log Avg loss: 0.00037703, Global Avg Loss: 0.00180409, Time: 0.0234
Training, Epoch: 0022, Batch: 160915, Sample Num: 160915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00031842, Log Avg loss: 0.00040009, Global Avg Loss: 0.00180260, Time: 0.0166
Training, Epoch: 0022, Batch: 164915, Sample Num: 164915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00031746, Log Avg loss: 0.00027897, Global Avg Loss: 0.00180098, Time: 0.0136
Training, Epoch: 0022, Batch: 168915, Sample Num: 168915, Cur Loss: 0.00000000, Cur Avg Loss: 0.00031985, Log Avg loss: 0.00041841, Global Avg Loss: 0.00179951, Time: 0.0366
***** Running evaluation checkpoint-3770470 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-3770470 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 4030.135402, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.18369, "eval_total_loss": 3931.692586, "eval_acc": 0.962811, "eval_prec": 0.967784, "eval_recall": 0.967781, "eval_f1": 0.967628, "eval_top2_acc": 0.994067, "eval_top3_acc": 0.998552, "eval_top5_acc": 0.999813, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999441, "eval_pr_auc": 0.987174, "eval_mcc": 0.95871, "eval_sn": 0.967781, "eval_sp": 0.998581, "update_flag": false, "test_avg_loss": 0.179766, "test_total_loss": 3847.701532, "test_acc": 0.964352, "test_prec": 0.982179, "test_recall": 0.980402, "test_f1": 0.981255, "test_top2_acc": 0.993506, "test_top3_acc": 0.998739, "test_top5_acc": 0.99986, "test_top10_acc": 1.0, "test_roc_auc": 0.999422, "test_pr_auc": 0.99298, "test_mcc": 0.960428, "test_sn": 0.980402, "test_sp": 0.998628, "lr": 0.00011287142120923723, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0017986615608070323, "train_cur_epoch_loss": 55.257539846790095, "train_cur_epoch_avg_loss": 0.00032241759691215737, "train_cur_epoch_time": 4030.135402202606, "train_cur_epoch_avg_time": 0.023515099934081782, "epoch": 22, "step": 3770470}
##################################################
Training, Epoch: 0023, Batch: 001530, Sample Num: 1530, Cur Loss: 0.00000010, Cur Avg Loss: 0.00012042, Log Avg loss: 0.00035342, Global Avg Loss: 0.00179798, Time: 0.0124
Training, Epoch: 0023, Batch: 005530, Sample Num: 5530, Cur Loss: 0.00000003, Cur Avg Loss: 0.00019483, Log Avg loss: 0.00022330, Global Avg Loss: 0.00179631, Time: 0.0127
Training, Epoch: 0023, Batch: 009530, Sample Num: 9530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022131, Log Avg loss: 0.00025791, Global Avg Loss: 0.00179468, Time: 0.0131
Training, Epoch: 0023, Batch: 013530, Sample Num: 13530, Cur Loss: 0.00000136, Cur Avg Loss: 0.00020682, Log Avg loss: 0.00017230, Global Avg Loss: 0.00179297, Time: 0.0176
Training, Epoch: 0023, Batch: 017530, Sample Num: 17530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021195, Log Avg loss: 0.00022930, Global Avg Loss: 0.00179132, Time: 0.0381
Training, Epoch: 0023, Batch: 021530, Sample Num: 21530, Cur Loss: 0.00000014, Cur Avg Loss: 0.00024777, Log Avg loss: 0.00040476, Global Avg Loss: 0.00178986, Time: 0.0292
Training, Epoch: 0023, Batch: 025530, Sample Num: 25530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00025618, Log Avg loss: 0.00030142, Global Avg Loss: 0.00178829, Time: 0.0258
Training, Epoch: 0023, Batch: 029530, Sample Num: 29530, Cur Loss: 0.00000001, Cur Avg Loss: 0.00026732, Log Avg loss: 0.00033845, Global Avg Loss: 0.00178676, Time: 0.0438
Training, Epoch: 0023, Batch: 033530, Sample Num: 33530, Cur Loss: 0.00000998, Cur Avg Loss: 0.00027204, Log Avg loss: 0.00030690, Global Avg Loss: 0.00178521, Time: 0.0177
Training, Epoch: 0023, Batch: 037530, Sample Num: 37530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00027251, Log Avg loss: 0.00027646, Global Avg Loss: 0.00178362, Time: 0.0260
Training, Epoch: 0023, Batch: 041530, Sample Num: 41530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00026771, Log Avg loss: 0.00022264, Global Avg Loss: 0.00178198, Time: 0.0124
Training, Epoch: 0023, Batch: 045530, Sample Num: 45530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00027058, Log Avg loss: 0.00030041, Global Avg Loss: 0.00178043, Time: 0.0123
Training, Epoch: 0023, Batch: 049530, Sample Num: 49530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00027164, Log Avg loss: 0.00028364, Global Avg Loss: 0.00177886, Time: 0.0179
Training, Epoch: 0023, Batch: 053530, Sample Num: 53530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00026799, Log Avg loss: 0.00022290, Global Avg Loss: 0.00177723, Time: 0.0143
Training, Epoch: 0023, Batch: 057530, Sample Num: 57530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00026384, Log Avg loss: 0.00020828, Global Avg Loss: 0.00177560, Time: 0.0171
Training, Epoch: 0023, Batch: 061530, Sample Num: 61530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00026539, Log Avg loss: 0.00028766, Global Avg Loss: 0.00177404, Time: 0.0190
Training, Epoch: 0023, Batch: 065530, Sample Num: 65530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00027041, Log Avg loss: 0.00034758, Global Avg Loss: 0.00177255, Time: 0.0125
Training, Epoch: 0023, Batch: 069530, Sample Num: 69530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00028368, Log Avg loss: 0.00050117, Global Avg Loss: 0.00177123, Time: 0.0174
Training, Epoch: 0023, Batch: 073530, Sample Num: 73530, Cur Loss: 0.00002211, Cur Avg Loss: 0.00028170, Log Avg loss: 0.00024719, Global Avg Loss: 0.00176964, Time: 0.0147
Training, Epoch: 0023, Batch: 077530, Sample Num: 77530, Cur Loss: 0.00000241, Cur Avg Loss: 0.00028030, Log Avg loss: 0.00025453, Global Avg Loss: 0.00176807, Time: 0.0271
Training, Epoch: 0023, Batch: 081530, Sample Num: 81530, Cur Loss: 0.00000003, Cur Avg Loss: 0.00028554, Log Avg loss: 0.00038712, Global Avg Loss: 0.00176664, Time: 0.0175
Training, Epoch: 0023, Batch: 085530, Sample Num: 85530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00029053, Log Avg loss: 0.00039242, Global Avg Loss: 0.00176521, Time: 0.0209
Training, Epoch: 0023, Batch: 089530, Sample Num: 89530, Cur Loss: 0.00000035, Cur Avg Loss: 0.00028307, Log Avg loss: 0.00012349, Global Avg Loss: 0.00176351, Time: 0.0208
Training, Epoch: 0023, Batch: 093530, Sample Num: 93530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00028332, Log Avg loss: 0.00028888, Global Avg Loss: 0.00176198, Time: 0.0192
Training, Epoch: 0023, Batch: 097530, Sample Num: 97530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00028177, Log Avg loss: 0.00024541, Global Avg Loss: 0.00176041, Time: 0.0384
Training, Epoch: 0023, Batch: 101530, Sample Num: 101530, Cur Loss: 0.00000044, Cur Avg Loss: 0.00028512, Log Avg loss: 0.00036699, Global Avg Loss: 0.00175897, Time: 0.0230
Training, Epoch: 0023, Batch: 105530, Sample Num: 105530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00028194, Log Avg loss: 0.00020108, Global Avg Loss: 0.00175737, Time: 0.0196
Training, Epoch: 0023, Batch: 109530, Sample Num: 109530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00028193, Log Avg loss: 0.00028168, Global Avg Loss: 0.00175585, Time: 0.0216
Training, Epoch: 0023, Batch: 113530, Sample Num: 113530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00028568, Log Avg loss: 0.00038835, Global Avg Loss: 0.00175444, Time: 0.0248
Training, Epoch: 0023, Batch: 117530, Sample Num: 117530, Cur Loss: 0.00000001, Cur Avg Loss: 0.00028353, Log Avg loss: 0.00022247, Global Avg Loss: 0.00175286, Time: 0.0426
Training, Epoch: 0023, Batch: 121530, Sample Num: 121530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00028750, Log Avg loss: 0.00040435, Global Avg Loss: 0.00175147, Time: 0.0193
Training, Epoch: 0023, Batch: 125530, Sample Num: 125530, Cur Loss: 0.00033963, Cur Avg Loss: 0.00029080, Log Avg loss: 0.00039092, Global Avg Loss: 0.00175008, Time: 0.0383
Training, Epoch: 0023, Batch: 129530, Sample Num: 129530, Cur Loss: 0.00001892, Cur Avg Loss: 0.00029004, Log Avg loss: 0.00026613, Global Avg Loss: 0.00174856, Time: 0.0128
Training, Epoch: 0023, Batch: 133530, Sample Num: 133530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00029018, Log Avg loss: 0.00029474, Global Avg Loss: 0.00174707, Time: 0.0190
Training, Epoch: 0023, Batch: 137530, Sample Num: 137530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00029303, Log Avg loss: 0.00038835, Global Avg Loss: 0.00174568, Time: 0.0480
Training, Epoch: 0023, Batch: 141530, Sample Num: 141530, Cur Loss: 0.00000003, Cur Avg Loss: 0.00029484, Log Avg loss: 0.00035687, Global Avg Loss: 0.00174426, Time: 0.0123
Training, Epoch: 0023, Batch: 145530, Sample Num: 145530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00029919, Log Avg loss: 0.00045329, Global Avg Loss: 0.00174294, Time: 0.0130
Training, Epoch: 0023, Batch: 149530, Sample Num: 149530, Cur Loss: 0.00000003, Cur Avg Loss: 0.00029753, Log Avg loss: 0.00023695, Global Avg Loss: 0.00174140, Time: 0.0206
Training, Epoch: 0023, Batch: 153530, Sample Num: 153530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00029762, Log Avg loss: 0.00030112, Global Avg Loss: 0.00173993, Time: 0.0167
Training, Epoch: 0023, Batch: 157530, Sample Num: 157530, Cur Loss: 0.00000001, Cur Avg Loss: 0.00029907, Log Avg loss: 0.00035484, Global Avg Loss: 0.00173852, Time: 0.0125
Training, Epoch: 0023, Batch: 161530, Sample Num: 161530, Cur Loss: 0.00000000, Cur Avg Loss: 0.00030063, Log Avg loss: 0.00036182, Global Avg Loss: 0.00173712, Time: 0.0378
Training, Epoch: 0023, Batch: 165530, Sample Num: 165530, Cur Loss: 0.00000013, Cur Avg Loss: 0.00030014, Log Avg loss: 0.00028040, Global Avg Loss: 0.00173564, Time: 0.0143
Training, Epoch: 0023, Batch: 169530, Sample Num: 169530, Cur Loss: 0.00001442, Cur Avg Loss: 0.00030248, Log Avg loss: 0.00039933, Global Avg Loss: 0.00173428, Time: 0.0139
***** Running evaluation checkpoint-3941855 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-3941855 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 4084.847911, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.187025, "eval_total_loss": 4003.092868, "eval_acc": 0.964773, "eval_prec": 0.978057, "eval_recall": 0.972667, "eval_f1": 0.975306, "eval_top2_acc": 0.994721, "eval_top3_acc": 0.998785, "eval_top5_acc": 0.99986, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999381, "eval_pr_auc": 0.988164, "eval_mcc": 0.960868, "eval_sn": 0.972667, "eval_sp": 0.998645, "update_flag": false, "test_avg_loss": 0.189853, "test_total_loss": 4063.621428, "test_acc": 0.963558, "test_prec": 0.986728, "test_recall": 0.973806, "test_f1": 0.979304, "test_top2_acc": 0.994394, "test_top3_acc": 0.998598, "test_top5_acc": 0.99986, "test_top10_acc": 0.999953, "test_roc_auc": 0.999348, "test_pr_auc": 0.993501, "test_mcc": 0.95953, "test_sn": 0.973806, "test_sp": 0.998588, "lr": 0.00010884263665328756, "cur_epoch_step": 171385, "train_global_avg_loss": 0.001733585642707826, "train_cur_epoch_loss": 51.74377845886299, "train_cur_epoch_avg_loss": 0.00030191544451884934, "train_cur_epoch_time": 4084.8479108810425, "train_cur_epoch_avg_time": 0.02383433737422203, "epoch": 23, "step": 3941855}
##################################################
Training, Epoch: 0024, Batch: 002145, Sample Num: 2145, Cur Loss: 0.00000009, Cur Avg Loss: 0.00018105, Log Avg loss: 0.00021319, Global Avg Loss: 0.00173274, Time: 0.0318
Training, Epoch: 0024, Batch: 006145, Sample Num: 6145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017909, Log Avg loss: 0.00017804, Global Avg Loss: 0.00173117, Time: 0.0340
Training, Epoch: 0024, Batch: 010145, Sample Num: 10145, Cur Loss: 0.00000003, Cur Avg Loss: 0.00017362, Log Avg loss: 0.00016520, Global Avg Loss: 0.00172958, Time: 0.0143
Training, Epoch: 0024, Batch: 014145, Sample Num: 14145, Cur Loss: 0.00000057, Cur Avg Loss: 0.00016707, Log Avg loss: 0.00015046, Global Avg Loss: 0.00172798, Time: 0.0256
Training, Epoch: 0024, Batch: 018145, Sample Num: 18145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017903, Log Avg loss: 0.00022133, Global Avg Loss: 0.00172646, Time: 0.0320
Training, Epoch: 0024, Batch: 022145, Sample Num: 22145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00020750, Log Avg loss: 0.00033665, Global Avg Loss: 0.00172506, Time: 0.0615
Training, Epoch: 0024, Batch: 026145, Sample Num: 26145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00020366, Log Avg loss: 0.00018240, Global Avg Loss: 0.00172351, Time: 0.0717
Training, Epoch: 0024, Batch: 030145, Sample Num: 30145, Cur Loss: 0.00000489, Cur Avg Loss: 0.00021242, Log Avg loss: 0.00026970, Global Avg Loss: 0.00172204, Time: 0.0289
Training, Epoch: 0024, Batch: 034145, Sample Num: 34145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021500, Log Avg loss: 0.00023442, Global Avg Loss: 0.00172054, Time: 0.0201
Training, Epoch: 0024, Batch: 038145, Sample Num: 38145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021575, Log Avg loss: 0.00022212, Global Avg Loss: 0.00171904, Time: 0.0194
Training, Epoch: 0024, Batch: 042145, Sample Num: 42145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021114, Log Avg loss: 0.00016718, Global Avg Loss: 0.00171748, Time: 0.0523
Training, Epoch: 0024, Batch: 046145, Sample Num: 46145, Cur Loss: 0.00000001, Cur Avg Loss: 0.00021741, Log Avg loss: 0.00028348, Global Avg Loss: 0.00171604, Time: 0.0268
Training, Epoch: 0024, Batch: 050145, Sample Num: 50145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022150, Log Avg loss: 0.00026874, Global Avg Loss: 0.00171459, Time: 0.0141
Training, Epoch: 0024, Batch: 054145, Sample Num: 54145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022224, Log Avg loss: 0.00023146, Global Avg Loss: 0.00171311, Time: 0.0376
Training, Epoch: 0024, Batch: 058145, Sample Num: 58145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022686, Log Avg loss: 0.00028936, Global Avg Loss: 0.00171168, Time: 0.0287
Training, Epoch: 0024, Batch: 062145, Sample Num: 62145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022745, Log Avg loss: 0.00023610, Global Avg Loss: 0.00171021, Time: 0.0401
Training, Epoch: 0024, Batch: 066145, Sample Num: 66145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022682, Log Avg loss: 0.00021692, Global Avg Loss: 0.00170872, Time: 0.0592
Training, Epoch: 0024, Batch: 070145, Sample Num: 70145, Cur Loss: 0.00040789, Cur Avg Loss: 0.00022409, Log Avg loss: 0.00017896, Global Avg Loss: 0.00170719, Time: 0.0125
Training, Epoch: 0024, Batch: 074145, Sample Num: 74145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022377, Log Avg loss: 0.00021823, Global Avg Loss: 0.00170571, Time: 0.0206
Training, Epoch: 0024, Batch: 078145, Sample Num: 78145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022114, Log Avg loss: 0.00017230, Global Avg Loss: 0.00170419, Time: 0.0577
Training, Epoch: 0024, Batch: 082145, Sample Num: 82145, Cur Loss: 0.00000407, Cur Avg Loss: 0.00022442, Log Avg loss: 0.00028849, Global Avg Loss: 0.00170278, Time: 0.0814
Training, Epoch: 0024, Batch: 086145, Sample Num: 86145, Cur Loss: 0.00003583, Cur Avg Loss: 0.00022008, Log Avg loss: 0.00013105, Global Avg Loss: 0.00170122, Time: 0.0337
Training, Epoch: 0024, Batch: 090145, Sample Num: 90145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022444, Log Avg loss: 0.00031823, Global Avg Loss: 0.00169984, Time: 0.0144
Training, Epoch: 0024, Batch: 094145, Sample Num: 94145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022841, Log Avg loss: 0.00031808, Global Avg Loss: 0.00169848, Time: 0.0464
Training, Epoch: 0024, Batch: 098145, Sample Num: 98145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022574, Log Avg loss: 0.00016279, Global Avg Loss: 0.00169696, Time: 0.0149
Training, Epoch: 0024, Batch: 102145, Sample Num: 102145, Cur Loss: 0.00000047, Cur Avg Loss: 0.00022655, Log Avg loss: 0.00024654, Global Avg Loss: 0.00169552, Time: 0.0191
Training, Epoch: 0024, Batch: 106145, Sample Num: 106145, Cur Loss: 0.00000011, Cur Avg Loss: 0.00022631, Log Avg loss: 0.00022021, Global Avg Loss: 0.00169406, Time: 0.0220
Training, Epoch: 0024, Batch: 110145, Sample Num: 110145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022747, Log Avg loss: 0.00025821, Global Avg Loss: 0.00169265, Time: 0.0358
Training, Epoch: 0024, Batch: 114145, Sample Num: 114145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022898, Log Avg loss: 0.00027037, Global Avg Loss: 0.00169124, Time: 0.0605
Training, Epoch: 0024, Batch: 118145, Sample Num: 118145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022940, Log Avg loss: 0.00024147, Global Avg Loss: 0.00168981, Time: 0.0131
Training, Epoch: 0024, Batch: 122145, Sample Num: 122145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022827, Log Avg loss: 0.00019507, Global Avg Loss: 0.00168834, Time: 0.0125
Training, Epoch: 0024, Batch: 126145, Sample Num: 126145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022880, Log Avg loss: 0.00024480, Global Avg Loss: 0.00168692, Time: 0.0126
Training, Epoch: 0024, Batch: 130145, Sample Num: 130145, Cur Loss: 0.00000004, Cur Avg Loss: 0.00022754, Log Avg loss: 0.00018768, Global Avg Loss: 0.00168545, Time: 0.0142
Training, Epoch: 0024, Batch: 134145, Sample Num: 134145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00023288, Log Avg loss: 0.00040668, Global Avg Loss: 0.00168420, Time: 0.0266
Training, Epoch: 0024, Batch: 138145, Sample Num: 138145, Cur Loss: 0.00002416, Cur Avg Loss: 0.00023170, Log Avg loss: 0.00019206, Global Avg Loss: 0.00168273, Time: 0.0125
Training, Epoch: 0024, Batch: 142145, Sample Num: 142145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022995, Log Avg loss: 0.00016956, Global Avg Loss: 0.00168125, Time: 0.0141
Training, Epoch: 0024, Batch: 146145, Sample Num: 146145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00023094, Log Avg loss: 0.00026625, Global Avg Loss: 0.00167987, Time: 0.0380
Training, Epoch: 0024, Batch: 150145, Sample Num: 150145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00023244, Log Avg loss: 0.00028718, Global Avg Loss: 0.00167851, Time: 0.0346
Training, Epoch: 0024, Batch: 154145, Sample Num: 154145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00023463, Log Avg loss: 0.00031689, Global Avg Loss: 0.00167718, Time: 0.0519
Training, Epoch: 0024, Batch: 158145, Sample Num: 158145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00023641, Log Avg loss: 0.00030515, Global Avg Loss: 0.00167584, Time: 0.0361
Training, Epoch: 0024, Batch: 162145, Sample Num: 162145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00023729, Log Avg loss: 0.00027175, Global Avg Loss: 0.00167447, Time: 0.0174
Training, Epoch: 0024, Batch: 166145, Sample Num: 166145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00023571, Log Avg loss: 0.00017175, Global Avg Loss: 0.00167301, Time: 0.0196
Training, Epoch: 0024, Batch: 170145, Sample Num: 170145, Cur Loss: 0.00000000, Cur Avg Loss: 0.00023943, Log Avg loss: 0.00039398, Global Avg Loss: 0.00167176, Time: 0.0145
***** Running evaluation checkpoint-4113240 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-4113240 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 4520.562238, Avg time per batch (s): 0.030000
{"eval_avg_loss": 0.194298, "eval_total_loss": 4158.760687, "eval_acc": 0.963278, "eval_prec": 0.985039, "eval_recall": 0.970212, "eval_f1": 0.975647, "eval_top2_acc": 0.994347, "eval_top3_acc": 0.998925, "eval_top5_acc": 0.999813, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999384, "eval_pr_auc": 0.987943, "eval_mcc": 0.959221, "eval_sn": 0.970212, "eval_sp": 0.998594, "update_flag": false, "test_avg_loss": 0.191434, "test_total_loss": 4097.461306, "test_acc": 0.964306, "test_prec": 0.98637, "test_recall": 0.973189, "test_f1": 0.978729, "test_top2_acc": 0.99388, "test_top3_acc": 0.998739, "test_top5_acc": 0.999907, "test_top10_acc": 0.999953, "test_roc_auc": 0.999342, "test_pr_auc": 0.993122, "test_mcc": 0.96035, "test_sn": 0.973189, "test_sp": 0.998623, "lr": 0.00010481385209733785, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0016713222386753061, "train_cur_epoch_loss": 41.00625137178252, "train_cur_epoch_avg_loss": 0.0002392639459216531, "train_cur_epoch_time": 4520.562237501144, "train_cur_epoch_avg_time": 0.02637665045074624, "epoch": 24, "step": 4113240}
##################################################
Steps: 4115000, Updated lr: 0.000105
Training, Epoch: 0025, Batch: 002760, Sample Num: 2760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00013696, Log Avg loss: 0.00016169, Global Avg Loss: 0.00167029, Time: 0.0155
Steps: 4119000, Updated lr: 0.000105
Training, Epoch: 0025, Batch: 006760, Sample Num: 6760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017036, Log Avg loss: 0.00019340, Global Avg Loss: 0.00166886, Time: 0.0483
Steps: 4123000, Updated lr: 0.000105
Training, Epoch: 0025, Batch: 010760, Sample Num: 10760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014972, Log Avg loss: 0.00011483, Global Avg Loss: 0.00166735, Time: 0.0160
Steps: 4127000, Updated lr: 0.000104
Training, Epoch: 0025, Batch: 014760, Sample Num: 14760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014901, Log Avg loss: 0.00014711, Global Avg Loss: 0.00166588, Time: 0.0295
Steps: 4131000, Updated lr: 0.000104
Training, Epoch: 0025, Batch: 018760, Sample Num: 18760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00016111, Log Avg loss: 0.00020575, Global Avg Loss: 0.00166447, Time: 0.0129
Steps: 4135000, Updated lr: 0.000104
Training, Epoch: 0025, Batch: 022760, Sample Num: 22760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00016675, Log Avg loss: 0.00019322, Global Avg Loss: 0.00166304, Time: 0.0190
Steps: 4139000, Updated lr: 0.000104
Training, Epoch: 0025, Batch: 026760, Sample Num: 26760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00018511, Log Avg loss: 0.00028955, Global Avg Loss: 0.00166172, Time: 0.0142
Steps: 4143000, Updated lr: 0.000104
Training, Epoch: 0025, Batch: 030760, Sample Num: 30760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00018459, Log Avg loss: 0.00018110, Global Avg Loss: 0.00166029, Time: 0.0189
Steps: 4147000, Updated lr: 0.000104
Training, Epoch: 0025, Batch: 034760, Sample Num: 34760, Cur Loss: 0.00000806, Cur Avg Loss: 0.00018815, Log Avg loss: 0.00021556, Global Avg Loss: 0.00165889, Time: 0.0143
Steps: 4151000, Updated lr: 0.000104
Training, Epoch: 0025, Batch: 038760, Sample Num: 38760, Cur Loss: 0.00000019, Cur Avg Loss: 0.00019518, Log Avg loss: 0.00025628, Global Avg Loss: 0.00165754, Time: 0.0126
Steps: 4155000, Updated lr: 0.000104
Training, Epoch: 0025, Batch: 042760, Sample Num: 42760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00019944, Log Avg loss: 0.00024066, Global Avg Loss: 0.00165618, Time: 0.0122
Steps: 4159000, Updated lr: 0.000104
Training, Epoch: 0025, Batch: 046760, Sample Num: 46760, Cur Loss: 0.00000004, Cur Avg Loss: 0.00019853, Log Avg loss: 0.00018885, Global Avg Loss: 0.00165477, Time: 0.0138
Steps: 4163000, Updated lr: 0.000104
Training, Epoch: 0025, Batch: 050760, Sample Num: 50760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00019389, Log Avg loss: 0.00013966, Global Avg Loss: 0.00165331, Time: 0.0140
Steps: 4167000, Updated lr: 0.000104
Training, Epoch: 0025, Batch: 054760, Sample Num: 54760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00019469, Log Avg loss: 0.00020481, Global Avg Loss: 0.00165192, Time: 0.0374
Steps: 4171000, Updated lr: 0.000103
Training, Epoch: 0025, Batch: 058760, Sample Num: 58760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00020044, Log Avg loss: 0.00027920, Global Avg Loss: 0.00165061, Time: 0.0163
Steps: 4175000, Updated lr: 0.000103
Training, Epoch: 0025, Batch: 062760, Sample Num: 62760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00020260, Log Avg loss: 0.00023430, Global Avg Loss: 0.00164925, Time: 0.0130
Steps: 4179000, Updated lr: 0.000103
Training, Epoch: 0025, Batch: 066760, Sample Num: 66760, Cur Loss: 0.00000053, Cur Avg Loss: 0.00021149, Log Avg loss: 0.00035100, Global Avg Loss: 0.00164801, Time: 0.0124
Steps: 4183000, Updated lr: 0.000103
Training, Epoch: 0025, Batch: 070760, Sample Num: 70760, Cur Loss: 0.00002515, Cur Avg Loss: 0.00021222, Log Avg loss: 0.00022437, Global Avg Loss: 0.00164665, Time: 0.0505
Steps: 4187000, Updated lr: 0.000103
Training, Epoch: 0025, Batch: 074760, Sample Num: 74760, Cur Loss: 0.00000009, Cur Avg Loss: 0.00021738, Log Avg loss: 0.00030870, Global Avg Loss: 0.00164537, Time: 0.0369
Steps: 4191000, Updated lr: 0.000103
Training, Epoch: 0025, Batch: 078760, Sample Num: 78760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021387, Log Avg loss: 0.00014822, Global Avg Loss: 0.00164394, Time: 0.0151
Steps: 4195000, Updated lr: 0.000103
Training, Epoch: 0025, Batch: 082760, Sample Num: 82760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021551, Log Avg loss: 0.00024778, Global Avg Loss: 0.00164261, Time: 0.0362
Steps: 4199000, Updated lr: 0.000103
Training, Epoch: 0025, Batch: 086760, Sample Num: 86760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021233, Log Avg loss: 0.00014665, Global Avg Loss: 0.00164118, Time: 0.0123
Steps: 4203000, Updated lr: 0.000103
Training, Epoch: 0025, Batch: 090760, Sample Num: 90760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021325, Log Avg loss: 0.00023301, Global Avg Loss: 0.00163984, Time: 0.0130
Steps: 4207000, Updated lr: 0.000103
Training, Epoch: 0025, Batch: 094760, Sample Num: 94760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021389, Log Avg loss: 0.00022847, Global Avg Loss: 0.00163850, Time: 0.0122
Steps: 4211000, Updated lr: 0.000103
Training, Epoch: 0025, Batch: 098760, Sample Num: 98760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021795, Log Avg loss: 0.00031416, Global Avg Loss: 0.00163724, Time: 0.0215
Steps: 4215000, Updated lr: 0.000102
Training, Epoch: 0025, Batch: 102760, Sample Num: 102760, Cur Loss: 0.00000001, Cur Avg Loss: 0.00022218, Log Avg loss: 0.00032662, Global Avg Loss: 0.00163600, Time: 0.0168
Steps: 4219000, Updated lr: 0.000102
Training, Epoch: 0025, Batch: 106760, Sample Num: 106760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022192, Log Avg loss: 0.00021531, Global Avg Loss: 0.00163465, Time: 0.0176
Steps: 4223000, Updated lr: 0.000102
Training, Epoch: 0025, Batch: 110760, Sample Num: 110760, Cur Loss: 0.00000206, Cur Avg Loss: 0.00021860, Log Avg loss: 0.00012989, Global Avg Loss: 0.00163323, Time: 0.0410
Steps: 4227000, Updated lr: 0.000102
Training, Epoch: 0025, Batch: 114760, Sample Num: 114760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021725, Log Avg loss: 0.00017985, Global Avg Loss: 0.00163185, Time: 0.0134
Steps: 4231000, Updated lr: 0.000102
Training, Epoch: 0025, Batch: 118760, Sample Num: 118760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021546, Log Avg loss: 0.00016418, Global Avg Loss: 0.00163047, Time: 0.0141
Steps: 4235000, Updated lr: 0.000102
Training, Epoch: 0025, Batch: 122760, Sample Num: 122760, Cur Loss: 0.00000004, Cur Avg Loss: 0.00022126, Log Avg loss: 0.00039355, Global Avg Loss: 0.00162930, Time: 0.0237
Steps: 4239000, Updated lr: 0.000102
Training, Epoch: 0025, Batch: 126760, Sample Num: 126760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022047, Log Avg loss: 0.00019621, Global Avg Loss: 0.00162795, Time: 0.0125
Steps: 4243000, Updated lr: 0.000102
Training, Epoch: 0025, Batch: 130760, Sample Num: 130760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022231, Log Avg loss: 0.00028062, Global Avg Loss: 0.00162668, Time: 0.0139
Steps: 4247000, Updated lr: 0.000102
Training, Epoch: 0025, Batch: 134760, Sample Num: 134760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022616, Log Avg loss: 0.00035204, Global Avg Loss: 0.00162548, Time: 0.0251
Steps: 4251000, Updated lr: 0.000102
Training, Epoch: 0025, Batch: 138760, Sample Num: 138760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022563, Log Avg loss: 0.00020760, Global Avg Loss: 0.00162414, Time: 0.0123
Steps: 4255000, Updated lr: 0.000101
Training, Epoch: 0025, Batch: 142760, Sample Num: 142760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022695, Log Avg loss: 0.00027263, Global Avg Loss: 0.00162287, Time: 0.0144
Steps: 4259000, Updated lr: 0.000101
Training, Epoch: 0025, Batch: 146760, Sample Num: 146760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022704, Log Avg loss: 0.00023028, Global Avg Loss: 0.00162157, Time: 0.0216
Steps: 4263000, Updated lr: 0.000101
Training, Epoch: 0025, Batch: 150760, Sample Num: 150760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022753, Log Avg loss: 0.00024571, Global Avg Loss: 0.00162027, Time: 0.0273
Steps: 4267000, Updated lr: 0.000101
Training, Epoch: 0025, Batch: 154760, Sample Num: 154760, Cur Loss: 0.00024759, Cur Avg Loss: 0.00022952, Log Avg loss: 0.00030454, Global Avg Loss: 0.00161904, Time: 0.0150
Steps: 4271000, Updated lr: 0.000101
Training, Epoch: 0025, Batch: 158760, Sample Num: 158760, Cur Loss: 0.00000002, Cur Avg Loss: 0.00023259, Log Avg loss: 0.00035146, Global Avg Loss: 0.00161785, Time: 0.0385
Steps: 4275000, Updated lr: 0.000101
Training, Epoch: 0025, Batch: 162760, Sample Num: 162760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00023204, Log Avg loss: 0.00021009, Global Avg Loss: 0.00161654, Time: 0.0147
Steps: 4279000, Updated lr: 0.000101
Training, Epoch: 0025, Batch: 166760, Sample Num: 166760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00023372, Log Avg loss: 0.00030202, Global Avg Loss: 0.00161531, Time: 0.0367
Steps: 4283000, Updated lr: 0.000101
Training, Epoch: 0025, Batch: 170760, Sample Num: 170760, Cur Loss: 0.00000000, Cur Avg Loss: 0.00023505, Log Avg loss: 0.00029071, Global Avg Loss: 0.00161407, Time: 0.0189
***** Running evaluation checkpoint-4284625 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-4284625 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3511.103500, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.199797, "eval_total_loss": 4276.445454, "eval_acc": 0.96197, "eval_prec": 0.978196, "eval_recall": 0.96962, "eval_f1": 0.973237, "eval_top2_acc": 0.99416, "eval_top3_acc": 0.998739, "eval_top5_acc": 0.999766, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.99939, "eval_pr_auc": 0.987811, "eval_mcc": 0.957809, "eval_sn": 0.96962, "eval_sp": 0.998547, "update_flag": false, "test_avg_loss": 0.194872, "test_total_loss": 4171.031643, "test_acc": 0.963091, "test_prec": 0.980249, "test_recall": 0.971898, "test_f1": 0.975686, "test_top2_acc": 0.993599, "test_top3_acc": 0.998692, "test_top5_acc": 0.999813, "test_top10_acc": 0.999953, "test_roc_auc": 0.999337, "test_pr_auc": 0.992989, "test_mcc": 0.959018, "test_sn": 0.971898, "test_sp": 0.998581, "lr": 0.00010078506754138815, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0016139067068849096, "train_cur_epoch_loss": 40.43553897684035, "train_cur_epoch_avg_loss": 0.00023593394390897888, "train_cur_epoch_time": 3511.103499650955, "train_cur_epoch_avg_time": 0.02048664410334017, "epoch": 25, "step": 4284625}
##################################################
Training, Epoch: 0026, Batch: 003375, Sample Num: 3375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00011754, Log Avg loss: 0.00017357, Global Avg Loss: 0.00161273, Time: 0.0212
Training, Epoch: 0026, Batch: 007375, Sample Num: 7375, Cur Loss: 0.00000413, Cur Avg Loss: 0.00014296, Log Avg loss: 0.00016441, Global Avg Loss: 0.00161138, Time: 0.0294
Training, Epoch: 0026, Batch: 011375, Sample Num: 11375, Cur Loss: 0.00000004, Cur Avg Loss: 0.00012943, Log Avg loss: 0.00010447, Global Avg Loss: 0.00160998, Time: 0.0234
Training, Epoch: 0026, Batch: 015375, Sample Num: 15375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00012518, Log Avg loss: 0.00011311, Global Avg Loss: 0.00160858, Time: 0.0243
Training, Epoch: 0026, Batch: 019375, Sample Num: 19375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00016297, Log Avg loss: 0.00030823, Global Avg Loss: 0.00160738, Time: 0.0194
Training, Epoch: 0026, Batch: 023375, Sample Num: 23375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00015272, Log Avg loss: 0.00010305, Global Avg Loss: 0.00160598, Time: 0.0139
Training, Epoch: 0026, Batch: 027375, Sample Num: 27375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00018110, Log Avg loss: 0.00034695, Global Avg Loss: 0.00160481, Time: 0.0141
Training, Epoch: 0026, Batch: 031375, Sample Num: 31375, Cur Loss: 0.00019881, Cur Avg Loss: 0.00017311, Log Avg loss: 0.00011840, Global Avg Loss: 0.00160343, Time: 0.0131
Training, Epoch: 0026, Batch: 035375, Sample Num: 35375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00018314, Log Avg loss: 0.00026185, Global Avg Loss: 0.00160219, Time: 0.0158
Training, Epoch: 0026, Batch: 039375, Sample Num: 39375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017954, Log Avg loss: 0.00014773, Global Avg Loss: 0.00160085, Time: 0.0140
Training, Epoch: 0026, Batch: 043375, Sample Num: 43375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00018691, Log Avg loss: 0.00025942, Global Avg Loss: 0.00159961, Time: 0.0459
Training, Epoch: 0026, Batch: 047375, Sample Num: 47375, Cur Loss: 0.00000005, Cur Avg Loss: 0.00019298, Log Avg loss: 0.00025875, Global Avg Loss: 0.00159837, Time: 0.0128
Training, Epoch: 0026, Batch: 051375, Sample Num: 51375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00018972, Log Avg loss: 0.00015119, Global Avg Loss: 0.00159703, Time: 0.0434
Training, Epoch: 0026, Batch: 055375, Sample Num: 55375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00019463, Log Avg loss: 0.00025772, Global Avg Loss: 0.00159580, Time: 0.0383
Training, Epoch: 0026, Batch: 059375, Sample Num: 59375, Cur Loss: 0.00000019, Cur Avg Loss: 0.00018813, Log Avg loss: 0.00009805, Global Avg Loss: 0.00159442, Time: 0.0121
Training, Epoch: 0026, Batch: 063375, Sample Num: 63375, Cur Loss: 0.00000009, Cur Avg Loss: 0.00018932, Log Avg loss: 0.00020708, Global Avg Loss: 0.00159314, Time: 0.0370
Training, Epoch: 0026, Batch: 067375, Sample Num: 67375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00019786, Log Avg loss: 0.00033311, Global Avg Loss: 0.00159198, Time: 0.0147
Training, Epoch: 0026, Batch: 071375, Sample Num: 71375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00019732, Log Avg loss: 0.00018832, Global Avg Loss: 0.00159070, Time: 0.0135
Training, Epoch: 0026, Batch: 075375, Sample Num: 75375, Cur Loss: 0.00000003, Cur Avg Loss: 0.00019731, Log Avg loss: 0.00019695, Global Avg Loss: 0.00158942, Time: 0.0149
Training, Epoch: 0026, Batch: 079375, Sample Num: 79375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00019676, Log Avg loss: 0.00018643, Global Avg Loss: 0.00158813, Time: 0.0147
Training, Epoch: 0026, Batch: 083375, Sample Num: 83375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00019610, Log Avg loss: 0.00018304, Global Avg Loss: 0.00158684, Time: 0.0480
Training, Epoch: 0026, Batch: 087375, Sample Num: 87375, Cur Loss: 0.00000005, Cur Avg Loss: 0.00020107, Log Avg loss: 0.00030477, Global Avg Loss: 0.00158567, Time: 0.0127
Training, Epoch: 0026, Batch: 091375, Sample Num: 91375, Cur Loss: 0.00000001, Cur Avg Loss: 0.00020384, Log Avg loss: 0.00026421, Global Avg Loss: 0.00158446, Time: 0.0126
Training, Epoch: 0026, Batch: 095375, Sample Num: 95375, Cur Loss: 0.00000048, Cur Avg Loss: 0.00020708, Log Avg loss: 0.00028112, Global Avg Loss: 0.00158327, Time: 0.0284
Training, Epoch: 0026, Batch: 099375, Sample Num: 99375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021047, Log Avg loss: 0.00029142, Global Avg Loss: 0.00158209, Time: 0.0521
Training, Epoch: 0026, Batch: 103375, Sample Num: 103375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021027, Log Avg loss: 0.00020509, Global Avg Loss: 0.00158084, Time: 0.0451
Training, Epoch: 0026, Batch: 107375, Sample Num: 107375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00020791, Log Avg loss: 0.00014694, Global Avg Loss: 0.00157953, Time: 0.0194
Training, Epoch: 0026, Batch: 111375, Sample Num: 111375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00020774, Log Avg loss: 0.00020341, Global Avg Loss: 0.00157828, Time: 0.0146
Training, Epoch: 0026, Batch: 115375, Sample Num: 115375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00020815, Log Avg loss: 0.00021933, Global Avg Loss: 0.00157705, Time: 0.0150
Training, Epoch: 0026, Batch: 119375, Sample Num: 119375, Cur Loss: 0.00000066, Cur Avg Loss: 0.00021028, Log Avg loss: 0.00027192, Global Avg Loss: 0.00157586, Time: 0.0128
Training, Epoch: 0026, Batch: 123375, Sample Num: 123375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021081, Log Avg loss: 0.00022643, Global Avg Loss: 0.00157464, Time: 0.0147
Training, Epoch: 0026, Batch: 127375, Sample Num: 127375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021155, Log Avg loss: 0.00023444, Global Avg Loss: 0.00157342, Time: 0.0212
Training, Epoch: 0026, Batch: 131375, Sample Num: 131375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021043, Log Avg loss: 0.00017473, Global Avg Loss: 0.00157215, Time: 0.0479
Training, Epoch: 0026, Batch: 135375, Sample Num: 135375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021057, Log Avg loss: 0.00021527, Global Avg Loss: 0.00157093, Time: 0.0231
Training, Epoch: 0026, Batch: 139375, Sample Num: 139375, Cur Loss: 0.00000211, Cur Avg Loss: 0.00021170, Log Avg loss: 0.00024994, Global Avg Loss: 0.00156973, Time: 0.0473
Training, Epoch: 0026, Batch: 143375, Sample Num: 143375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021242, Log Avg loss: 0.00023753, Global Avg Loss: 0.00156853, Time: 0.0422
Training, Epoch: 0026, Batch: 147375, Sample Num: 147375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00020960, Log Avg loss: 0.00010837, Global Avg Loss: 0.00156721, Time: 0.0270
Training, Epoch: 0026, Batch: 151375, Sample Num: 151375, Cur Loss: 0.00000001, Cur Avg Loss: 0.00021165, Log Avg loss: 0.00028713, Global Avg Loss: 0.00156606, Time: 0.0228
Training, Epoch: 0026, Batch: 155375, Sample Num: 155375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021432, Log Avg loss: 0.00031560, Global Avg Loss: 0.00156493, Time: 0.0203
Training, Epoch: 0026, Batch: 159375, Sample Num: 159375, Cur Loss: 0.00000013, Cur Avg Loss: 0.00021410, Log Avg loss: 0.00020548, Global Avg Loss: 0.00156371, Time: 0.0189
Training, Epoch: 0026, Batch: 163375, Sample Num: 163375, Cur Loss: 0.00000004, Cur Avg Loss: 0.00021609, Log Avg loss: 0.00029527, Global Avg Loss: 0.00156256, Time: 0.0361
Training, Epoch: 0026, Batch: 167375, Sample Num: 167375, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021985, Log Avg loss: 0.00037332, Global Avg Loss: 0.00156150, Time: 0.0356
Training, Epoch: 0026, Batch: 171375, Sample Num: 171375, Cur Loss: 0.00000001, Cur Avg Loss: 0.00022037, Log Avg loss: 0.00024245, Global Avg Loss: 0.00156031, Time: 0.0172
***** Running evaluation checkpoint-4456010 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-4456010 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3629.755070, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.211872, "eval_total_loss": 4534.90549, "eval_acc": 0.961175, "eval_prec": 0.975176, "eval_recall": 0.964024, "eval_f1": 0.969262, "eval_top2_acc": 0.994347, "eval_top3_acc": 0.998598, "eval_top5_acc": 0.99986, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999309, "eval_pr_auc": 0.986727, "eval_mcc": 0.956916, "eval_sn": 0.964024, "eval_sp": 0.998506, "update_flag": false, "test_avg_loss": 0.202804, "test_total_loss": 4340.819936, "test_acc": 0.963325, "test_prec": 0.983495, "test_recall": 0.975253, "test_f1": 0.979055, "test_top2_acc": 0.993973, "test_top3_acc": 0.998739, "test_top5_acc": 0.999813, "test_top10_acc": 1.0, "test_roc_auc": 0.999261, "test_pr_auc": 0.993234, "test_mcc": 0.959271, "test_sn": 0.975253, "test_sp": 0.998584, "lr": 9.675628298543845e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0015603088881243298, "train_cur_epoch_loss": 37.76698458312421, "train_cur_epoch_avg_loss": 0.0002203634191039135, "train_cur_epoch_time": 3629.7550699710846, "train_cur_epoch_avg_time": 0.021178954225697023, "epoch": 26, "step": 4456010}
##################################################
Training, Epoch: 0027, Batch: 003990, Sample Num: 3990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021169, Log Avg loss: 0.00021130, Global Avg Loss: 0.00155910, Time: 0.0475
Training, Epoch: 0027, Batch: 007990, Sample Num: 7990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00020259, Log Avg loss: 0.00019352, Global Avg Loss: 0.00155788, Time: 0.0127
Training, Epoch: 0027, Batch: 011990, Sample Num: 11990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00019063, Log Avg loss: 0.00016674, Global Avg Loss: 0.00155663, Time: 0.0268
Training, Epoch: 0027, Batch: 015990, Sample Num: 15990, Cur Loss: 0.00000048, Cur Avg Loss: 0.00021123, Log Avg loss: 0.00027296, Global Avg Loss: 0.00155549, Time: 0.0222
Training, Epoch: 0027, Batch: 019990, Sample Num: 19990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00021229, Log Avg loss: 0.00021653, Global Avg Loss: 0.00155429, Time: 0.0133
Training, Epoch: 0027, Batch: 023990, Sample Num: 23990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00022175, Log Avg loss: 0.00026905, Global Avg Loss: 0.00155314, Time: 0.0290
Training, Epoch: 0027, Batch: 027990, Sample Num: 27990, Cur Loss: 0.00000010, Cur Avg Loss: 0.00021387, Log Avg loss: 0.00016658, Global Avg Loss: 0.00155190, Time: 0.0183
Training, Epoch: 0027, Batch: 031990, Sample Num: 31990, Cur Loss: 0.00001319, Cur Avg Loss: 0.00020850, Log Avg loss: 0.00017093, Global Avg Loss: 0.00155067, Time: 0.0251
Training, Epoch: 0027, Batch: 035990, Sample Num: 35990, Cur Loss: 0.00000058, Cur Avg Loss: 0.00020717, Log Avg loss: 0.00019654, Global Avg Loss: 0.00154947, Time: 0.0140
Training, Epoch: 0027, Batch: 039990, Sample Num: 39990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00019645, Log Avg loss: 0.00010002, Global Avg Loss: 0.00154818, Time: 0.0126
Training, Epoch: 0027, Batch: 043990, Sample Num: 43990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00019010, Log Avg loss: 0.00012663, Global Avg Loss: 0.00154691, Time: 0.0135
Training, Epoch: 0027, Batch: 047990, Sample Num: 47990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00018306, Log Avg loss: 0.00010566, Global Avg Loss: 0.00154563, Time: 0.0150
Training, Epoch: 0027, Batch: 051990, Sample Num: 51990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017745, Log Avg loss: 0.00011014, Global Avg Loss: 0.00154436, Time: 0.0149
Training, Epoch: 0027, Batch: 055990, Sample Num: 55990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017231, Log Avg loss: 0.00010549, Global Avg Loss: 0.00154309, Time: 0.0367
Training, Epoch: 0027, Batch: 059990, Sample Num: 59990, Cur Loss: 0.00000241, Cur Avg Loss: 0.00017139, Log Avg loss: 0.00015842, Global Avg Loss: 0.00154186, Time: 0.0126
Training, Epoch: 0027, Batch: 063990, Sample Num: 63990, Cur Loss: 0.00000001, Cur Avg Loss: 0.00017613, Log Avg loss: 0.00024735, Global Avg Loss: 0.00154071, Time: 0.0151
Training, Epoch: 0027, Batch: 067990, Sample Num: 67990, Cur Loss: 0.00000011, Cur Avg Loss: 0.00017511, Log Avg loss: 0.00015871, Global Avg Loss: 0.00153949, Time: 0.0156
Training, Epoch: 0027, Batch: 071990, Sample Num: 71990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017391, Log Avg loss: 0.00015355, Global Avg Loss: 0.00153827, Time: 0.0126
Training, Epoch: 0027, Batch: 075990, Sample Num: 75990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017287, Log Avg loss: 0.00015418, Global Avg Loss: 0.00153705, Time: 0.0153
Training, Epoch: 0027, Batch: 079990, Sample Num: 79990, Cur Loss: 0.00000840, Cur Avg Loss: 0.00017165, Log Avg loss: 0.00014849, Global Avg Loss: 0.00153582, Time: 0.0121
Training, Epoch: 0027, Batch: 083990, Sample Num: 83990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017538, Log Avg loss: 0.00024997, Global Avg Loss: 0.00153469, Time: 0.0155
Training, Epoch: 0027, Batch: 087990, Sample Num: 87990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017599, Log Avg loss: 0.00018882, Global Avg Loss: 0.00153350, Time: 0.0144
Training, Epoch: 0027, Batch: 091990, Sample Num: 91990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017397, Log Avg loss: 0.00012953, Global Avg Loss: 0.00153227, Time: 0.0185
Training, Epoch: 0027, Batch: 095990, Sample Num: 95990, Cur Loss: 0.00000071, Cur Avg Loss: 0.00017504, Log Avg loss: 0.00019945, Global Avg Loss: 0.00153110, Time: 0.0151
Training, Epoch: 0027, Batch: 099990, Sample Num: 99990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017642, Log Avg loss: 0.00020971, Global Avg Loss: 0.00152994, Time: 0.0144
Training, Epoch: 0027, Batch: 103990, Sample Num: 103990, Cur Loss: 0.00001096, Cur Avg Loss: 0.00017870, Log Avg loss: 0.00023554, Global Avg Loss: 0.00152880, Time: 0.0124
Training, Epoch: 0027, Batch: 107990, Sample Num: 107990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017809, Log Avg loss: 0.00016239, Global Avg Loss: 0.00152760, Time: 0.0125
Training, Epoch: 0027, Batch: 111990, Sample Num: 111990, Cur Loss: 0.00000166, Cur Avg Loss: 0.00017746, Log Avg loss: 0.00016035, Global Avg Loss: 0.00152641, Time: 0.0508
Training, Epoch: 0027, Batch: 115990, Sample Num: 115990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017827, Log Avg loss: 0.00020104, Global Avg Loss: 0.00152525, Time: 0.0145
Training, Epoch: 0027, Batch: 119990, Sample Num: 119990, Cur Loss: 0.00962338, Cur Avg Loss: 0.00017859, Log Avg loss: 0.00018794, Global Avg Loss: 0.00152408, Time: 0.0126
Training, Epoch: 0027, Batch: 123990, Sample Num: 123990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00018305, Log Avg loss: 0.00031670, Global Avg Loss: 0.00152302, Time: 0.0161
Training, Epoch: 0027, Batch: 127990, Sample Num: 127990, Cur Loss: 0.00000215, Cur Avg Loss: 0.00018307, Log Avg loss: 0.00018376, Global Avg Loss: 0.00152186, Time: 0.0126
Training, Epoch: 0027, Batch: 131990, Sample Num: 131990, Cur Loss: 0.00000005, Cur Avg Loss: 0.00018776, Log Avg loss: 0.00033776, Global Avg Loss: 0.00152082, Time: 0.0127
Training, Epoch: 0027, Batch: 135990, Sample Num: 135990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00018707, Log Avg loss: 0.00016448, Global Avg Loss: 0.00151964, Time: 0.0122
Training, Epoch: 0027, Batch: 139990, Sample Num: 139990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00018602, Log Avg loss: 0.00015012, Global Avg Loss: 0.00151845, Time: 0.0123
Training, Epoch: 0027, Batch: 143990, Sample Num: 143990, Cur Loss: 0.00000001, Cur Avg Loss: 0.00018586, Log Avg loss: 0.00018021, Global Avg Loss: 0.00151729, Time: 0.0202
Training, Epoch: 0027, Batch: 147990, Sample Num: 147990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00018411, Log Avg loss: 0.00012118, Global Avg Loss: 0.00151607, Time: 0.0164
Training, Epoch: 0027, Batch: 151990, Sample Num: 151990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00018690, Log Avg loss: 0.00029028, Global Avg Loss: 0.00151501, Time: 0.0326
Training, Epoch: 0027, Batch: 155990, Sample Num: 155990, Cur Loss: 0.00000004, Cur Avg Loss: 0.00018742, Log Avg loss: 0.00020692, Global Avg Loss: 0.00151387, Time: 0.0124
Training, Epoch: 0027, Batch: 159990, Sample Num: 159990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00018897, Log Avg loss: 0.00024964, Global Avg Loss: 0.00151278, Time: 0.0145
Training, Epoch: 0027, Batch: 163990, Sample Num: 163990, Cur Loss: 0.00000000, Cur Avg Loss: 0.00018701, Log Avg loss: 0.00010842, Global Avg Loss: 0.00151156, Time: 0.0484
Training, Epoch: 0027, Batch: 167990, Sample Num: 167990, Cur Loss: 0.00000002, Cur Avg Loss: 0.00018664, Log Avg loss: 0.00017175, Global Avg Loss: 0.00151040, Time: 0.0359
***** Running evaluation checkpoint-4627395 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-4627395 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3605.819953, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.213699, "eval_total_loss": 4574.017737, "eval_acc": 0.963325, "eval_prec": 0.978904, "eval_recall": 0.968124, "eval_f1": 0.972836, "eval_top2_acc": 0.994113, "eval_top3_acc": 0.998598, "eval_top5_acc": 0.99986, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999364, "eval_pr_auc": 0.987357, "eval_mcc": 0.959272, "eval_sn": 0.968124, "eval_sp": 0.998594, "update_flag": false, "test_avg_loss": 0.208611, "test_total_loss": 4465.100888, "test_acc": 0.964679, "test_prec": 0.986446, "test_recall": 0.973286, "test_f1": 0.97881, "test_top2_acc": 0.99402, "test_top3_acc": 0.998692, "test_top5_acc": 0.999813, "test_top10_acc": 1.0, "test_roc_auc": 0.999319, "test_pr_auc": 0.993064, "test_mcc": 0.960766, "test_sn": 0.973286, "test_sp": 0.99864, "lr": 9.272749842948874e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0015094361675502114, "train_cur_epoch_loss": 32.00536596920269, "train_cur_epoch_avg_loss": 0.00018674543261780607, "train_cur_epoch_time": 3605.8199532032013, "train_cur_epoch_avg_time": 0.021039297215060836, "epoch": 27, "step": 4627395}
##################################################
Training, Epoch: 0028, Batch: 000605, Sample Num: 605, Cur Loss: 0.00000032, Cur Avg Loss: 0.00009848, Log Avg loss: 0.00017764, Global Avg Loss: 0.00150925, Time: 0.0129
Training, Epoch: 0028, Batch: 004605, Sample Num: 4605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00011860, Log Avg loss: 0.00012164, Global Avg Loss: 0.00150805, Time: 0.0164
Training, Epoch: 0028, Batch: 008605, Sample Num: 8605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00015495, Log Avg loss: 0.00019680, Global Avg Loss: 0.00150692, Time: 0.0297
Training, Epoch: 0028, Batch: 012605, Sample Num: 12605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00016508, Log Avg loss: 0.00018686, Global Avg Loss: 0.00150578, Time: 0.0155
Training, Epoch: 0028, Batch: 016605, Sample Num: 16605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014726, Log Avg loss: 0.00009113, Global Avg Loss: 0.00150457, Time: 0.0206
Training, Epoch: 0028, Batch: 020605, Sample Num: 20605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00015751, Log Avg loss: 0.00020004, Global Avg Loss: 0.00150344, Time: 0.0152
Training, Epoch: 0028, Batch: 024605, Sample Num: 24605, Cur Loss: 0.00000001, Cur Avg Loss: 0.00016089, Log Avg loss: 0.00017834, Global Avg Loss: 0.00150230, Time: 0.0336
Training, Epoch: 0028, Batch: 028605, Sample Num: 28605, Cur Loss: 0.00000003, Cur Avg Loss: 0.00015532, Log Avg loss: 0.00012101, Global Avg Loss: 0.00150112, Time: 0.0301
Training, Epoch: 0028, Batch: 032605, Sample Num: 32605, Cur Loss: 0.00000001, Cur Avg Loss: 0.00014253, Log Avg loss: 0.00005113, Global Avg Loss: 0.00149987, Time: 0.0356
Training, Epoch: 0028, Batch: 036605, Sample Num: 36605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00013659, Log Avg loss: 0.00008814, Global Avg Loss: 0.00149866, Time: 0.0332
Training, Epoch: 0028, Batch: 040605, Sample Num: 40605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014897, Log Avg loss: 0.00026225, Global Avg Loss: 0.00149760, Time: 0.0252
Training, Epoch: 0028, Batch: 044605, Sample Num: 44605, Cur Loss: 0.00000012, Cur Avg Loss: 0.00014701, Log Avg loss: 0.00012710, Global Avg Loss: 0.00149643, Time: 0.0170
Training, Epoch: 0028, Batch: 048605, Sample Num: 48605, Cur Loss: 0.00002224, Cur Avg Loss: 0.00014003, Log Avg loss: 0.00006216, Global Avg Loss: 0.00149520, Time: 0.0128
Training, Epoch: 0028, Batch: 052605, Sample Num: 52605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00013616, Log Avg loss: 0.00008915, Global Avg Loss: 0.00149400, Time: 0.0131
Training, Epoch: 0028, Batch: 056605, Sample Num: 56605, Cur Loss: 0.00000438, Cur Avg Loss: 0.00014662, Log Avg loss: 0.00028428, Global Avg Loss: 0.00149297, Time: 0.0164
Training, Epoch: 0028, Batch: 060605, Sample Num: 60605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014389, Log Avg loss: 0.00010521, Global Avg Loss: 0.00149178, Time: 0.0128
Training, Epoch: 0028, Batch: 064605, Sample Num: 64605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014737, Log Avg loss: 0.00020013, Global Avg Loss: 0.00149068, Time: 0.0163
Training, Epoch: 0028, Batch: 068605, Sample Num: 68605, Cur Loss: 0.00000110, Cur Avg Loss: 0.00015259, Log Avg loss: 0.00023693, Global Avg Loss: 0.00148961, Time: 0.0149
Training, Epoch: 0028, Batch: 072605, Sample Num: 72605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00015024, Log Avg loss: 0.00010991, Global Avg Loss: 0.00148844, Time: 0.0128
Training, Epoch: 0028, Batch: 076605, Sample Num: 76605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014783, Log Avg loss: 0.00010396, Global Avg Loss: 0.00148726, Time: 0.0487
Training, Epoch: 0028, Batch: 080605, Sample Num: 80605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00015344, Log Avg loss: 0.00026098, Global Avg Loss: 0.00148622, Time: 0.0141
Training, Epoch: 0028, Batch: 084605, Sample Num: 84605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00015100, Log Avg loss: 0.00010191, Global Avg Loss: 0.00148505, Time: 0.0128
Training, Epoch: 0028, Batch: 088605, Sample Num: 88605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014843, Log Avg loss: 0.00009400, Global Avg Loss: 0.00148387, Time: 0.0129
Training, Epoch: 0028, Batch: 092605, Sample Num: 92605, Cur Loss: 0.00000012, Cur Avg Loss: 0.00014677, Log Avg loss: 0.00010991, Global Avg Loss: 0.00148270, Time: 0.0126
Training, Epoch: 0028, Batch: 096605, Sample Num: 96605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00015433, Log Avg loss: 0.00032948, Global Avg Loss: 0.00148172, Time: 0.0142
Training, Epoch: 0028, Batch: 100605, Sample Num: 100605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00015484, Log Avg loss: 0.00016698, Global Avg Loss: 0.00148061, Time: 0.0206
Training, Epoch: 0028, Batch: 104605, Sample Num: 104605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00015393, Log Avg loss: 0.00013125, Global Avg Loss: 0.00147947, Time: 0.0153
Training, Epoch: 0028, Batch: 108605, Sample Num: 108605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00015684, Log Avg loss: 0.00023288, Global Avg Loss: 0.00147842, Time: 0.0147
Training, Epoch: 0028, Batch: 112605, Sample Num: 112605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00015562, Log Avg loss: 0.00012242, Global Avg Loss: 0.00147727, Time: 0.0310
Training, Epoch: 0028, Batch: 116605, Sample Num: 116605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00015947, Log Avg loss: 0.00026780, Global Avg Loss: 0.00147625, Time: 0.0450
Training, Epoch: 0028, Batch: 120605, Sample Num: 120605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00016017, Log Avg loss: 0.00018079, Global Avg Loss: 0.00147516, Time: 0.0175
Training, Epoch: 0028, Batch: 124605, Sample Num: 124605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00016192, Log Avg loss: 0.00021465, Global Avg Loss: 0.00147410, Time: 0.0137
Training, Epoch: 0028, Batch: 128605, Sample Num: 128605, Cur Loss: 0.00002205, Cur Avg Loss: 0.00016234, Log Avg loss: 0.00017538, Global Avg Loss: 0.00147301, Time: 0.0128
Training, Epoch: 0028, Batch: 132605, Sample Num: 132605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00016259, Log Avg loss: 0.00017054, Global Avg Loss: 0.00147192, Time: 0.0326
Training, Epoch: 0028, Batch: 136605, Sample Num: 136605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00016800, Log Avg loss: 0.00034741, Global Avg Loss: 0.00147097, Time: 0.0131
Training, Epoch: 0028, Batch: 140605, Sample Num: 140605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017107, Log Avg loss: 0.00027585, Global Avg Loss: 0.00146997, Time: 0.0418
Training, Epoch: 0028, Batch: 144605, Sample Num: 144605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017199, Log Avg loss: 0.00020447, Global Avg Loss: 0.00146891, Time: 0.0127
Training, Epoch: 0028, Batch: 148605, Sample Num: 148605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017154, Log Avg loss: 0.00015502, Global Avg Loss: 0.00146781, Time: 0.0163
Training, Epoch: 0028, Batch: 152605, Sample Num: 152605, Cur Loss: 0.00000001, Cur Avg Loss: 0.00017328, Log Avg loss: 0.00023804, Global Avg Loss: 0.00146678, Time: 0.0207
Training, Epoch: 0028, Batch: 156605, Sample Num: 156605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017437, Log Avg loss: 0.00021582, Global Avg Loss: 0.00146573, Time: 0.0128
Training, Epoch: 0028, Batch: 160605, Sample Num: 160605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017386, Log Avg loss: 0.00015427, Global Avg Loss: 0.00146464, Time: 0.0156
Training, Epoch: 0028, Batch: 164605, Sample Num: 164605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017289, Log Avg loss: 0.00013393, Global Avg Loss: 0.00146353, Time: 0.0335
Training, Epoch: 0028, Batch: 168605, Sample Num: 168605, Cur Loss: 0.00000000, Cur Avg Loss: 0.00017355, Log Avg loss: 0.00020070, Global Avg Loss: 0.00146247, Time: 0.0131
***** Running evaluation checkpoint-4798780 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-4798780 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3558.198020, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.212176, "eval_total_loss": 4541.407632, "eval_acc": 0.963932, "eval_prec": 0.973761, "eval_recall": 0.971776, "eval_f1": 0.972567, "eval_top2_acc": 0.994113, "eval_top3_acc": 0.998365, "eval_top5_acc": 0.99986, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999358, "eval_pr_auc": 0.988003, "eval_mcc": 0.959946, "eval_sn": 0.971776, "eval_sp": 0.998622, "update_flag": false, "test_avg_loss": 0.213736, "test_total_loss": 4574.795349, "test_acc": 0.962998, "test_prec": 0.978101, "test_recall": 0.977106, "test_f1": 0.977361, "test_top2_acc": 0.993459, "test_top3_acc": 0.998271, "test_top5_acc": 0.999766, "test_top10_acc": 1.0, "test_roc_auc": 0.999333, "test_pr_auc": 0.993034, "test_mcc": 0.958912, "test_sn": 0.977106, "test_sp": 0.998578, "lr": 8.869871387353905e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.001461676451096493, "train_cur_epoch_loss": 29.50634545090211, "train_cur_epoch_avg_loss": 0.000172164106840751, "train_cur_epoch_time": 3558.1980197429657, "train_cur_epoch_avg_time": 0.02076143197912866, "epoch": 28, "step": 4798780}
##################################################
Training, Epoch: 0029, Batch: 001220, Sample Num: 1220, Cur Loss: 0.00000001, Cur Avg Loss: 0.00007659, Log Avg loss: 0.00008443, Global Avg Loss: 0.00146132, Time: 0.0136
Training, Epoch: 0029, Batch: 005220, Sample Num: 5220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00012709, Log Avg loss: 0.00014249, Global Avg Loss: 0.00146023, Time: 0.0193
Training, Epoch: 0029, Batch: 009220, Sample Num: 9220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00011062, Log Avg loss: 0.00008912, Global Avg Loss: 0.00145909, Time: 0.0329
Training, Epoch: 0029, Batch: 013220, Sample Num: 13220, Cur Loss: 0.00218316, Cur Avg Loss: 0.00012867, Log Avg loss: 0.00017028, Global Avg Loss: 0.00145801, Time: 0.0123
Training, Epoch: 0029, Batch: 017220, Sample Num: 17220, Cur Loss: 0.00000097, Cur Avg Loss: 0.00012175, Log Avg loss: 0.00009890, Global Avg Loss: 0.00145689, Time: 0.0201
Training, Epoch: 0029, Batch: 021220, Sample Num: 21220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010815, Log Avg loss: 0.00004958, Global Avg Loss: 0.00145572, Time: 0.0170
Training, Epoch: 0029, Batch: 025220, Sample Num: 25220, Cur Loss: 0.00000009, Cur Avg Loss: 0.00011397, Log Avg loss: 0.00014484, Global Avg Loss: 0.00145463, Time: 0.0178
Training, Epoch: 0029, Batch: 029220, Sample Num: 29220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00011306, Log Avg loss: 0.00010735, Global Avg Loss: 0.00145351, Time: 0.0126
Training, Epoch: 0029, Batch: 033220, Sample Num: 33220, Cur Loss: 0.00000001, Cur Avg Loss: 0.00011464, Log Avg loss: 0.00012616, Global Avg Loss: 0.00145242, Time: 0.0131
Training, Epoch: 0029, Batch: 037220, Sample Num: 37220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00011316, Log Avg loss: 0.00010084, Global Avg Loss: 0.00145130, Time: 0.0155
Training, Epoch: 0029, Batch: 041220, Sample Num: 41220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00011350, Log Avg loss: 0.00011667, Global Avg Loss: 0.00145019, Time: 0.0290
Training, Epoch: 0029, Batch: 045220, Sample Num: 45220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00011652, Log Avg loss: 0.00014769, Global Avg Loss: 0.00144912, Time: 0.0358
Training, Epoch: 0029, Batch: 049220, Sample Num: 49220, Cur Loss: 0.00000061, Cur Avg Loss: 0.00011639, Log Avg loss: 0.00011492, Global Avg Loss: 0.00144802, Time: 0.0535
Training, Epoch: 0029, Batch: 053220, Sample Num: 53220, Cur Loss: 0.00000194, Cur Avg Loss: 0.00011636, Log Avg loss: 0.00011599, Global Avg Loss: 0.00144692, Time: 0.0440
Training, Epoch: 0029, Batch: 057220, Sample Num: 57220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00012684, Log Avg loss: 0.00026629, Global Avg Loss: 0.00144595, Time: 0.0188
Training, Epoch: 0029, Batch: 061220, Sample Num: 61220, Cur Loss: 0.00000001, Cur Avg Loss: 0.00012812, Log Avg loss: 0.00014632, Global Avg Loss: 0.00144488, Time: 0.0173
Training, Epoch: 0029, Batch: 065220, Sample Num: 65220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00012704, Log Avg loss: 0.00011057, Global Avg Loss: 0.00144378, Time: 0.0264
Training, Epoch: 0029, Batch: 069220, Sample Num: 69220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00013745, Log Avg loss: 0.00030724, Global Avg Loss: 0.00144285, Time: 0.0690
Training, Epoch: 0029, Batch: 073220, Sample Num: 73220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00013512, Log Avg loss: 0.00009471, Global Avg Loss: 0.00144174, Time: 0.0302
Training, Epoch: 0029, Batch: 077220, Sample Num: 77220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00013736, Log Avg loss: 0.00017836, Global Avg Loss: 0.00144070, Time: 0.0402
Training, Epoch: 0029, Batch: 081220, Sample Num: 81220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00013849, Log Avg loss: 0.00016026, Global Avg Loss: 0.00143965, Time: 0.0244
Training, Epoch: 0029, Batch: 085220, Sample Num: 85220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00013652, Log Avg loss: 0.00009659, Global Avg Loss: 0.00143855, Time: 0.0470
Training, Epoch: 0029, Batch: 089220, Sample Num: 89220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00013828, Log Avg loss: 0.00017570, Global Avg Loss: 0.00143752, Time: 0.0568
Training, Epoch: 0029, Batch: 093220, Sample Num: 93220, Cur Loss: 0.00000008, Cur Avg Loss: 0.00014000, Log Avg loss: 0.00017847, Global Avg Loss: 0.00143649, Time: 0.0398
Training, Epoch: 0029, Batch: 097220, Sample Num: 97220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014186, Log Avg loss: 0.00018526, Global Avg Loss: 0.00143547, Time: 0.0190
Training, Epoch: 0029, Batch: 101220, Sample Num: 101220, Cur Loss: 0.00000001, Cur Avg Loss: 0.00014075, Log Avg loss: 0.00011369, Global Avg Loss: 0.00143439, Time: 0.0323
Training, Epoch: 0029, Batch: 105220, Sample Num: 105220, Cur Loss: 0.00000288, Cur Avg Loss: 0.00014069, Log Avg loss: 0.00013913, Global Avg Loss: 0.00143333, Time: 0.0333
Training, Epoch: 0029, Batch: 109220, Sample Num: 109220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00013880, Log Avg loss: 0.00008927, Global Avg Loss: 0.00143224, Time: 0.0180
Training, Epoch: 0029, Batch: 113220, Sample Num: 113220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014106, Log Avg loss: 0.00020258, Global Avg Loss: 0.00143124, Time: 0.0315
Training, Epoch: 0029, Batch: 117220, Sample Num: 117220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014251, Log Avg loss: 0.00018366, Global Avg Loss: 0.00143022, Time: 0.0194
Training, Epoch: 0029, Batch: 121220, Sample Num: 121220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014461, Log Avg loss: 0.00020613, Global Avg Loss: 0.00142923, Time: 0.0142
Training, Epoch: 0029, Batch: 125220, Sample Num: 125220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014248, Log Avg loss: 0.00007801, Global Avg Loss: 0.00142813, Time: 0.0141
Training, Epoch: 0029, Batch: 129220, Sample Num: 129220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014471, Log Avg loss: 0.00021438, Global Avg Loss: 0.00142714, Time: 0.0163
Training, Epoch: 0029, Batch: 133220, Sample Num: 133220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014854, Log Avg loss: 0.00027229, Global Avg Loss: 0.00142621, Time: 0.0325
Training, Epoch: 0029, Batch: 137220, Sample Num: 137220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014924, Log Avg loss: 0.00017243, Global Avg Loss: 0.00142519, Time: 0.0192
Training, Epoch: 0029, Batch: 141220, Sample Num: 141220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014690, Log Avg loss: 0.00006672, Global Avg Loss: 0.00142409, Time: 0.0155
Training, Epoch: 0029, Batch: 145220, Sample Num: 145220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014912, Log Avg loss: 0.00022765, Global Avg Loss: 0.00142312, Time: 0.0123
Training, Epoch: 0029, Batch: 149220, Sample Num: 149220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014952, Log Avg loss: 0.00016404, Global Avg Loss: 0.00142211, Time: 0.0262
Training, Epoch: 0029, Batch: 153220, Sample Num: 153220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014961, Log Avg loss: 0.00015266, Global Avg Loss: 0.00142108, Time: 0.0350
Training, Epoch: 0029, Batch: 157220, Sample Num: 157220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014722, Log Avg loss: 0.00005570, Global Avg Loss: 0.00141998, Time: 0.0204
Training, Epoch: 0029, Batch: 161220, Sample Num: 161220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014620, Log Avg loss: 0.00010638, Global Avg Loss: 0.00141892, Time: 0.0152
Training, Epoch: 0029, Batch: 165220, Sample Num: 165220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014608, Log Avg loss: 0.00014123, Global Avg Loss: 0.00141789, Time: 0.0191
Training, Epoch: 0029, Batch: 169220, Sample Num: 169220, Cur Loss: 0.00000000, Cur Avg Loss: 0.00014847, Log Avg loss: 0.00024722, Global Avg Loss: 0.00141695, Time: 0.0315
***** Running evaluation checkpoint-4970165 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-4970165 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 4373.088035, Avg time per batch (s): 0.030000
{"eval_avg_loss": 0.219098, "eval_total_loss": 4689.578041, "eval_acc": 0.964352, "eval_prec": 0.979672, "eval_recall": 0.969819, "eval_f1": 0.974078, "eval_top2_acc": 0.994487, "eval_top3_acc": 0.998458, "eval_top5_acc": 0.999907, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999342, "eval_pr_auc": 0.987864, "eval_mcc": 0.960401, "eval_sn": 0.969819, "eval_sp": 0.998628, "update_flag": false, "test_avg_loss": 0.221722, "test_total_loss": 4745.740609, "test_acc": 0.963745, "test_prec": 0.986235, "test_recall": 0.976166, "test_f1": 0.980337, "test_top2_acc": 0.993739, "test_top3_acc": 0.998458, "test_top5_acc": 0.99986, "test_top10_acc": 1.0, "test_roc_auc": 0.999315, "test_pr_auc": 0.992466, "test_mcc": 0.959736, "test_sn": 0.976166, "test_sp": 0.998599, "lr": 8.466992931758935e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0014164167999003538, "train_cur_epoch_loss": 25.561484283054252, "train_cur_epoch_avg_loss": 0.00014914656640344402, "train_cur_epoch_time": 4373.088035106659, "train_cur_epoch_avg_time": 0.025516165563536242, "epoch": 29, "step": 4970165}
##################################################
Training, Epoch: 0030, Batch: 001835, Sample Num: 1835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005558, Log Avg loss: 0.00013472, Global Avg Loss: 0.00141591, Time: 0.0369
Training, Epoch: 0030, Batch: 005835, Sample Num: 5835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006373, Log Avg loss: 0.00006746, Global Avg Loss: 0.00141483, Time: 0.0187
Training, Epoch: 0030, Batch: 009835, Sample Num: 9835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006096, Log Avg loss: 0.00005693, Global Avg Loss: 0.00141374, Time: 0.0262
Training, Epoch: 0030, Batch: 013835, Sample Num: 13835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007791, Log Avg loss: 0.00011958, Global Avg Loss: 0.00141270, Time: 0.0146
Training, Epoch: 0030, Batch: 017835, Sample Num: 17835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007063, Log Avg loss: 0.00004543, Global Avg Loss: 0.00141160, Time: 0.0151
Training, Epoch: 0030, Batch: 021835, Sample Num: 21835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006449, Log Avg loss: 0.00003715, Global Avg Loss: 0.00141050, Time: 0.0142
Training, Epoch: 0030, Batch: 025835, Sample Num: 25835, Cur Loss: 0.00000008, Cur Avg Loss: 0.00006882, Log Avg loss: 0.00009246, Global Avg Loss: 0.00140945, Time: 0.0124
Training, Epoch: 0030, Batch: 029835, Sample Num: 29835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007785, Log Avg loss: 0.00013616, Global Avg Loss: 0.00140843, Time: 0.0125
Training, Epoch: 0030, Batch: 033835, Sample Num: 33835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009074, Log Avg loss: 0.00018690, Global Avg Loss: 0.00140745, Time: 0.0307
Training, Epoch: 0030, Batch: 037835, Sample Num: 37835, Cur Loss: 0.00000001, Cur Avg Loss: 0.00009851, Log Avg loss: 0.00016418, Global Avg Loss: 0.00140646, Time: 0.0433
Training, Epoch: 0030, Batch: 041835, Sample Num: 41835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009637, Log Avg loss: 0.00007617, Global Avg Loss: 0.00140540, Time: 0.0144
Training, Epoch: 0030, Batch: 045835, Sample Num: 45835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009505, Log Avg loss: 0.00008117, Global Avg Loss: 0.00140434, Time: 0.0346
Training, Epoch: 0030, Batch: 049835, Sample Num: 49835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009776, Log Avg loss: 0.00012883, Global Avg Loss: 0.00140333, Time: 0.0168
Training, Epoch: 0030, Batch: 053835, Sample Num: 53835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009618, Log Avg loss: 0.00007655, Global Avg Loss: 0.00140227, Time: 0.0148
Training, Epoch: 0030, Batch: 057835, Sample Num: 57835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009587, Log Avg loss: 0.00009167, Global Avg Loss: 0.00140123, Time: 0.0151
Training, Epoch: 0030, Batch: 061835, Sample Num: 61835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009442, Log Avg loss: 0.00007346, Global Avg Loss: 0.00140017, Time: 0.0154
Training, Epoch: 0030, Batch: 065835, Sample Num: 65835, Cur Loss: 0.00000470, Cur Avg Loss: 0.00009615, Log Avg loss: 0.00012287, Global Avg Loss: 0.00139916, Time: 0.0154
Training, Epoch: 0030, Batch: 069835, Sample Num: 69835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009980, Log Avg loss: 0.00015993, Global Avg Loss: 0.00139817, Time: 0.0128
Training, Epoch: 0030, Batch: 073835, Sample Num: 73835, Cur Loss: 0.00000001, Cur Avg Loss: 0.00010507, Log Avg loss: 0.00019711, Global Avg Loss: 0.00139722, Time: 0.0322
Training, Epoch: 0030, Batch: 077835, Sample Num: 77835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010671, Log Avg loss: 0.00013686, Global Avg Loss: 0.00139622, Time: 0.0162
Training, Epoch: 0030, Batch: 081835, Sample Num: 81835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010954, Log Avg loss: 0.00016469, Global Avg Loss: 0.00139525, Time: 0.0131
Training, Epoch: 0030, Batch: 085835, Sample Num: 85835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00011320, Log Avg loss: 0.00018814, Global Avg Loss: 0.00139429, Time: 0.0132
Training, Epoch: 0030, Batch: 089835, Sample Num: 89835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00011233, Log Avg loss: 0.00009348, Global Avg Loss: 0.00139326, Time: 0.0188
Training, Epoch: 0030, Batch: 093835, Sample Num: 93835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00011240, Log Avg loss: 0.00011407, Global Avg Loss: 0.00139225, Time: 0.0160
Training, Epoch: 0030, Batch: 097835, Sample Num: 97835, Cur Loss: 0.00000010, Cur Avg Loss: 0.00010960, Log Avg loss: 0.00004393, Global Avg Loss: 0.00139119, Time: 0.0596
Training, Epoch: 0030, Batch: 101835, Sample Num: 101835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010834, Log Avg loss: 0.00007757, Global Avg Loss: 0.00139015, Time: 0.0511
Training, Epoch: 0030, Batch: 105835, Sample Num: 105835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00011009, Log Avg loss: 0.00015443, Global Avg Loss: 0.00138918, Time: 0.0124
Training, Epoch: 0030, Batch: 109835, Sample Num: 109835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010791, Log Avg loss: 0.00005024, Global Avg Loss: 0.00138813, Time: 0.0149
Training, Epoch: 0030, Batch: 113835, Sample Num: 113835, Cur Loss: 0.00000016, Cur Avg Loss: 0.00011039, Log Avg loss: 0.00017847, Global Avg Loss: 0.00138717, Time: 0.0122
Training, Epoch: 0030, Batch: 117835, Sample Num: 117835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00011818, Log Avg loss: 0.00033990, Global Avg Loss: 0.00138635, Time: 0.0188
Training, Epoch: 0030, Batch: 121835, Sample Num: 121835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00011823, Log Avg loss: 0.00011981, Global Avg Loss: 0.00138536, Time: 0.0138
Training, Epoch: 0030, Batch: 125835, Sample Num: 125835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00011772, Log Avg loss: 0.00010234, Global Avg Loss: 0.00138435, Time: 0.0129
Training, Epoch: 0030, Batch: 129835, Sample Num: 129835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00011866, Log Avg loss: 0.00014797, Global Avg Loss: 0.00138338, Time: 0.0215
Training, Epoch: 0030, Batch: 133835, Sample Num: 133835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00011923, Log Avg loss: 0.00013779, Global Avg Loss: 0.00138240, Time: 0.0145
Training, Epoch: 0030, Batch: 137835, Sample Num: 137835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00012024, Log Avg loss: 0.00015394, Global Avg Loss: 0.00138144, Time: 0.0124
Training, Epoch: 0030, Batch: 141835, Sample Num: 141835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00011893, Log Avg loss: 0.00007400, Global Avg Loss: 0.00138042, Time: 0.0124
Training, Epoch: 0030, Batch: 145835, Sample Num: 145835, Cur Loss: 0.00000001, Cur Avg Loss: 0.00011806, Log Avg loss: 0.00008730, Global Avg Loss: 0.00137941, Time: 0.0126
Training, Epoch: 0030, Batch: 149835, Sample Num: 149835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00012011, Log Avg loss: 0.00019482, Global Avg Loss: 0.00137848, Time: 0.0130
Training, Epoch: 0030, Batch: 153835, Sample Num: 153835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00012025, Log Avg loss: 0.00012536, Global Avg Loss: 0.00137750, Time: 0.0198
Training, Epoch: 0030, Batch: 157835, Sample Num: 157835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00012044, Log Avg loss: 0.00012778, Global Avg Loss: 0.00137653, Time: 0.0299
Training, Epoch: 0030, Batch: 161835, Sample Num: 161835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00011983, Log Avg loss: 0.00009572, Global Avg Loss: 0.00137553, Time: 0.0146
Training, Epoch: 0030, Batch: 165835, Sample Num: 165835, Cur Loss: 0.00000014, Cur Avg Loss: 0.00012298, Log Avg loss: 0.00025054, Global Avg Loss: 0.00137465, Time: 0.0223
Training, Epoch: 0030, Batch: 169835, Sample Num: 169835, Cur Loss: 0.00000000, Cur Avg Loss: 0.00012320, Log Avg loss: 0.00013228, Global Avg Loss: 0.00137369, Time: 0.0261
***** Running evaluation checkpoint-5141550 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-5141550 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3456.015898, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.224503, "eval_total_loss": 4805.256426, "eval_acc": 0.964493, "eval_prec": 0.974993, "eval_recall": 0.971524, "eval_f1": 0.973057, "eval_top2_acc": 0.99402, "eval_top3_acc": 0.998692, "eval_top5_acc": 0.999907, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999361, "eval_pr_auc": 0.987437, "eval_mcc": 0.960571, "eval_sn": 0.971524, "eval_sp": 0.998638, "update_flag": false, "test_avg_loss": 0.224911, "test_total_loss": 4813.992138, "test_acc": 0.963839, "test_prec": 0.987429, "test_recall": 0.981265, "test_f1": 0.98412, "test_top2_acc": 0.994113, "test_top3_acc": 0.998598, "test_top5_acc": 0.999907, "test_top10_acc": 1.0, "test_roc_auc": 0.999299, "test_pr_auc": 0.993993, "test_mcc": 0.959833, "test_sn": 0.981265, "test_sp": 0.998606, "lr": 8.064114476163965e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0013733262733169429, "train_cur_epoch_loss": 21.20049629514264, "train_cur_epoch_avg_loss": 0.00012370100239310697, "train_cur_epoch_time": 3456.0158984661102, "train_cur_epoch_avg_time": 0.020165218067311086, "epoch": 30, "step": 5141550}
##################################################
Training, Epoch: 0031, Batch: 002450, Sample Num: 2450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001607, Log Avg loss: 0.00007899, Global Avg Loss: 0.00137268, Time: 0.0290
Training, Epoch: 0031, Batch: 006450, Sample Num: 6450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002712, Log Avg loss: 0.00003388, Global Avg Loss: 0.00137164, Time: 0.0370
Training, Epoch: 0031, Batch: 010450, Sample Num: 10450, Cur Loss: 0.00007322, Cur Avg Loss: 0.00003160, Log Avg loss: 0.00003883, Global Avg Loss: 0.00137060, Time: 0.0132
Training, Epoch: 0031, Batch: 014450, Sample Num: 14450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004412, Log Avg loss: 0.00007685, Global Avg Loss: 0.00136960, Time: 0.0129
Training, Epoch: 0031, Batch: 018450, Sample Num: 18450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006119, Log Avg loss: 0.00012285, Global Avg Loss: 0.00136863, Time: 0.0511
Training, Epoch: 0031, Batch: 022450, Sample Num: 22450, Cur Loss: 0.00000178, Cur Avg Loss: 0.00006109, Log Avg loss: 0.00006061, Global Avg Loss: 0.00136762, Time: 0.0150
Training, Epoch: 0031, Batch: 026450, Sample Num: 26450, Cur Loss: 0.00000012, Cur Avg Loss: 0.00005838, Log Avg loss: 0.00004320, Global Avg Loss: 0.00136660, Time: 0.0252
Training, Epoch: 0031, Batch: 030450, Sample Num: 30450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006445, Log Avg loss: 0.00010456, Global Avg Loss: 0.00136562, Time: 0.0352
Training, Epoch: 0031, Batch: 034450, Sample Num: 34450, Cur Loss: 0.01688309, Cur Avg Loss: 0.00006023, Log Avg loss: 0.00002809, Global Avg Loss: 0.00136459, Time: 0.0132
Training, Epoch: 0031, Batch: 038450, Sample Num: 38450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005914, Log Avg loss: 0.00004979, Global Avg Loss: 0.00136357, Time: 0.0123
Training, Epoch: 0031, Batch: 042450, Sample Num: 42450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005942, Log Avg loss: 0.00006206, Global Avg Loss: 0.00136257, Time: 0.0126
Training, Epoch: 0031, Batch: 046450, Sample Num: 46450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006118, Log Avg loss: 0.00007993, Global Avg Loss: 0.00136158, Time: 0.0173
Training, Epoch: 0031, Batch: 050450, Sample Num: 50450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006597, Log Avg loss: 0.00012159, Global Avg Loss: 0.00136062, Time: 0.0163
Training, Epoch: 0031, Batch: 054450, Sample Num: 54450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007441, Log Avg loss: 0.00018083, Global Avg Loss: 0.00135971, Time: 0.0128
Training, Epoch: 0031, Batch: 058450, Sample Num: 58450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008066, Log Avg loss: 0.00016571, Global Avg Loss: 0.00135880, Time: 0.0124
Training, Epoch: 0031, Batch: 062450, Sample Num: 62450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007826, Log Avg loss: 0.00004326, Global Avg Loss: 0.00135779, Time: 0.0208
Training, Epoch: 0031, Batch: 066450, Sample Num: 66450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007491, Log Avg loss: 0.00002263, Global Avg Loss: 0.00135676, Time: 0.0134
Training, Epoch: 0031, Batch: 070450, Sample Num: 70450, Cur Loss: 0.00000016, Cur Avg Loss: 0.00008300, Log Avg loss: 0.00021740, Global Avg Loss: 0.00135589, Time: 0.0283
Training, Epoch: 0031, Batch: 074450, Sample Num: 74450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009116, Log Avg loss: 0.00023473, Global Avg Loss: 0.00135503, Time: 0.0467
Training, Epoch: 0031, Batch: 078450, Sample Num: 78450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008979, Log Avg loss: 0.00006429, Global Avg Loss: 0.00135404, Time: 0.0195
Training, Epoch: 0031, Batch: 082450, Sample Num: 82450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009296, Log Avg loss: 0.00015528, Global Avg Loss: 0.00135312, Time: 0.0155
Training, Epoch: 0031, Batch: 086450, Sample Num: 86450, Cur Loss: 0.00000159, Cur Avg Loss: 0.00009601, Log Avg loss: 0.00015876, Global Avg Loss: 0.00135220, Time: 0.0162
Training, Epoch: 0031, Batch: 090450, Sample Num: 90450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009773, Log Avg loss: 0.00013499, Global Avg Loss: 0.00135127, Time: 0.0150
Training, Epoch: 0031, Batch: 094450, Sample Num: 94450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009894, Log Avg loss: 0.00012623, Global Avg Loss: 0.00135034, Time: 0.0175
Training, Epoch: 0031, Batch: 098450, Sample Num: 98450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009947, Log Avg loss: 0.00011208, Global Avg Loss: 0.00134939, Time: 0.0142
Training, Epoch: 0031, Batch: 102450, Sample Num: 102450, Cur Loss: 0.00000001, Cur Avg Loss: 0.00010123, Log Avg loss: 0.00014444, Global Avg Loss: 0.00134847, Time: 0.0129
Training, Epoch: 0031, Batch: 106450, Sample Num: 106450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010144, Log Avg loss: 0.00010690, Global Avg Loss: 0.00134753, Time: 0.0138
Training, Epoch: 0031, Batch: 110450, Sample Num: 110450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010088, Log Avg loss: 0.00008587, Global Avg Loss: 0.00134657, Time: 0.0150
Training, Epoch: 0031, Batch: 114450, Sample Num: 114450, Cur Loss: 0.00000001, Cur Avg Loss: 0.00010078, Log Avg loss: 0.00009817, Global Avg Loss: 0.00134562, Time: 0.0255
Training, Epoch: 0031, Batch: 118450, Sample Num: 118450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010050, Log Avg loss: 0.00009228, Global Avg Loss: 0.00134466, Time: 0.0279
Training, Epoch: 0031, Batch: 122450, Sample Num: 122450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010215, Log Avg loss: 0.00015106, Global Avg Loss: 0.00134376, Time: 0.0221
Training, Epoch: 0031, Batch: 126450, Sample Num: 126450, Cur Loss: 0.00000003, Cur Avg Loss: 0.00010055, Log Avg loss: 0.00005174, Global Avg Loss: 0.00134278, Time: 0.0210
Training, Epoch: 0031, Batch: 130450, Sample Num: 130450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010430, Log Avg loss: 0.00022274, Global Avg Loss: 0.00134193, Time: 0.0127
Training, Epoch: 0031, Batch: 134450, Sample Num: 134450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010419, Log Avg loss: 0.00010065, Global Avg Loss: 0.00134098, Time: 0.0340
Training, Epoch: 0031, Batch: 138450, Sample Num: 138450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010390, Log Avg loss: 0.00009423, Global Avg Loss: 0.00134004, Time: 0.0150
Training, Epoch: 0031, Batch: 142450, Sample Num: 142450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010763, Log Avg loss: 0.00023668, Global Avg Loss: 0.00133920, Time: 0.0183
Training, Epoch: 0031, Batch: 146450, Sample Num: 146450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010702, Log Avg loss: 0.00008509, Global Avg Loss: 0.00133826, Time: 0.0140
Training, Epoch: 0031, Batch: 150450, Sample Num: 150450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010921, Log Avg loss: 0.00018939, Global Avg Loss: 0.00133739, Time: 0.0123
Training, Epoch: 0031, Batch: 154450, Sample Num: 154450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010836, Log Avg loss: 0.00007657, Global Avg Loss: 0.00133644, Time: 0.0364
Training, Epoch: 0031, Batch: 158450, Sample Num: 158450, Cur Loss: 0.00001011, Cur Avg Loss: 0.00010941, Log Avg loss: 0.00014993, Global Avg Loss: 0.00133554, Time: 0.0123
Training, Epoch: 0031, Batch: 162450, Sample Num: 162450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010864, Log Avg loss: 0.00007808, Global Avg Loss: 0.00133459, Time: 0.0180
Training, Epoch: 0031, Batch: 166450, Sample Num: 166450, Cur Loss: 0.00000001, Cur Avg Loss: 0.00010796, Log Avg loss: 0.00008045, Global Avg Loss: 0.00133365, Time: 0.0310
Training, Epoch: 0031, Batch: 170450, Sample Num: 170450, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010740, Log Avg loss: 0.00008404, Global Avg Loss: 0.00133271, Time: 0.0126
***** Running evaluation checkpoint-5312935 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-5312935 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3598.572515, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.239833, "eval_total_loss": 5133.377331, "eval_acc": 0.962016, "eval_prec": 0.984535, "eval_recall": 0.971436, "eval_f1": 0.97598, "eval_top2_acc": 0.9943, "eval_top3_acc": 0.998692, "eval_top5_acc": 0.99986, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.99938, "eval_pr_auc": 0.987112, "eval_mcc": 0.957923, "eval_sn": 0.971436, "eval_sp": 0.998552, "update_flag": false, "test_avg_loss": 0.234591, "test_total_loss": 5021.191236, "test_acc": 0.962577, "test_prec": 0.985025, "test_recall": 0.977142, "test_f1": 0.98022, "test_top2_acc": 0.993272, "test_top3_acc": 0.998458, "test_top5_acc": 0.999766, "test_top10_acc": 1.0, "test_roc_auc": 0.999348, "test_pr_auc": 0.991812, "test_mcc": 0.958485, "test_sn": 0.977142, "test_sp": 0.998568, "lr": 7.661236020568996e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0013324925104483683, "train_cur_epoch_loss": 18.42039542538732, "train_cur_epoch_avg_loss": 0.00010747962438595745, "train_cur_epoch_time": 3598.572515487671, "train_cur_epoch_avg_time": 0.020997009746988773, "epoch": 31, "step": 5312935}
##################################################
Training, Epoch: 0032, Batch: 003065, Sample Num: 3065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008694, Log Avg loss: 0.00009515, Global Avg Loss: 0.00133177, Time: 0.0230
Training, Epoch: 0032, Batch: 007065, Sample Num: 7065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006509, Log Avg loss: 0.00004835, Global Avg Loss: 0.00133081, Time: 0.0172
Training, Epoch: 0032, Batch: 011065, Sample Num: 11065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007673, Log Avg loss: 0.00009728, Global Avg Loss: 0.00132988, Time: 0.0230
Training, Epoch: 0032, Batch: 015065, Sample Num: 15065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007242, Log Avg loss: 0.00006052, Global Avg Loss: 0.00132893, Time: 0.0243
Training, Epoch: 0032, Batch: 019065, Sample Num: 19065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007059, Log Avg loss: 0.00006369, Global Avg Loss: 0.00132798, Time: 0.0154
Training, Epoch: 0032, Batch: 023065, Sample Num: 23065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007426, Log Avg loss: 0.00009172, Global Avg Loss: 0.00132705, Time: 0.0612
Training, Epoch: 0032, Batch: 027065, Sample Num: 27065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008046, Log Avg loss: 0.00011620, Global Avg Loss: 0.00132615, Time: 0.0149
Training, Epoch: 0032, Batch: 031065, Sample Num: 31065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007803, Log Avg loss: 0.00006164, Global Avg Loss: 0.00132520, Time: 0.0266
Training, Epoch: 0032, Batch: 035065, Sample Num: 35065, Cur Loss: 0.00000249, Cur Avg Loss: 0.00007775, Log Avg loss: 0.00007554, Global Avg Loss: 0.00132427, Time: 0.0212
Training, Epoch: 0032, Batch: 039065, Sample Num: 39065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008483, Log Avg loss: 0.00014695, Global Avg Loss: 0.00132339, Time: 0.0129
Training, Epoch: 0032, Batch: 043065, Sample Num: 43065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008424, Log Avg loss: 0.00007846, Global Avg Loss: 0.00132246, Time: 0.0369
Training, Epoch: 0032, Batch: 047065, Sample Num: 47065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008146, Log Avg loss: 0.00005146, Global Avg Loss: 0.00132151, Time: 0.0254
Training, Epoch: 0032, Batch: 051065, Sample Num: 51065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008032, Log Avg loss: 0.00006694, Global Avg Loss: 0.00132057, Time: 0.0141
Training, Epoch: 0032, Batch: 055065, Sample Num: 55065, Cur Loss: 0.00000001, Cur Avg Loss: 0.00008251, Log Avg loss: 0.00011050, Global Avg Loss: 0.00131967, Time: 0.0124
Training, Epoch: 0032, Batch: 059065, Sample Num: 59065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008681, Log Avg loss: 0.00014596, Global Avg Loss: 0.00131880, Time: 0.0153
Training, Epoch: 0032, Batch: 063065, Sample Num: 63065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008916, Log Avg loss: 0.00012388, Global Avg Loss: 0.00131791, Time: 0.0223
Training, Epoch: 0032, Batch: 067065, Sample Num: 67065, Cur Loss: 0.00000006, Cur Avg Loss: 0.00008995, Log Avg loss: 0.00010240, Global Avg Loss: 0.00131700, Time: 0.0510
Training, Epoch: 0032, Batch: 071065, Sample Num: 71065, Cur Loss: 0.00000087, Cur Avg Loss: 0.00008982, Log Avg loss: 0.00008766, Global Avg Loss: 0.00131609, Time: 0.0125
Training, Epoch: 0032, Batch: 075065, Sample Num: 75065, Cur Loss: 0.00006925, Cur Avg Loss: 0.00009342, Log Avg loss: 0.00015740, Global Avg Loss: 0.00131523, Time: 0.0282
Training, Epoch: 0032, Batch: 079065, Sample Num: 79065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009279, Log Avg loss: 0.00008103, Global Avg Loss: 0.00131431, Time: 0.0444
Training, Epoch: 0032, Batch: 083065, Sample Num: 83065, Cur Loss: 0.00000001, Cur Avg Loss: 0.00009466, Log Avg loss: 0.00013150, Global Avg Loss: 0.00131344, Time: 0.0285
Training, Epoch: 0032, Batch: 087065, Sample Num: 87065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009572, Log Avg loss: 0.00011785, Global Avg Loss: 0.00131255, Time: 0.0137
Training, Epoch: 0032, Batch: 091065, Sample Num: 91065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009618, Log Avg loss: 0.00010620, Global Avg Loss: 0.00131166, Time: 0.0348
Training, Epoch: 0032, Batch: 095065, Sample Num: 95065, Cur Loss: 0.00000001, Cur Avg Loss: 0.00009494, Log Avg loss: 0.00006651, Global Avg Loss: 0.00131074, Time: 0.0123
Training, Epoch: 0032, Batch: 099065, Sample Num: 99065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009967, Log Avg loss: 0.00021212, Global Avg Loss: 0.00130993, Time: 0.0181
Training, Epoch: 0032, Batch: 103065, Sample Num: 103065, Cur Loss: 0.00000001, Cur Avg Loss: 0.00009947, Log Avg loss: 0.00009451, Global Avg Loss: 0.00130903, Time: 0.0226
Training, Epoch: 0032, Batch: 107065, Sample Num: 107065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009954, Log Avg loss: 0.00010138, Global Avg Loss: 0.00130814, Time: 0.0408
Training, Epoch: 0032, Batch: 111065, Sample Num: 111065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009823, Log Avg loss: 0.00006334, Global Avg Loss: 0.00130722, Time: 0.0243
Training, Epoch: 0032, Batch: 115065, Sample Num: 115065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009742, Log Avg loss: 0.00007487, Global Avg Loss: 0.00130631, Time: 0.0130
Training, Epoch: 0032, Batch: 119065, Sample Num: 119065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010194, Log Avg loss: 0.00023178, Global Avg Loss: 0.00130552, Time: 0.0416
Training, Epoch: 0032, Batch: 123065, Sample Num: 123065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010204, Log Avg loss: 0.00010502, Global Avg Loss: 0.00130464, Time: 0.0142
Training, Epoch: 0032, Batch: 127065, Sample Num: 127065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010485, Log Avg loss: 0.00019144, Global Avg Loss: 0.00130382, Time: 0.0272
Training, Epoch: 0032, Batch: 131065, Sample Num: 131065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010432, Log Avg loss: 0.00008743, Global Avg Loss: 0.00130292, Time: 0.0393
Training, Epoch: 0032, Batch: 135065, Sample Num: 135065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010349, Log Avg loss: 0.00007634, Global Avg Loss: 0.00130202, Time: 0.0134
Training, Epoch: 0032, Batch: 139065, Sample Num: 139065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010377, Log Avg loss: 0.00011304, Global Avg Loss: 0.00130115, Time: 0.0165
Training, Epoch: 0032, Batch: 143065, Sample Num: 143065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010311, Log Avg loss: 0.00008040, Global Avg Loss: 0.00130026, Time: 0.0191
Training, Epoch: 0032, Batch: 147065, Sample Num: 147065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010283, Log Avg loss: 0.00009267, Global Avg Loss: 0.00129937, Time: 0.0292
Training, Epoch: 0032, Batch: 151065, Sample Num: 151065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010524, Log Avg loss: 0.00019408, Global Avg Loss: 0.00129856, Time: 0.0149
Training, Epoch: 0032, Batch: 155065, Sample Num: 155065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010532, Log Avg loss: 0.00010814, Global Avg Loss: 0.00129769, Time: 0.0164
Training, Epoch: 0032, Batch: 159065, Sample Num: 159065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010683, Log Avg loss: 0.00016530, Global Avg Loss: 0.00129686, Time: 0.0281
Training, Epoch: 0032, Batch: 163065, Sample Num: 163065, Cur Loss: 0.00000014, Cur Avg Loss: 0.00010826, Log Avg loss: 0.00016512, Global Avg Loss: 0.00129604, Time: 0.0132
Training, Epoch: 0032, Batch: 167065, Sample Num: 167065, Cur Loss: 0.00000444, Cur Avg Loss: 0.00010689, Log Avg loss: 0.00005106, Global Avg Loss: 0.00129513, Time: 0.0383
Training, Epoch: 0032, Batch: 171065, Sample Num: 171065, Cur Loss: 0.00000000, Cur Avg Loss: 0.00010547, Log Avg loss: 0.00004642, Global Avg Loss: 0.00129422, Time: 0.0327
***** Running evaluation checkpoint-5484320 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-5484320 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 4345.807425, Avg time per batch (s): 0.030000
{"eval_avg_loss": 0.243388, "eval_total_loss": 5209.468395, "eval_acc": 0.963839, "eval_prec": 0.974569, "eval_recall": 0.968517, "eval_f1": 0.971375, "eval_top2_acc": 0.993786, "eval_top3_acc": 0.998645, "eval_top5_acc": 0.999813, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999362, "eval_pr_auc": 0.986347, "eval_mcc": 0.959882, "eval_sn": 0.968517, "eval_sp": 0.998607, "update_flag": false, "test_avg_loss": 0.2431, "test_total_loss": 5203.313319, "test_acc": 0.963839, "test_prec": 0.980903, "test_recall": 0.975164, "test_f1": 0.9778, "test_top2_acc": 0.993553, "test_top3_acc": 0.998271, "test_top5_acc": 0.999766, "test_top10_acc": 1.0, "test_roc_auc": 0.99932, "test_pr_auc": 0.992793, "test_mcc": 0.959915, "test_sn": 0.975164, "test_sp": 0.998598, "lr": 7.258357564974026e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0012941556747983847, "train_cur_epoch_loss": 18.11775441043075, "train_cur_epoch_avg_loss": 0.00010571376964396387, "train_cur_epoch_time": 4345.807425260544, "train_cur_epoch_avg_time": 0.0253569882151912, "epoch": 32, "step": 5484320}
##################################################
Training, Epoch: 0033, Batch: 003680, Sample Num: 3680, Cur Loss: 0.00000046, Cur Avg Loss: 0.00008699, Log Avg loss: 0.00009873, Global Avg Loss: 0.00129335, Time: 0.0155
Steps: 5488000, Updated lr: 0.000072
Training, Epoch: 0033, Batch: 007680, Sample Num: 7680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007338, Log Avg loss: 0.00006086, Global Avg Loss: 0.00129245, Time: 0.0204
Steps: 5492000, Updated lr: 0.000072
Training, Epoch: 0033, Batch: 011680, Sample Num: 11680, Cur Loss: 0.00003606, Cur Avg Loss: 0.00006169, Log Avg loss: 0.00003924, Global Avg Loss: 0.00129154, Time: 0.0125
Steps: 5496000, Updated lr: 0.000072
Training, Epoch: 0033, Batch: 015680, Sample Num: 15680, Cur Loss: 0.00000003, Cur Avg Loss: 0.00005464, Log Avg loss: 0.00003405, Global Avg Loss: 0.00129062, Time: 0.0123
Steps: 5500000, Updated lr: 0.000072
Training, Epoch: 0033, Batch: 019680, Sample Num: 19680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004794, Log Avg loss: 0.00002169, Global Avg Loss: 0.00128970, Time: 0.0193
Steps: 5504000, Updated lr: 0.000072
Training, Epoch: 0033, Batch: 023680, Sample Num: 23680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006068, Log Avg loss: 0.00012334, Global Avg Loss: 0.00128885, Time: 0.0163
Steps: 5508000, Updated lr: 0.000072
Training, Epoch: 0033, Batch: 027680, Sample Num: 27680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006313, Log Avg loss: 0.00007768, Global Avg Loss: 0.00128797, Time: 0.0134
Steps: 5512000, Updated lr: 0.000072
Training, Epoch: 0033, Batch: 031680, Sample Num: 31680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006296, Log Avg loss: 0.00006175, Global Avg Loss: 0.00128708, Time: 0.0313
Steps: 5516000, Updated lr: 0.000072
Training, Epoch: 0033, Batch: 035680, Sample Num: 35680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005854, Log Avg loss: 0.00002353, Global Avg Loss: 0.00128617, Time: 0.0141
Steps: 5520000, Updated lr: 0.000072
Training, Epoch: 0033, Batch: 039680, Sample Num: 39680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006530, Log Avg loss: 0.00012561, Global Avg Loss: 0.00128533, Time: 0.0157
Steps: 5524000, Updated lr: 0.000072
Training, Epoch: 0033, Batch: 043680, Sample Num: 43680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006355, Log Avg loss: 0.00004624, Global Avg Loss: 0.00128443, Time: 0.0279
Steps: 5528000, Updated lr: 0.000072
Training, Epoch: 0033, Batch: 047680, Sample Num: 47680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006466, Log Avg loss: 0.00007676, Global Avg Loss: 0.00128356, Time: 0.0126
Steps: 5532000, Updated lr: 0.000071
Training, Epoch: 0033, Batch: 051680, Sample Num: 51680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006893, Log Avg loss: 0.00011976, Global Avg Loss: 0.00128272, Time: 0.0164
Steps: 5536000, Updated lr: 0.000071
Training, Epoch: 0033, Batch: 055680, Sample Num: 55680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006952, Log Avg loss: 0.00007720, Global Avg Loss: 0.00128185, Time: 0.0124
Steps: 5540000, Updated lr: 0.000071
Training, Epoch: 0033, Batch: 059680, Sample Num: 59680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007413, Log Avg loss: 0.00013824, Global Avg Loss: 0.00128102, Time: 0.0125
Steps: 5544000, Updated lr: 0.000071
Training, Epoch: 0033, Batch: 063680, Sample Num: 63680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007341, Log Avg loss: 0.00006264, Global Avg Loss: 0.00128014, Time: 0.0124
Steps: 5548000, Updated lr: 0.000071
Training, Epoch: 0033, Batch: 067680, Sample Num: 67680, Cur Loss: 0.00000010, Cur Avg Loss: 0.00007586, Log Avg loss: 0.00011488, Global Avg Loss: 0.00127930, Time: 0.0132
Steps: 5552000, Updated lr: 0.000071
Training, Epoch: 0033, Batch: 071680, Sample Num: 71680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007793, Log Avg loss: 0.00011294, Global Avg Loss: 0.00127846, Time: 0.0182
Steps: 5556000, Updated lr: 0.000071
Training, Epoch: 0033, Batch: 075680, Sample Num: 75680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008112, Log Avg loss: 0.00013830, Global Avg Loss: 0.00127764, Time: 0.0128
Steps: 5560000, Updated lr: 0.000071
Training, Epoch: 0033, Batch: 079680, Sample Num: 79680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008285, Log Avg loss: 0.00011570, Global Avg Loss: 0.00127681, Time: 0.0196
Steps: 5564000, Updated lr: 0.000071
Training, Epoch: 0033, Batch: 083680, Sample Num: 83680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008327, Log Avg loss: 0.00009149, Global Avg Loss: 0.00127596, Time: 0.0187
Steps: 5568000, Updated lr: 0.000071
Training, Epoch: 0033, Batch: 087680, Sample Num: 87680, Cur Loss: 0.00000001, Cur Avg Loss: 0.00008413, Log Avg loss: 0.00010230, Global Avg Loss: 0.00127511, Time: 0.0174
Steps: 5572000, Updated lr: 0.000071
Training, Epoch: 0033, Batch: 091680, Sample Num: 91680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008444, Log Avg loss: 0.00009106, Global Avg Loss: 0.00127427, Time: 0.0402
Steps: 5576000, Updated lr: 0.000070
Training, Epoch: 0033, Batch: 095680, Sample Num: 95680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008649, Log Avg loss: 0.00013351, Global Avg Loss: 0.00127345, Time: 0.0126
Steps: 5580000, Updated lr: 0.000070
Training, Epoch: 0033, Batch: 099680, Sample Num: 99680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008536, Log Avg loss: 0.00005834, Global Avg Loss: 0.00127258, Time: 0.0149
Steps: 5584000, Updated lr: 0.000070
Training, Epoch: 0033, Batch: 103680, Sample Num: 103680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008892, Log Avg loss: 0.00017758, Global Avg Loss: 0.00127179, Time: 0.0372
Steps: 5588000, Updated lr: 0.000070
Training, Epoch: 0033, Batch: 107680, Sample Num: 107680, Cur Loss: 0.00032367, Cur Avg Loss: 0.00008790, Log Avg loss: 0.00006150, Global Avg Loss: 0.00127093, Time: 0.0592
Steps: 5592000, Updated lr: 0.000070
Training, Epoch: 0033, Batch: 111680, Sample Num: 111680, Cur Loss: 0.00000001, Cur Avg Loss: 0.00009032, Log Avg loss: 0.00015561, Global Avg Loss: 0.00127013, Time: 0.0390
Steps: 5596000, Updated lr: 0.000070
Training, Epoch: 0033, Batch: 115680, Sample Num: 115680, Cur Loss: 0.00000552, Cur Avg Loss: 0.00009254, Log Avg loss: 0.00015444, Global Avg Loss: 0.00126933, Time: 0.0273
Steps: 5600000, Updated lr: 0.000070
Training, Epoch: 0033, Batch: 119680, Sample Num: 119680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009539, Log Avg loss: 0.00017770, Global Avg Loss: 0.00126855, Time: 0.0122
Steps: 5604000, Updated lr: 0.000070
Training, Epoch: 0033, Batch: 123680, Sample Num: 123680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009375, Log Avg loss: 0.00004488, Global Avg Loss: 0.00126768, Time: 0.0154
Steps: 5608000, Updated lr: 0.000070
Training, Epoch: 0033, Batch: 127680, Sample Num: 127680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009200, Log Avg loss: 0.00003797, Global Avg Loss: 0.00126681, Time: 0.0142
Steps: 5612000, Updated lr: 0.000070
Training, Epoch: 0033, Batch: 131680, Sample Num: 131680, Cur Loss: 0.00000835, Cur Avg Loss: 0.00009121, Log Avg loss: 0.00006596, Global Avg Loss: 0.00126595, Time: 0.0130
Steps: 5616000, Updated lr: 0.000069
Training, Epoch: 0033, Batch: 135680, Sample Num: 135680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008956, Log Avg loss: 0.00003521, Global Avg Loss: 0.00126507, Time: 0.0417
Steps: 5620000, Updated lr: 0.000069
Training, Epoch: 0033, Batch: 139680, Sample Num: 139680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009221, Log Avg loss: 0.00018199, Global Avg Loss: 0.00126430, Time: 0.0126
Steps: 5624000, Updated lr: 0.000069
Training, Epoch: 0033, Batch: 143680, Sample Num: 143680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009171, Log Avg loss: 0.00007411, Global Avg Loss: 0.00126346, Time: 0.0743
Steps: 5628000, Updated lr: 0.000069
Training, Epoch: 0033, Batch: 147680, Sample Num: 147680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009040, Log Avg loss: 0.00004350, Global Avg Loss: 0.00126259, Time: 0.0470
Steps: 5632000, Updated lr: 0.000069
Training, Epoch: 0033, Batch: 151680, Sample Num: 151680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009099, Log Avg loss: 0.00011274, Global Avg Loss: 0.00126178, Time: 0.0432
Steps: 5636000, Updated lr: 0.000069
Training, Epoch: 0033, Batch: 155680, Sample Num: 155680, Cur Loss: 0.00230262, Cur Avg Loss: 0.00008988, Log Avg loss: 0.00004790, Global Avg Loss: 0.00126091, Time: 0.0203
Steps: 5640000, Updated lr: 0.000069
Training, Epoch: 0033, Batch: 159680, Sample Num: 159680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008981, Log Avg loss: 0.00008692, Global Avg Loss: 0.00126008, Time: 0.0342
Steps: 5644000, Updated lr: 0.000069
Training, Epoch: 0033, Batch: 163680, Sample Num: 163680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008940, Log Avg loss: 0.00007295, Global Avg Loss: 0.00125924, Time: 0.0144
Steps: 5648000, Updated lr: 0.000069
Training, Epoch: 0033, Batch: 167680, Sample Num: 167680, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008964, Log Avg loss: 0.00009966, Global Avg Loss: 0.00125842, Time: 0.0301
Steps: 5652000, Updated lr: 0.000069
***** Running evaluation checkpoint-5655705 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-5655705 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3736.481452, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.246881, "eval_total_loss": 5284.249847, "eval_acc": 0.962016, "eval_prec": 0.96954, "eval_recall": 0.969169, "eval_f1": 0.969261, "eval_top2_acc": 0.993132, "eval_top3_acc": 0.998645, "eval_top5_acc": 0.999813, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999341, "eval_pr_auc": 0.987044, "eval_mcc": 0.957869, "eval_sn": 0.969169, "eval_sp": 0.998557, "update_flag": false, "test_avg_loss": 0.242336, "test_total_loss": 5186.960686, "test_acc": 0.963698, "test_prec": 0.985953, "test_recall": 0.982388, "test_f1": 0.983892, "test_top2_acc": 0.992758, "test_top3_acc": 0.998552, "test_top5_acc": 0.99972, "test_top10_acc": 1.0, "test_roc_auc": 0.999315, "test_pr_auc": 0.993502, "test_mcc": 0.959706, "test_sn": 0.982388, "test_sp": 0.998612, "lr": 6.855479109379055e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0012576920157651482, "train_cur_epoch_loss": 15.571171611975739, "train_cur_epoch_avg_loss": 9.085492669706065e-05, "train_cur_epoch_time": 3736.481451511383, "train_cur_epoch_avg_time": 0.021801683061594558, "epoch": 33, "step": 5655705}
##################################################
Training, Epoch: 0034, Batch: 000295, Sample Num: 295, Cur Loss: 0.00000008, Cur Avg Loss: 0.00000905, Log Avg loss: 0.00013573, Global Avg Loss: 0.00125763, Time: 0.0121
Training, Epoch: 0034, Batch: 004295, Sample Num: 4295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00009671, Log Avg loss: 0.00010318, Global Avg Loss: 0.00125681, Time: 0.0369
Training, Epoch: 0034, Batch: 008295, Sample Num: 8295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007199, Log Avg loss: 0.00004545, Global Avg Loss: 0.00125596, Time: 0.0124
Training, Epoch: 0034, Batch: 012295, Sample Num: 12295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005645, Log Avg loss: 0.00002421, Global Avg Loss: 0.00125509, Time: 0.0163
Training, Epoch: 0034, Batch: 016295, Sample Num: 16295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006753, Log Avg loss: 0.00010161, Global Avg Loss: 0.00125427, Time: 0.0194
Training, Epoch: 0034, Batch: 020295, Sample Num: 20295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00008135, Log Avg loss: 0.00013762, Global Avg Loss: 0.00125349, Time: 0.0165
Training, Epoch: 0034, Batch: 024295, Sample Num: 24295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007098, Log Avg loss: 0.00001840, Global Avg Loss: 0.00125262, Time: 0.0252
Training, Epoch: 0034, Batch: 028295, Sample Num: 28295, Cur Loss: 0.00000016, Cur Avg Loss: 0.00006675, Log Avg loss: 0.00004103, Global Avg Loss: 0.00125176, Time: 0.0174
Training, Epoch: 0034, Batch: 032295, Sample Num: 32295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006018, Log Avg loss: 0.00001372, Global Avg Loss: 0.00125089, Time: 0.0138
Training, Epoch: 0034, Batch: 036295, Sample Num: 36295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006161, Log Avg loss: 0.00007313, Global Avg Loss: 0.00125007, Time: 0.0317
Training, Epoch: 0034, Batch: 040295, Sample Num: 40295, Cur Loss: 0.00000001, Cur Avg Loss: 0.00006295, Log Avg loss: 0.00007515, Global Avg Loss: 0.00124924, Time: 0.0142
Training, Epoch: 0034, Batch: 044295, Sample Num: 44295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006365, Log Avg loss: 0.00007069, Global Avg Loss: 0.00124841, Time: 0.0307
Training, Epoch: 0034, Batch: 048295, Sample Num: 48295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006551, Log Avg loss: 0.00008616, Global Avg Loss: 0.00124760, Time: 0.0146
Training, Epoch: 0034, Batch: 052295, Sample Num: 52295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006484, Log Avg loss: 0.00005670, Global Avg Loss: 0.00124676, Time: 0.0155
Training, Epoch: 0034, Batch: 056295, Sample Num: 56295, Cur Loss: 0.00000013, Cur Avg Loss: 0.00006791, Log Avg loss: 0.00010803, Global Avg Loss: 0.00124597, Time: 0.0289
Training, Epoch: 0034, Batch: 060295, Sample Num: 60295, Cur Loss: 0.00000001, Cur Avg Loss: 0.00007361, Log Avg loss: 0.00015379, Global Avg Loss: 0.00124520, Time: 0.0250
Training, Epoch: 0034, Batch: 064295, Sample Num: 64295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007138, Log Avg loss: 0.00003789, Global Avg Loss: 0.00124436, Time: 0.0152
Training, Epoch: 0034, Batch: 068295, Sample Num: 68295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007106, Log Avg loss: 0.00006579, Global Avg Loss: 0.00124353, Time: 0.0123
Training, Epoch: 0034, Batch: 072295, Sample Num: 72295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006942, Log Avg loss: 0.00004153, Global Avg Loss: 0.00124269, Time: 0.0125
Training, Epoch: 0034, Batch: 076295, Sample Num: 76295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007002, Log Avg loss: 0.00008078, Global Avg Loss: 0.00124188, Time: 0.0130
Training, Epoch: 0034, Batch: 080295, Sample Num: 80295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006958, Log Avg loss: 0.00006125, Global Avg Loss: 0.00124106, Time: 0.0223
Training, Epoch: 0034, Batch: 084295, Sample Num: 84295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007065, Log Avg loss: 0.00009206, Global Avg Loss: 0.00124026, Time: 0.0450
Training, Epoch: 0034, Batch: 088295, Sample Num: 88295, Cur Loss: 0.00000012, Cur Avg Loss: 0.00007758, Log Avg loss: 0.00022356, Global Avg Loss: 0.00123955, Time: 0.0513
Training, Epoch: 0034, Batch: 092295, Sample Num: 92295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007656, Log Avg loss: 0.00005402, Global Avg Loss: 0.00123873, Time: 0.0144
Training, Epoch: 0034, Batch: 096295, Sample Num: 96295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007548, Log Avg loss: 0.00005076, Global Avg Loss: 0.00123790, Time: 0.0157
Training, Epoch: 0034, Batch: 100295, Sample Num: 100295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007551, Log Avg loss: 0.00007623, Global Avg Loss: 0.00123709, Time: 0.0358
Training, Epoch: 0034, Batch: 104295, Sample Num: 104295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007953, Log Avg loss: 0.00018020, Global Avg Loss: 0.00123636, Time: 0.0151
Training, Epoch: 0034, Batch: 108295, Sample Num: 108295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007783, Log Avg loss: 0.00003358, Global Avg Loss: 0.00123552, Time: 0.0251
Training, Epoch: 0034, Batch: 112295, Sample Num: 112295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007750, Log Avg loss: 0.00006857, Global Avg Loss: 0.00123472, Time: 0.0208
Training, Epoch: 0034, Batch: 116295, Sample Num: 116295, Cur Loss: 0.00000279, Cur Avg Loss: 0.00007869, Log Avg loss: 0.00011216, Global Avg Loss: 0.00123394, Time: 0.0171
Training, Epoch: 0034, Batch: 120295, Sample Num: 120295, Cur Loss: 0.00000001, Cur Avg Loss: 0.00007851, Log Avg loss: 0.00007330, Global Avg Loss: 0.00123313, Time: 0.0192
Training, Epoch: 0034, Batch: 124295, Sample Num: 124295, Cur Loss: 0.00000006, Cur Avg Loss: 0.00007674, Log Avg loss: 0.00002354, Global Avg Loss: 0.00123230, Time: 0.0127
Training, Epoch: 0034, Batch: 128295, Sample Num: 128295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007612, Log Avg loss: 0.00005665, Global Avg Loss: 0.00123148, Time: 0.0364
Training, Epoch: 0034, Batch: 132295, Sample Num: 132295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007571, Log Avg loss: 0.00006274, Global Avg Loss: 0.00123068, Time: 0.0173
Training, Epoch: 0034, Batch: 136295, Sample Num: 136295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007442, Log Avg loss: 0.00003165, Global Avg Loss: 0.00122985, Time: 0.0177
Training, Epoch: 0034, Batch: 140295, Sample Num: 140295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007670, Log Avg loss: 0.00015441, Global Avg Loss: 0.00122911, Time: 0.0128
Training, Epoch: 0034, Batch: 144295, Sample Num: 144295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007642, Log Avg loss: 0.00006664, Global Avg Loss: 0.00122830, Time: 0.0123
Training, Epoch: 0034, Batch: 148295, Sample Num: 148295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007921, Log Avg loss: 0.00017977, Global Avg Loss: 0.00122758, Time: 0.0164
Training, Epoch: 0034, Batch: 152295, Sample Num: 152295, Cur Loss: 0.00000012, Cur Avg Loss: 0.00007760, Log Avg loss: 0.00001778, Global Avg Loss: 0.00122675, Time: 0.0121
Training, Epoch: 0034, Batch: 156295, Sample Num: 156295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007914, Log Avg loss: 0.00013782, Global Avg Loss: 0.00122600, Time: 0.0365
Training, Epoch: 0034, Batch: 160295, Sample Num: 160295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007845, Log Avg loss: 0.00005162, Global Avg Loss: 0.00122519, Time: 0.0183
Training, Epoch: 0034, Batch: 164295, Sample Num: 164295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007729, Log Avg loss: 0.00003088, Global Avg Loss: 0.00122437, Time: 0.0249
Training, Epoch: 0034, Batch: 168295, Sample Num: 168295, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007684, Log Avg loss: 0.00005823, Global Avg Loss: 0.00122357, Time: 0.0182
***** Running evaluation checkpoint-5827090 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-5827090 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3636.641062, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.24538, "eval_total_loss": 5252.108431, "eval_acc": 0.96482, "eval_prec": 0.978038, "eval_recall": 0.975161, "eval_f1": 0.976569, "eval_top2_acc": 0.993786, "eval_top3_acc": 0.998458, "eval_top5_acc": 0.999813, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999362, "eval_pr_auc": 0.987593, "eval_mcc": 0.960936, "eval_sn": 0.975161, "eval_sp": 0.998647, "update_flag": false, "test_avg_loss": 0.245235, "test_total_loss": 5249.011552, "test_acc": 0.964493, "test_prec": 0.986351, "test_recall": 0.97702, "test_f1": 0.980817, "test_top2_acc": 0.993599, "test_top3_acc": 0.998225, "test_top5_acc": 0.99986, "test_top10_acc": 1.0, "test_roc_auc": 0.999316, "test_pr_auc": 0.99295, "test_mcc": 0.960596, "test_sn": 0.97702, "test_sp": 0.998629, "lr": 6.452600653784085e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0012230106943925873, "train_cur_epoch_loss": 13.45836516429766, "train_cur_epoch_avg_loss": 7.852708909354763e-05, "train_cur_epoch_time": 3636.6410615444183, "train_cur_epoch_avg_time": 0.02121913272190926, "epoch": 34, "step": 5827090}
##################################################
Training, Epoch: 0035, Batch: 000910, Sample Num: 910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000711, Log Avg loss: 0.00013328, Global Avg Loss: 0.00122282, Time: 0.0424
Training, Epoch: 0035, Batch: 004910, Sample Num: 4910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005998, Log Avg loss: 0.00007201, Global Avg Loss: 0.00122203, Time: 0.0203
Training, Epoch: 0035, Batch: 008910, Sample Num: 8910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006148, Log Avg loss: 0.00006333, Global Avg Loss: 0.00122124, Time: 0.0151
Training, Epoch: 0035, Batch: 012910, Sample Num: 12910, Cur Loss: 0.00000024, Cur Avg Loss: 0.00004523, Log Avg loss: 0.00000902, Global Avg Loss: 0.00122041, Time: 0.0139
Training, Epoch: 0035, Batch: 016910, Sample Num: 16910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004403, Log Avg loss: 0.00004015, Global Avg Loss: 0.00121960, Time: 0.0147
Training, Epoch: 0035, Batch: 020910, Sample Num: 20910, Cur Loss: 0.00004449, Cur Avg Loss: 0.00005738, Log Avg loss: 0.00011382, Global Avg Loss: 0.00121884, Time: 0.0153
Training, Epoch: 0035, Batch: 024910, Sample Num: 24910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005508, Log Avg loss: 0.00004305, Global Avg Loss: 0.00121804, Time: 0.0373
Training, Epoch: 0035, Batch: 028910, Sample Num: 28910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005260, Log Avg loss: 0.00003720, Global Avg Loss: 0.00121723, Time: 0.0142
Training, Epoch: 0035, Batch: 032910, Sample Num: 32910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005120, Log Avg loss: 0.00004102, Global Avg Loss: 0.00121643, Time: 0.0122
Training, Epoch: 0035, Batch: 036910, Sample Num: 36910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004903, Log Avg loss: 0.00003121, Global Avg Loss: 0.00121562, Time: 0.0356
Training, Epoch: 0035, Batch: 040910, Sample Num: 40910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005032, Log Avg loss: 0.00006222, Global Avg Loss: 0.00121484, Time: 0.0142
Training, Epoch: 0035, Batch: 044910, Sample Num: 44910, Cur Loss: 0.00000798, Cur Avg Loss: 0.00004992, Log Avg loss: 0.00004580, Global Avg Loss: 0.00121404, Time: 0.0148
Training, Epoch: 0035, Batch: 048910, Sample Num: 48910, Cur Loss: 0.00000153, Cur Avg Loss: 0.00005538, Log Avg loss: 0.00011672, Global Avg Loss: 0.00121329, Time: 0.0128
Training, Epoch: 0035, Batch: 052910, Sample Num: 52910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005470, Log Avg loss: 0.00004631, Global Avg Loss: 0.00121250, Time: 0.0355
Training, Epoch: 0035, Batch: 056910, Sample Num: 56910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005419, Log Avg loss: 0.00004748, Global Avg Loss: 0.00121171, Time: 0.0447
Training, Epoch: 0035, Batch: 060910, Sample Num: 60910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005477, Log Avg loss: 0.00006300, Global Avg Loss: 0.00121093, Time: 0.0157
Training, Epoch: 0035, Batch: 064910, Sample Num: 64910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005429, Log Avg loss: 0.00004697, Global Avg Loss: 0.00121014, Time: 0.0130
Training, Epoch: 0035, Batch: 068910, Sample Num: 68910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005530, Log Avg loss: 0.00007182, Global Avg Loss: 0.00120936, Time: 0.0143
Training, Epoch: 0035, Batch: 072910, Sample Num: 72910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005956, Log Avg loss: 0.00013279, Global Avg Loss: 0.00120863, Time: 0.0142
Training, Epoch: 0035, Batch: 076910, Sample Num: 76910, Cur Loss: 0.00000001, Cur Avg Loss: 0.00005715, Log Avg loss: 0.00001325, Global Avg Loss: 0.00120782, Time: 0.0147
Training, Epoch: 0035, Batch: 080910, Sample Num: 80910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005755, Log Avg loss: 0.00006530, Global Avg Loss: 0.00120705, Time: 0.0296
Training, Epoch: 0035, Batch: 084910, Sample Num: 84910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005566, Log Avg loss: 0.00001750, Global Avg Loss: 0.00120624, Time: 0.0122
Training, Epoch: 0035, Batch: 088910, Sample Num: 88910, Cur Loss: 0.00000002, Cur Avg Loss: 0.00005606, Log Avg loss: 0.00006454, Global Avg Loss: 0.00120547, Time: 0.0138
Training, Epoch: 0035, Batch: 092910, Sample Num: 92910, Cur Loss: 0.00000014, Cur Avg Loss: 0.00005866, Log Avg loss: 0.00011636, Global Avg Loss: 0.00120474, Time: 0.0121
Training, Epoch: 0035, Batch: 096910, Sample Num: 96910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005730, Log Avg loss: 0.00002573, Global Avg Loss: 0.00120394, Time: 0.0188
Training, Epoch: 0035, Batch: 100910, Sample Num: 100910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005592, Log Avg loss: 0.00002248, Global Avg Loss: 0.00120314, Time: 0.0347
Training, Epoch: 0035, Batch: 104910, Sample Num: 104910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005746, Log Avg loss: 0.00009638, Global Avg Loss: 0.00120240, Time: 0.0319
Training, Epoch: 0035, Batch: 108910, Sample Num: 108910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005581, Log Avg loss: 0.00001236, Global Avg Loss: 0.00120160, Time: 0.0144
Training, Epoch: 0035, Batch: 112910, Sample Num: 112910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005686, Log Avg loss: 0.00008571, Global Avg Loss: 0.00120084, Time: 0.0125
Training, Epoch: 0035, Batch: 116910, Sample Num: 116910, Cur Loss: 0.00000003, Cur Avg Loss: 0.00005810, Log Avg loss: 0.00009295, Global Avg Loss: 0.00120010, Time: 0.0161
Training, Epoch: 0035, Batch: 120910, Sample Num: 120910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005745, Log Avg loss: 0.00003836, Global Avg Loss: 0.00119932, Time: 0.0198
Training, Epoch: 0035, Batch: 124910, Sample Num: 124910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005725, Log Avg loss: 0.00005128, Global Avg Loss: 0.00119855, Time: 0.0132
Training, Epoch: 0035, Batch: 128910, Sample Num: 128910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005826, Log Avg loss: 0.00008973, Global Avg Loss: 0.00119780, Time: 0.0190
Training, Epoch: 0035, Batch: 132910, Sample Num: 132910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006064, Log Avg loss: 0.00013734, Global Avg Loss: 0.00119709, Time: 0.0246
Training, Epoch: 0035, Batch: 136910, Sample Num: 136910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006003, Log Avg loss: 0.00003988, Global Avg Loss: 0.00119631, Time: 0.0123
Training, Epoch: 0035, Batch: 140910, Sample Num: 140910, Cur Loss: 0.00000007, Cur Avg Loss: 0.00006015, Log Avg loss: 0.00006417, Global Avg Loss: 0.00119555, Time: 0.0146
Training, Epoch: 0035, Batch: 144910, Sample Num: 144910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006210, Log Avg loss: 0.00013077, Global Avg Loss: 0.00119484, Time: 0.0372
Training, Epoch: 0035, Batch: 148910, Sample Num: 148910, Cur Loss: 0.00011240, Cur Avg Loss: 0.00006160, Log Avg loss: 0.00004343, Global Avg Loss: 0.00119407, Time: 0.0176
Training, Epoch: 0035, Batch: 152910, Sample Num: 152910, Cur Loss: 0.00000007, Cur Avg Loss: 0.00006158, Log Avg loss: 0.00006082, Global Avg Loss: 0.00119331, Time: 0.0122
Training, Epoch: 0035, Batch: 156910, Sample Num: 156910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006454, Log Avg loss: 0.00017783, Global Avg Loss: 0.00119263, Time: 0.0125
Training, Epoch: 0035, Batch: 160910, Sample Num: 160910, Cur Loss: 0.00000759, Cur Avg Loss: 0.00006650, Log Avg loss: 0.00014347, Global Avg Loss: 0.00119193, Time: 0.0173
Training, Epoch: 0035, Batch: 164910, Sample Num: 164910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006756, Log Avg loss: 0.00011020, Global Avg Loss: 0.00119121, Time: 0.0229
Training, Epoch: 0035, Batch: 168910, Sample Num: 168910, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007141, Log Avg loss: 0.00023018, Global Avg Loss: 0.00119057, Time: 0.0149
***** Running evaluation checkpoint-5998475 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-5998475 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3409.735780, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.245322, "eval_total_loss": 5250.874279, "eval_acc": 0.964352, "eval_prec": 0.980057, "eval_recall": 0.969377, "eval_f1": 0.973997, "eval_top2_acc": 0.99416, "eval_top3_acc": 0.998458, "eval_top5_acc": 0.99986, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999371, "eval_pr_auc": 0.987459, "eval_mcc": 0.960407, "eval_sn": 0.969377, "eval_sp": 0.998625, "update_flag": false, "test_avg_loss": 0.247208, "test_total_loss": 5291.242249, "test_acc": 0.964259, "test_prec": 0.98683, "test_recall": 0.97606, "test_f1": 0.980527, "test_top2_acc": 0.99402, "test_top3_acc": 0.998412, "test_top5_acc": 0.99986, "test_top10_acc": 1.0, "test_roc_auc": 0.999309, "test_pr_auc": 0.992832, "test_mcc": 0.960317, "test_sn": 0.97606, "test_sp": 0.998617, "lr": 6.049722198189116e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0011901293507578443, "train_cur_epoch_loss": 12.367770098274224, "train_cur_epoch_avg_loss": 7.216366717200587e-05, "train_cur_epoch_time": 3409.735779762268, "train_cur_epoch_avg_time": 0.019895182074057053, "epoch": 35, "step": 5998475}
##################################################
Training, Epoch: 0036, Batch: 001525, Sample Num: 1525, Cur Loss: 0.00000001, Cur Avg Loss: 0.00001363, Log Avg loss: 0.00008157, Global Avg Loss: 0.00118983, Time: 0.0142
Training, Epoch: 0036, Batch: 005525, Sample Num: 5525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002493, Log Avg loss: 0.00002924, Global Avg Loss: 0.00118906, Time: 0.0169
Training, Epoch: 0036, Batch: 009525, Sample Num: 9525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001665, Log Avg loss: 0.00000521, Global Avg Loss: 0.00118827, Time: 0.0123
Training, Epoch: 0036, Batch: 013525, Sample Num: 13525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001416, Log Avg loss: 0.00000822, Global Avg Loss: 0.00118748, Time: 0.0145
Training, Epoch: 0036, Batch: 017525, Sample Num: 17525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001372, Log Avg loss: 0.00001226, Global Avg Loss: 0.00118670, Time: 0.0196
Training, Epoch: 0036, Batch: 021525, Sample Num: 21525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001358, Log Avg loss: 0.00001297, Global Avg Loss: 0.00118592, Time: 0.0154
Training, Epoch: 0036, Batch: 025525, Sample Num: 25525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001926, Log Avg loss: 0.00004984, Global Avg Loss: 0.00118517, Time: 0.0141
Training, Epoch: 0036, Batch: 029525, Sample Num: 29525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002538, Log Avg loss: 0.00006439, Global Avg Loss: 0.00118442, Time: 0.0147
Training, Epoch: 0036, Batch: 033525, Sample Num: 33525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002633, Log Avg loss: 0.00003339, Global Avg Loss: 0.00118366, Time: 0.0123
Training, Epoch: 0036, Batch: 037525, Sample Num: 37525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003048, Log Avg loss: 0.00006520, Global Avg Loss: 0.00118292, Time: 0.0200
Training, Epoch: 0036, Batch: 041525, Sample Num: 41525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002968, Log Avg loss: 0.00002221, Global Avg Loss: 0.00118215, Time: 0.0154
Training, Epoch: 0036, Batch: 045525, Sample Num: 45525, Cur Loss: 0.00000003, Cur Avg Loss: 0.00002862, Log Avg loss: 0.00001761, Global Avg Loss: 0.00118138, Time: 0.0125
Training, Epoch: 0036, Batch: 049525, Sample Num: 49525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003562, Log Avg loss: 0.00011524, Global Avg Loss: 0.00118068, Time: 0.0458
Training, Epoch: 0036, Batch: 053525, Sample Num: 53525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003567, Log Avg loss: 0.00003632, Global Avg Loss: 0.00117992, Time: 0.0178
Training, Epoch: 0036, Batch: 057525, Sample Num: 57525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003578, Log Avg loss: 0.00003728, Global Avg Loss: 0.00117916, Time: 0.0126
Training, Epoch: 0036, Batch: 061525, Sample Num: 61525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003624, Log Avg loss: 0.00004285, Global Avg Loss: 0.00117841, Time: 0.0129
Training, Epoch: 0036, Batch: 065525, Sample Num: 65525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003873, Log Avg loss: 0.00007701, Global Avg Loss: 0.00117769, Time: 0.0544
Training, Epoch: 0036, Batch: 069525, Sample Num: 69525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004071, Log Avg loss: 0.00007308, Global Avg Loss: 0.00117696, Time: 0.0340
Training, Epoch: 0036, Batch: 073525, Sample Num: 73525, Cur Loss: 0.00000285, Cur Avg Loss: 0.00004176, Log Avg loss: 0.00006006, Global Avg Loss: 0.00117622, Time: 0.0178
Training, Epoch: 0036, Batch: 077525, Sample Num: 77525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004383, Log Avg loss: 0.00008194, Global Avg Loss: 0.00117550, Time: 0.0126
Training, Epoch: 0036, Batch: 081525, Sample Num: 81525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004220, Log Avg loss: 0.00001061, Global Avg Loss: 0.00117474, Time: 0.0383
Training, Epoch: 0036, Batch: 085525, Sample Num: 85525, Cur Loss: 0.00000003, Cur Avg Loss: 0.00004228, Log Avg loss: 0.00004394, Global Avg Loss: 0.00117399, Time: 0.0152
Training, Epoch: 0036, Batch: 089525, Sample Num: 89525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004564, Log Avg loss: 0.00011743, Global Avg Loss: 0.00117330, Time: 0.0248
Training, Epoch: 0036, Batch: 093525, Sample Num: 93525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004574, Log Avg loss: 0.00004803, Global Avg Loss: 0.00117256, Time: 0.0142
Training, Epoch: 0036, Batch: 097525, Sample Num: 97525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004544, Log Avg loss: 0.00003830, Global Avg Loss: 0.00117182, Time: 0.0763
Training, Epoch: 0036, Batch: 101525, Sample Num: 101525, Cur Loss: 0.00000001, Cur Avg Loss: 0.00004473, Log Avg loss: 0.00002751, Global Avg Loss: 0.00117107, Time: 0.0300
Training, Epoch: 0036, Batch: 105525, Sample Num: 105525, Cur Loss: 0.00000622, Cur Avg Loss: 0.00004549, Log Avg loss: 0.00006463, Global Avg Loss: 0.00117034, Time: 0.0220
Training, Epoch: 0036, Batch: 109525, Sample Num: 109525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004422, Log Avg loss: 0.00001076, Global Avg Loss: 0.00116958, Time: 0.0129
Training, Epoch: 0036, Batch: 113525, Sample Num: 113525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004341, Log Avg loss: 0.00002123, Global Avg Loss: 0.00116883, Time: 0.0217
Training, Epoch: 0036, Batch: 117525, Sample Num: 117525, Cur Loss: 0.00000001, Cur Avg Loss: 0.00004296, Log Avg loss: 0.00003012, Global Avg Loss: 0.00116809, Time: 0.0125
Training, Epoch: 0036, Batch: 121525, Sample Num: 121525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004390, Log Avg loss: 0.00007153, Global Avg Loss: 0.00116737, Time: 0.0233
Training, Epoch: 0036, Batch: 125525, Sample Num: 125525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004345, Log Avg loss: 0.00002996, Global Avg Loss: 0.00116663, Time: 0.0184
Training, Epoch: 0036, Batch: 129525, Sample Num: 129525, Cur Loss: 0.00000001, Cur Avg Loss: 0.00004678, Log Avg loss: 0.00015120, Global Avg Loss: 0.00116596, Time: 0.0579
Training, Epoch: 0036, Batch: 133525, Sample Num: 133525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005032, Log Avg loss: 0.00016500, Global Avg Loss: 0.00116531, Time: 0.0186
Training, Epoch: 0036, Batch: 137525, Sample Num: 137525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005015, Log Avg loss: 0.00004446, Global Avg Loss: 0.00116458, Time: 0.0342
Training, Epoch: 0036, Batch: 141525, Sample Num: 141525, Cur Loss: 0.00000019, Cur Avg Loss: 0.00005012, Log Avg loss: 0.00004892, Global Avg Loss: 0.00116385, Time: 0.0163
Training, Epoch: 0036, Batch: 145525, Sample Num: 145525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004991, Log Avg loss: 0.00004246, Global Avg Loss: 0.00116312, Time: 0.0135
Training, Epoch: 0036, Batch: 149525, Sample Num: 149525, Cur Loss: 0.00000595, Cur Avg Loss: 0.00005100, Log Avg loss: 0.00009099, Global Avg Loss: 0.00116242, Time: 0.0133
Training, Epoch: 0036, Batch: 153525, Sample Num: 153525, Cur Loss: 0.00000001, Cur Avg Loss: 0.00004996, Log Avg loss: 0.00001086, Global Avg Loss: 0.00116168, Time: 0.0135
Training, Epoch: 0036, Batch: 157525, Sample Num: 157525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004914, Log Avg loss: 0.00001769, Global Avg Loss: 0.00116093, Time: 0.0142
Training, Epoch: 0036, Batch: 161525, Sample Num: 161525, Cur Loss: 0.00000003, Cur Avg Loss: 0.00005002, Log Avg loss: 0.00008455, Global Avg Loss: 0.00116023, Time: 0.0122
Training, Epoch: 0036, Batch: 165525, Sample Num: 165525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004945, Log Avg loss: 0.00002641, Global Avg Loss: 0.00115950, Time: 0.0403
Training, Epoch: 0036, Batch: 169525, Sample Num: 169525, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004888, Log Avg loss: 0.00002559, Global Avg Loss: 0.00115876, Time: 0.0170
***** Running evaluation checkpoint-6169860 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-6169860 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3735.205842, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.255892, "eval_total_loss": 5477.113332, "eval_acc": 0.964586, "eval_prec": 0.986235, "eval_recall": 0.971036, "eval_f1": 0.976641, "eval_top2_acc": 0.994347, "eval_top3_acc": 0.998458, "eval_top5_acc": 0.999907, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999365, "eval_pr_auc": 0.986183, "eval_mcc": 0.96067, "eval_sn": 0.971036, "eval_sp": 0.99864, "update_flag": false, "test_avg_loss": 0.256677, "test_total_loss": 5493.905398, "test_acc": 0.964119, "test_prec": 0.98632, "test_recall": 0.976888, "test_f1": 0.980746, "test_top2_acc": 0.99388, "test_top3_acc": 0.998552, "test_top5_acc": 0.999907, "test_top10_acc": 1.0, "test_roc_auc": 0.999319, "test_pr_auc": 0.992334, "test_mcc": 0.960146, "test_sn": 0.976888, "test_sp": 0.998618, "lr": 5.6468437425941465e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0011584181100993036, "train_cur_epoch_loss": 8.316403489445126, "train_cur_epoch_avg_loss": 4.852468704638752e-05, "train_cur_epoch_time": 3735.2058424949646, "train_cur_epoch_avg_time": 0.021794240117250428, "epoch": 36, "step": 6169860}
##################################################
Training, Epoch: 0037, Batch: 002140, Sample Num: 2140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004575, Log Avg loss: 0.00003186, Global Avg Loss: 0.00115803, Time: 0.0480
Training, Epoch: 0037, Batch: 006140, Sample Num: 6140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002009, Log Avg loss: 0.00000636, Global Avg Loss: 0.00115729, Time: 0.0125
Training, Epoch: 0037, Batch: 010140, Sample Num: 10140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003232, Log Avg loss: 0.00005111, Global Avg Loss: 0.00115657, Time: 0.0130
Training, Epoch: 0037, Batch: 014140, Sample Num: 14140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003738, Log Avg loss: 0.00005020, Global Avg Loss: 0.00115585, Time: 0.0146
Training, Epoch: 0037, Batch: 018140, Sample Num: 18140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003446, Log Avg loss: 0.00002416, Global Avg Loss: 0.00115512, Time: 0.0125
Training, Epoch: 0037, Batch: 022140, Sample Num: 22140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003782, Log Avg loss: 0.00005303, Global Avg Loss: 0.00115441, Time: 0.0137
Training, Epoch: 0037, Batch: 026140, Sample Num: 26140, Cur Loss: 0.00000050, Cur Avg Loss: 0.00004029, Log Avg loss: 0.00005397, Global Avg Loss: 0.00115370, Time: 0.0126
Training, Epoch: 0037, Batch: 030140, Sample Num: 30140, Cur Loss: 0.00005682, Cur Avg Loss: 0.00003956, Log Avg loss: 0.00003481, Global Avg Loss: 0.00115298, Time: 0.0125
Training, Epoch: 0037, Batch: 034140, Sample Num: 34140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004301, Log Avg loss: 0.00006902, Global Avg Loss: 0.00115228, Time: 0.0124
Training, Epoch: 0037, Batch: 038140, Sample Num: 38140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004024, Log Avg loss: 0.00001652, Global Avg Loss: 0.00115155, Time: 0.0360
Training, Epoch: 0037, Batch: 042140, Sample Num: 42140, Cur Loss: 0.00000001, Cur Avg Loss: 0.00004284, Log Avg loss: 0.00006772, Global Avg Loss: 0.00115085, Time: 0.0356
Training, Epoch: 0037, Batch: 046140, Sample Num: 46140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004061, Log Avg loss: 0.00001704, Global Avg Loss: 0.00115012, Time: 0.0372
Training, Epoch: 0037, Batch: 050140, Sample Num: 50140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004014, Log Avg loss: 0.00003480, Global Avg Loss: 0.00114940, Time: 0.0240
Training, Epoch: 0037, Batch: 054140, Sample Num: 54140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003898, Log Avg loss: 0.00002434, Global Avg Loss: 0.00114868, Time: 0.0299
Training, Epoch: 0037, Batch: 058140, Sample Num: 58140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003965, Log Avg loss: 0.00004882, Global Avg Loss: 0.00114797, Time: 0.0168
Training, Epoch: 0037, Batch: 062140, Sample Num: 62140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004255, Log Avg loss: 0.00008458, Global Avg Loss: 0.00114729, Time: 0.0308
Training, Epoch: 0037, Batch: 066140, Sample Num: 66140, Cur Loss: 0.00000001, Cur Avg Loss: 0.00004235, Log Avg loss: 0.00003934, Global Avg Loss: 0.00114658, Time: 0.0337
Training, Epoch: 0037, Batch: 070140, Sample Num: 70140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004096, Log Avg loss: 0.00001792, Global Avg Loss: 0.00114586, Time: 0.0149
Training, Epoch: 0037, Batch: 074140, Sample Num: 74140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004406, Log Avg loss: 0.00009848, Global Avg Loss: 0.00114519, Time: 0.0260
Training, Epoch: 0037, Batch: 078140, Sample Num: 78140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004876, Log Avg loss: 0.00013574, Global Avg Loss: 0.00114454, Time: 0.0175
Training, Epoch: 0037, Batch: 082140, Sample Num: 82140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005060, Log Avg loss: 0.00008662, Global Avg Loss: 0.00114386, Time: 0.0145
Training, Epoch: 0037, Batch: 086140, Sample Num: 86140, Cur Loss: 0.00000001, Cur Avg Loss: 0.00004888, Log Avg loss: 0.00001352, Global Avg Loss: 0.00114314, Time: 0.0437
Training, Epoch: 0037, Batch: 090140, Sample Num: 90140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004712, Log Avg loss: 0.00000937, Global Avg Loss: 0.00114242, Time: 0.0342
Training, Epoch: 0037, Batch: 094140, Sample Num: 94140, Cur Loss: 0.00000003, Cur Avg Loss: 0.00004568, Log Avg loss: 0.00001312, Global Avg Loss: 0.00114170, Time: 0.0318
Training, Epoch: 0037, Batch: 098140, Sample Num: 98140, Cur Loss: 0.00000006, Cur Avg Loss: 0.00004719, Log Avg loss: 0.00008269, Global Avg Loss: 0.00114102, Time: 0.0156
Training, Epoch: 0037, Batch: 102140, Sample Num: 102140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004946, Log Avg loss: 0.00010510, Global Avg Loss: 0.00114036, Time: 0.0236
Training, Epoch: 0037, Batch: 106140, Sample Num: 106140, Cur Loss: 0.00004010, Cur Avg Loss: 0.00005027, Log Avg loss: 0.00007094, Global Avg Loss: 0.00113968, Time: 0.0151
Training, Epoch: 0037, Batch: 110140, Sample Num: 110140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005262, Log Avg loss: 0.00011503, Global Avg Loss: 0.00113902, Time: 0.0123
Training, Epoch: 0037, Batch: 114140, Sample Num: 114140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005156, Log Avg loss: 0.00002243, Global Avg Loss: 0.00113831, Time: 0.0225
Training, Epoch: 0037, Batch: 118140, Sample Num: 118140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005035, Log Avg loss: 0.00001586, Global Avg Loss: 0.00113760, Time: 0.0217
Training, Epoch: 0037, Batch: 122140, Sample Num: 122140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005081, Log Avg loss: 0.00006427, Global Avg Loss: 0.00113692, Time: 0.0189
Training, Epoch: 0037, Batch: 126140, Sample Num: 126140, Cur Loss: 0.00000105, Cur Avg Loss: 0.00004993, Log Avg loss: 0.00002303, Global Avg Loss: 0.00113621, Time: 0.0174
Training, Epoch: 0037, Batch: 130140, Sample Num: 130140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004906, Log Avg loss: 0.00002191, Global Avg Loss: 0.00113550, Time: 0.0220
Training, Epoch: 0037, Batch: 134140, Sample Num: 134140, Cur Loss: 0.00000001, Cur Avg Loss: 0.00004820, Log Avg loss: 0.00002021, Global Avg Loss: 0.00113479, Time: 0.0282
Training, Epoch: 0037, Batch: 138140, Sample Num: 138140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004735, Log Avg loss: 0.00001874, Global Avg Loss: 0.00113409, Time: 0.0176
Training, Epoch: 0037, Batch: 142140, Sample Num: 142140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004748, Log Avg loss: 0.00005189, Global Avg Loss: 0.00113340, Time: 0.0148
Training, Epoch: 0037, Batch: 146140, Sample Num: 146140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004820, Log Avg loss: 0.00007373, Global Avg Loss: 0.00113273, Time: 0.0349
Training, Epoch: 0037, Batch: 150140, Sample Num: 150140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004840, Log Avg loss: 0.00005579, Global Avg Loss: 0.00113205, Time: 0.0237
Training, Epoch: 0037, Batch: 154140, Sample Num: 154140, Cur Loss: 0.00000015, Cur Avg Loss: 0.00004954, Log Avg loss: 0.00009245, Global Avg Loss: 0.00113139, Time: 0.0155
Training, Epoch: 0037, Batch: 158140, Sample Num: 158140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005010, Log Avg loss: 0.00007147, Global Avg Loss: 0.00113072, Time: 0.0126
Training, Epoch: 0037, Batch: 162140, Sample Num: 162140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005068, Log Avg loss: 0.00007369, Global Avg Loss: 0.00113005, Time: 0.0150
Training, Epoch: 0037, Batch: 166140, Sample Num: 166140, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005321, Log Avg loss: 0.00015564, Global Avg Loss: 0.00112944, Time: 0.0306
Training, Epoch: 0037, Batch: 170140, Sample Num: 170140, Cur Loss: 0.00000019, Cur Avg Loss: 0.00005342, Log Avg loss: 0.00006237, Global Avg Loss: 0.00112876, Time: 0.0176
***** Running evaluation checkpoint-6341245 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-6341245 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3649.353050, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.262771, "eval_total_loss": 5624.353494, "eval_acc": 0.963979, "eval_prec": 0.985975, "eval_recall": 0.971345, "eval_f1": 0.976682, "eval_top2_acc": 0.99388, "eval_top3_acc": 0.998365, "eval_top5_acc": 0.999673, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999368, "eval_pr_auc": 0.987314, "eval_mcc": 0.959999, "eval_sn": 0.971345, "eval_sp": 0.998617, "update_flag": false, "test_avg_loss": 0.260191, "test_total_loss": 5569.120104, "test_acc": 0.964119, "test_prec": 0.985824, "test_recall": 0.977192, "test_f1": 0.980663, "test_top2_acc": 0.993693, "test_top3_acc": 0.998458, "test_top5_acc": 0.99986, "test_top10_acc": 1.0, "test_roc_auc": 0.999313, "test_pr_auc": 0.993622, "test_mcc": 0.96015, "test_sn": 0.977192, "test_sp": 0.998621, "lr": 5.243965286999176e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0011285476917195514, "train_cur_epoch_loss": 9.11984660027224, "train_cur_epoch_avg_loss": 5.321263004505785e-05, "train_cur_epoch_time": 3649.353049516678, "train_cur_epoch_avg_time": 0.021293304837160066, "epoch": 37, "step": 6341245}
##################################################
Training, Epoch: 0038, Batch: 002755, Sample Num: 2755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00012074, Log Avg loss: 0.00009082, Global Avg Loss: 0.00112811, Time: 0.0126
Training, Epoch: 0038, Batch: 006755, Sample Num: 6755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007122, Log Avg loss: 0.00003712, Global Avg Loss: 0.00112742, Time: 0.0140
Training, Epoch: 0038, Batch: 010755, Sample Num: 10755, Cur Loss: 0.00000021, Cur Avg Loss: 0.00005326, Log Avg loss: 0.00002293, Global Avg Loss: 0.00112673, Time: 0.0232
Training, Epoch: 0038, Batch: 014755, Sample Num: 14755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006709, Log Avg loss: 0.00010427, Global Avg Loss: 0.00112608, Time: 0.0340
Training, Epoch: 0038, Batch: 018755, Sample Num: 18755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00007048, Log Avg loss: 0.00008299, Global Avg Loss: 0.00112543, Time: 0.0148
Training, Epoch: 0038, Batch: 022755, Sample Num: 22755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005953, Log Avg loss: 0.00000817, Global Avg Loss: 0.00112473, Time: 0.0161
Training, Epoch: 0038, Batch: 026755, Sample Num: 26755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005198, Log Avg loss: 0.00000904, Global Avg Loss: 0.00112402, Time: 0.0151
Training, Epoch: 0038, Batch: 030755, Sample Num: 30755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004750, Log Avg loss: 0.00001752, Global Avg Loss: 0.00112333, Time: 0.0417
Training, Epoch: 0038, Batch: 034755, Sample Num: 34755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004809, Log Avg loss: 0.00005264, Global Avg Loss: 0.00112266, Time: 0.0125
Training, Epoch: 0038, Batch: 038755, Sample Num: 38755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004387, Log Avg loss: 0.00000723, Global Avg Loss: 0.00112196, Time: 0.0194
Training, Epoch: 0038, Batch: 042755, Sample Num: 42755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004493, Log Avg loss: 0.00005523, Global Avg Loss: 0.00112129, Time: 0.0183
Training, Epoch: 0038, Batch: 046755, Sample Num: 46755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004210, Log Avg loss: 0.00001183, Global Avg Loss: 0.00112060, Time: 0.0261
Training, Epoch: 0038, Batch: 050755, Sample Num: 50755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004194, Log Avg loss: 0.00003999, Global Avg Loss: 0.00111992, Time: 0.0131
Training, Epoch: 0038, Batch: 054755, Sample Num: 54755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004513, Log Avg loss: 0.00008569, Global Avg Loss: 0.00111927, Time: 0.0473
Training, Epoch: 0038, Batch: 058755, Sample Num: 58755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004304, Log Avg loss: 0.00001436, Global Avg Loss: 0.00111858, Time: 0.0152
Training, Epoch: 0038, Batch: 062755, Sample Num: 62755, Cur Loss: 0.00000001, Cur Avg Loss: 0.00004655, Log Avg loss: 0.00009813, Global Avg Loss: 0.00111794, Time: 0.0409
Training, Epoch: 0038, Batch: 066755, Sample Num: 66755, Cur Loss: 0.00000133, Cur Avg Loss: 0.00004762, Log Avg loss: 0.00006448, Global Avg Loss: 0.00111729, Time: 0.0212
Training, Epoch: 0038, Batch: 070755, Sample Num: 70755, Cur Loss: 0.00000172, Cur Avg Loss: 0.00004803, Log Avg loss: 0.00005478, Global Avg Loss: 0.00111662, Time: 0.0124
Training, Epoch: 0038, Batch: 074755, Sample Num: 74755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005158, Log Avg loss: 0.00011439, Global Avg Loss: 0.00111600, Time: 0.0221
Training, Epoch: 0038, Batch: 078755, Sample Num: 78755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005011, Log Avg loss: 0.00002271, Global Avg Loss: 0.00111532, Time: 0.0185
Training, Epoch: 0038, Batch: 082755, Sample Num: 82755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005076, Log Avg loss: 0.00006353, Global Avg Loss: 0.00111466, Time: 0.0381
Training, Epoch: 0038, Batch: 086755, Sample Num: 86755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004992, Log Avg loss: 0.00003248, Global Avg Loss: 0.00111399, Time: 0.0243
Training, Epoch: 0038, Batch: 090755, Sample Num: 90755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005848, Log Avg loss: 0.00024424, Global Avg Loss: 0.00111345, Time: 0.0281
Training, Epoch: 0038, Batch: 094755, Sample Num: 94755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005685, Log Avg loss: 0.00001979, Global Avg Loss: 0.00111277, Time: 0.0290
Training, Epoch: 0038, Batch: 098755, Sample Num: 98755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005489, Log Avg loss: 0.00000851, Global Avg Loss: 0.00111208, Time: 0.0291
Training, Epoch: 0038, Batch: 102755, Sample Num: 102755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005534, Log Avg loss: 0.00006630, Global Avg Loss: 0.00111143, Time: 0.0193
Training, Epoch: 0038, Batch: 106755, Sample Num: 106755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005516, Log Avg loss: 0.00005076, Global Avg Loss: 0.00111078, Time: 0.0318
Training, Epoch: 0038, Batch: 110755, Sample Num: 110755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005730, Log Avg loss: 0.00011418, Global Avg Loss: 0.00111016, Time: 0.0150
Training, Epoch: 0038, Batch: 114755, Sample Num: 114755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005586, Log Avg loss: 0.00001609, Global Avg Loss: 0.00110948, Time: 0.0222
Training, Epoch: 0038, Batch: 118755, Sample Num: 118755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005601, Log Avg loss: 0.00006036, Global Avg Loss: 0.00110883, Time: 0.0245
Training, Epoch: 0038, Batch: 122755, Sample Num: 122755, Cur Loss: 0.00000001, Cur Avg Loss: 0.00005540, Log Avg loss: 0.00003725, Global Avg Loss: 0.00110817, Time: 0.0127
Training, Epoch: 0038, Batch: 126755, Sample Num: 126755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005569, Log Avg loss: 0.00006447, Global Avg Loss: 0.00110752, Time: 0.0359
Training, Epoch: 0038, Batch: 130755, Sample Num: 130755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005414, Log Avg loss: 0.00000521, Global Avg Loss: 0.00110684, Time: 0.0124
Training, Epoch: 0038, Batch: 134755, Sample Num: 134755, Cur Loss: 0.00000001, Cur Avg Loss: 0.00005267, Log Avg loss: 0.00000463, Global Avg Loss: 0.00110616, Time: 0.0177
Training, Epoch: 0038, Batch: 138755, Sample Num: 138755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005218, Log Avg loss: 0.00003577, Global Avg Loss: 0.00110550, Time: 0.0583
Training, Epoch: 0038, Batch: 142755, Sample Num: 142755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005134, Log Avg loss: 0.00002211, Global Avg Loss: 0.00110483, Time: 0.0146
Training, Epoch: 0038, Batch: 146755, Sample Num: 146755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005054, Log Avg loss: 0.00002189, Global Avg Loss: 0.00110416, Time: 0.0230
Training, Epoch: 0038, Batch: 150755, Sample Num: 150755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005086, Log Avg loss: 0.00006275, Global Avg Loss: 0.00110352, Time: 0.0211
Training, Epoch: 0038, Batch: 154755, Sample Num: 154755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005276, Log Avg loss: 0.00012416, Global Avg Loss: 0.00110292, Time: 0.0126
Training, Epoch: 0038, Batch: 158755, Sample Num: 158755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005176, Log Avg loss: 0.00001328, Global Avg Loss: 0.00110225, Time: 0.0317
Training, Epoch: 0038, Batch: 162755, Sample Num: 162755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00005080, Log Avg loss: 0.00001248, Global Avg Loss: 0.00110158, Time: 0.0337
Training, Epoch: 0038, Batch: 166755, Sample Num: 166755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004980, Log Avg loss: 0.00000932, Global Avg Loss: 0.00110091, Time: 0.0167
Training, Epoch: 0038, Batch: 170755, Sample Num: 170755, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004930, Log Avg loss: 0.00002830, Global Avg Loss: 0.00110025, Time: 0.0349
***** Running evaluation checkpoint-6512630 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-6512630 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 4253.764256, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.269148, "eval_total_loss": 5760.833937, "eval_acc": 0.964493, "eval_prec": 0.97287, "eval_recall": 0.970559, "eval_f1": 0.971214, "eval_top2_acc": 0.993786, "eval_top3_acc": 0.998131, "eval_top5_acc": 0.999813, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999346, "eval_pr_auc": 0.988539, "eval_mcc": 0.96057, "eval_sn": 0.970559, "eval_sp": 0.998638, "update_flag": false, "test_avg_loss": 0.264553, "test_total_loss": 5662.489541, "test_acc": 0.963558, "test_prec": 0.982368, "test_recall": 0.979905, "test_f1": 0.981106, "test_top2_acc": 0.99416, "test_top3_acc": 0.998458, "test_top5_acc": 0.999813, "test_top10_acc": 1.0, "test_roc_auc": 0.999316, "test_pr_auc": 0.993461, "test_mcc": 0.959528, "test_sn": 0.979905, "test_sp": 0.998599, "lr": 4.8410868314042066e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0011001479735039765, "train_cur_epoch_loss": 8.45928930241905, "train_cur_epoch_avg_loss": 4.935839952399014e-05, "train_cur_epoch_time": 4253.764256000519, "train_cur_epoch_avg_time": 0.024819933226364728, "epoch": 38, "step": 6512630}
##################################################
Training, Epoch: 0039, Batch: 003370, Sample Num: 3370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000369, Log Avg loss: 0.00001345, Global Avg Loss: 0.00109958, Time: 0.0129
Training, Epoch: 0039, Batch: 007370, Sample Num: 7370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000734, Log Avg loss: 0.00001043, Global Avg Loss: 0.00109891, Time: 0.0268
Training, Epoch: 0039, Batch: 011370, Sample Num: 11370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001120, Log Avg loss: 0.00001831, Global Avg Loss: 0.00109825, Time: 0.0149
Training, Epoch: 0039, Batch: 015370, Sample Num: 15370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001901, Log Avg loss: 0.00004123, Global Avg Loss: 0.00109760, Time: 0.0138
Training, Epoch: 0039, Batch: 019370, Sample Num: 19370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002273, Log Avg loss: 0.00003702, Global Avg Loss: 0.00109695, Time: 0.0394
Training, Epoch: 0039, Batch: 023370, Sample Num: 23370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002899, Log Avg loss: 0.00005931, Global Avg Loss: 0.00109632, Time: 0.0150
Training, Epoch: 0039, Batch: 027370, Sample Num: 27370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002630, Log Avg loss: 0.00001058, Global Avg Loss: 0.00109565, Time: 0.0191
Training, Epoch: 0039, Batch: 031370, Sample Num: 31370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002426, Log Avg loss: 0.00001032, Global Avg Loss: 0.00109499, Time: 0.0370
Training, Epoch: 0039, Batch: 035370, Sample Num: 35370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003699, Log Avg loss: 0.00013685, Global Avg Loss: 0.00109441, Time: 0.0128
Training, Epoch: 0039, Batch: 039370, Sample Num: 39370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003974, Log Avg loss: 0.00006399, Global Avg Loss: 0.00109378, Time: 0.0123
Training, Epoch: 0039, Batch: 043370, Sample Num: 43370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003676, Log Avg loss: 0.00000743, Global Avg Loss: 0.00109311, Time: 0.0205
Training, Epoch: 0039, Batch: 047370, Sample Num: 47370, Cur Loss: 0.00000001, Cur Avg Loss: 0.00003425, Log Avg loss: 0.00000701, Global Avg Loss: 0.00109245, Time: 0.0154
Training, Epoch: 0039, Batch: 051370, Sample Num: 51370, Cur Loss: 0.00000847, Cur Avg Loss: 0.00003462, Log Avg loss: 0.00003902, Global Avg Loss: 0.00109181, Time: 0.0151
Training, Epoch: 0039, Batch: 055370, Sample Num: 55370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003811, Log Avg loss: 0.00008301, Global Avg Loss: 0.00109119, Time: 0.0155
Training, Epoch: 0039, Batch: 059370, Sample Num: 59370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003755, Log Avg loss: 0.00002974, Global Avg Loss: 0.00109055, Time: 0.0173
Training, Epoch: 0039, Batch: 063370, Sample Num: 63370, Cur Loss: 0.00000001, Cur Avg Loss: 0.00004308, Log Avg loss: 0.00012514, Global Avg Loss: 0.00108996, Time: 0.0119
Training, Epoch: 0039, Batch: 067370, Sample Num: 67370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004363, Log Avg loss: 0.00005235, Global Avg Loss: 0.00108933, Time: 0.0320
Training, Epoch: 0039, Batch: 071370, Sample Num: 71370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004289, Log Avg loss: 0.00003041, Global Avg Loss: 0.00108869, Time: 0.0223
Training, Epoch: 0039, Batch: 075370, Sample Num: 75370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004283, Log Avg loss: 0.00004174, Global Avg Loss: 0.00108805, Time: 0.0200
Training, Epoch: 0039, Batch: 079370, Sample Num: 79370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004459, Log Avg loss: 0.00007790, Global Avg Loss: 0.00108744, Time: 0.0134
Training, Epoch: 0039, Batch: 083370, Sample Num: 83370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004292, Log Avg loss: 0.00000979, Global Avg Loss: 0.00108679, Time: 0.0148
Training, Epoch: 0039, Batch: 087370, Sample Num: 87370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00004115, Log Avg loss: 0.00000413, Global Avg Loss: 0.00108613, Time: 0.0151
Training, Epoch: 0039, Batch: 091370, Sample Num: 91370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003963, Log Avg loss: 0.00000639, Global Avg Loss: 0.00108548, Time: 0.0320
Training, Epoch: 0039, Batch: 095370, Sample Num: 95370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003873, Log Avg loss: 0.00001819, Global Avg Loss: 0.00108483, Time: 0.0129
Training, Epoch: 0039, Batch: 099370, Sample Num: 99370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003730, Log Avg loss: 0.00000319, Global Avg Loss: 0.00108417, Time: 0.0227
Training, Epoch: 0039, Batch: 103370, Sample Num: 103370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003703, Log Avg loss: 0.00003040, Global Avg Loss: 0.00108354, Time: 0.0157
Training, Epoch: 0039, Batch: 107370, Sample Num: 107370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003666, Log Avg loss: 0.00002717, Global Avg Loss: 0.00108290, Time: 0.0126
Training, Epoch: 0039, Batch: 111370, Sample Num: 111370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003718, Log Avg loss: 0.00005099, Global Avg Loss: 0.00108228, Time: 0.0316
Training, Epoch: 0039, Batch: 115370, Sample Num: 115370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003631, Log Avg loss: 0.00001214, Global Avg Loss: 0.00108163, Time: 0.0168
Training, Epoch: 0039, Batch: 119370, Sample Num: 119370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003536, Log Avg loss: 0.00000802, Global Avg Loss: 0.00108098, Time: 0.0137
Training, Epoch: 0039, Batch: 123370, Sample Num: 123370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003502, Log Avg loss: 0.00002499, Global Avg Loss: 0.00108035, Time: 0.0355
Training, Epoch: 0039, Batch: 127370, Sample Num: 127370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003678, Log Avg loss: 0.00009101, Global Avg Loss: 0.00107975, Time: 0.0474
Training, Epoch: 0039, Batch: 131370, Sample Num: 131370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003615, Log Avg loss: 0.00001594, Global Avg Loss: 0.00107911, Time: 0.0200
Training, Epoch: 0039, Batch: 135370, Sample Num: 135370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003527, Log Avg loss: 0.00000626, Global Avg Loss: 0.00107846, Time: 0.0223
Training, Epoch: 0039, Batch: 139370, Sample Num: 139370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003432, Log Avg loss: 0.00000249, Global Avg Loss: 0.00107782, Time: 0.0290
Training, Epoch: 0039, Batch: 143370, Sample Num: 143370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003403, Log Avg loss: 0.00002380, Global Avg Loss: 0.00107718, Time: 0.0319
Training, Epoch: 0039, Batch: 147370, Sample Num: 147370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003411, Log Avg loss: 0.00003703, Global Avg Loss: 0.00107656, Time: 0.0123
Training, Epoch: 0039, Batch: 151370, Sample Num: 151370, Cur Loss: 0.00000026, Cur Avg Loss: 0.00003345, Log Avg loss: 0.00000920, Global Avg Loss: 0.00107592, Time: 0.0289
Training, Epoch: 0039, Batch: 155370, Sample Num: 155370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003441, Log Avg loss: 0.00007051, Global Avg Loss: 0.00107532, Time: 0.0142
Training, Epoch: 0039, Batch: 159370, Sample Num: 159370, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003426, Log Avg loss: 0.00002832, Global Avg Loss: 0.00107469, Time: 0.0138
Training, Epoch: 0039, Batch: 163370, Sample Num: 163370, Cur Loss: 0.00000066, Cur Avg Loss: 0.00003783, Log Avg loss: 0.00018012, Global Avg Loss: 0.00107415, Time: 0.0227
Training, Epoch: 0039, Batch: 167370, Sample Num: 167370, Cur Loss: 0.00000775, Cur Avg Loss: 0.00004281, Log Avg loss: 0.00024615, Global Avg Loss: 0.00107366, Time: 0.0138
Training, Epoch: 0039, Batch: 171370, Sample Num: 171370, Cur Loss: 0.00000660, Cur Avg Loss: 0.00004289, Log Avg loss: 0.00004650, Global Avg Loss: 0.00107304, Time: 0.0201
***** Running evaluation checkpoint-6684015 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-6684015 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3778.763410, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.266171, "eval_total_loss": 5697.131727, "eval_acc": 0.965193, "eval_prec": 0.979726, "eval_recall": 0.97178, "eval_f1": 0.975109, "eval_top2_acc": 0.994113, "eval_top3_acc": 0.998225, "eval_top5_acc": 0.999766, "eval_top10_acc": 0.999953, "eval_roc_auc": 0.999357, "eval_pr_auc": 0.987422, "eval_mcc": 0.961337, "eval_sn": 0.97178, "eval_sp": 0.998662, "update_flag": false, "test_avg_loss": 0.262727, "test_total_loss": 5623.41506, "test_acc": 0.963371, "test_prec": 0.986064, "test_recall": 0.976784, "test_f1": 0.980567, "test_top2_acc": 0.993739, "test_top3_acc": 0.998552, "test_top5_acc": 0.999813, "test_top10_acc": 1.0, "test_roc_auc": 0.999343, "test_pr_auc": 0.992299, "test_mcc": 0.959325, "test_sn": 0.976784, "test_sp": 0.998587, "lr": 4.438208375809237e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0010730387463551733, "train_cur_epoch_loss": 7.350379537381193, "train_cur_epoch_avg_loss": 4.288811469720917e-05, "train_cur_epoch_time": 3778.763409614563, "train_cur_epoch_avg_time": 0.022048390522009294, "epoch": 39, "step": 6684015}
##################################################
Training, Epoch: 0040, Batch: 003985, Sample Num: 3985, Cur Loss: 0.00000263, Cur Avg Loss: 0.00001163, Log Avg loss: 0.00001159, Global Avg Loss: 0.00107241, Time: 0.0728
Training, Epoch: 0040, Batch: 007985, Sample Num: 7985, Cur Loss: 0.00000001, Cur Avg Loss: 0.00000677, Log Avg loss: 0.00000193, Global Avg Loss: 0.00107177, Time: 0.0188
Training, Epoch: 0040, Batch: 011985, Sample Num: 11985, Cur Loss: 0.00000002, Cur Avg Loss: 0.00000533, Log Avg loss: 0.00000245, Global Avg Loss: 0.00107113, Time: 0.0443
Training, Epoch: 0040, Batch: 015985, Sample Num: 15985, Cur Loss: 0.00000031, Cur Avg Loss: 0.00000605, Log Avg loss: 0.00000820, Global Avg Loss: 0.00107049, Time: 0.0197
Training, Epoch: 0040, Batch: 019985, Sample Num: 19985, Cur Loss: 0.00000013, Cur Avg Loss: 0.00000866, Log Avg loss: 0.00001911, Global Avg Loss: 0.00106987, Time: 0.0351
Training, Epoch: 0040, Batch: 023985, Sample Num: 23985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000848, Log Avg loss: 0.00000760, Global Avg Loss: 0.00106923, Time: 0.0437
Training, Epoch: 0040, Batch: 027985, Sample Num: 27985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000854, Log Avg loss: 0.00000886, Global Avg Loss: 0.00106860, Time: 0.0242
Training, Epoch: 0040, Batch: 031985, Sample Num: 31985, Cur Loss: 0.00000008, Cur Avg Loss: 0.00000789, Log Avg loss: 0.00000339, Global Avg Loss: 0.00106797, Time: 0.0296
Training, Epoch: 0040, Batch: 035985, Sample Num: 35985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000760, Log Avg loss: 0.00000529, Global Avg Loss: 0.00106733, Time: 0.0208
Training, Epoch: 0040, Batch: 039985, Sample Num: 39985, Cur Loss: 0.00016919, Cur Avg Loss: 0.00000732, Log Avg loss: 0.00000475, Global Avg Loss: 0.00106670, Time: 0.0211
Training, Epoch: 0040, Batch: 043985, Sample Num: 43985, Cur Loss: 0.00000010, Cur Avg Loss: 0.00001275, Log Avg loss: 0.00006705, Global Avg Loss: 0.00106611, Time: 0.0561
Training, Epoch: 0040, Batch: 047985, Sample Num: 47985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002138, Log Avg loss: 0.00011623, Global Avg Loss: 0.00106554, Time: 0.0216
Training, Epoch: 0040, Batch: 051985, Sample Num: 51985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002195, Log Avg loss: 0.00002881, Global Avg Loss: 0.00106493, Time: 0.0125
Training, Epoch: 0040, Batch: 055985, Sample Num: 55985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002064, Log Avg loss: 0.00000360, Global Avg Loss: 0.00106430, Time: 0.0240
Training, Epoch: 0040, Batch: 059985, Sample Num: 59985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001999, Log Avg loss: 0.00001091, Global Avg Loss: 0.00106367, Time: 0.0556
Training, Epoch: 0040, Batch: 063985, Sample Num: 63985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001948, Log Avg loss: 0.00001183, Global Avg Loss: 0.00106305, Time: 0.0230
Training, Epoch: 0040, Batch: 067985, Sample Num: 67985, Cur Loss: 0.00000001, Cur Avg Loss: 0.00002137, Log Avg loss: 0.00005161, Global Avg Loss: 0.00106245, Time: 0.0222
Training, Epoch: 0040, Batch: 071985, Sample Num: 71985, Cur Loss: 0.00000001, Cur Avg Loss: 0.00002226, Log Avg loss: 0.00003735, Global Avg Loss: 0.00106184, Time: 0.0406
Training, Epoch: 0040, Batch: 075985, Sample Num: 75985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002264, Log Avg loss: 0.00002957, Global Avg Loss: 0.00106123, Time: 0.0270
Training, Epoch: 0040, Batch: 079985, Sample Num: 79985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002239, Log Avg loss: 0.00001766, Global Avg Loss: 0.00106061, Time: 0.0246
Training, Epoch: 0040, Batch: 083985, Sample Num: 83985, Cur Loss: 0.00000010, Cur Avg Loss: 0.00002197, Log Avg loss: 0.00001344, Global Avg Loss: 0.00106000, Time: 0.0125
Training, Epoch: 0040, Batch: 087985, Sample Num: 87985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002141, Log Avg loss: 0.00000963, Global Avg Loss: 0.00105938, Time: 0.0293
Training, Epoch: 0040, Batch: 091985, Sample Num: 91985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002127, Log Avg loss: 0.00001821, Global Avg Loss: 0.00105876, Time: 0.0231
Training, Epoch: 0040, Batch: 095985, Sample Num: 95985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002113, Log Avg loss: 0.00001789, Global Avg Loss: 0.00105815, Time: 0.0399
Training, Epoch: 0040, Batch: 099985, Sample Num: 99985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002071, Log Avg loss: 0.00001081, Global Avg Loss: 0.00105753, Time: 0.0125
Training, Epoch: 0040, Batch: 103985, Sample Num: 103985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002143, Log Avg loss: 0.00003944, Global Avg Loss: 0.00105693, Time: 0.0186
Training, Epoch: 0040, Batch: 107985, Sample Num: 107985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002333, Log Avg loss: 0.00007265, Global Avg Loss: 0.00105635, Time: 0.0244
Training, Epoch: 0040, Batch: 111985, Sample Num: 111985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002265, Log Avg loss: 0.00000414, Global Avg Loss: 0.00105573, Time: 0.0309
Training, Epoch: 0040, Batch: 115985, Sample Num: 115985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002357, Log Avg loss: 0.00004952, Global Avg Loss: 0.00105514, Time: 0.0167
Training, Epoch: 0040, Batch: 119985, Sample Num: 119985, Cur Loss: 0.00000004, Cur Avg Loss: 0.00002472, Log Avg loss: 0.00005787, Global Avg Loss: 0.00105455, Time: 0.0166
Training, Epoch: 0040, Batch: 123985, Sample Num: 123985, Cur Loss: 0.00000012, Cur Avg Loss: 0.00002455, Log Avg loss: 0.00001955, Global Avg Loss: 0.00105394, Time: 0.0286
Training, Epoch: 0040, Batch: 127985, Sample Num: 127985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002618, Log Avg loss: 0.00007673, Global Avg Loss: 0.00105337, Time: 0.0323
Training, Epoch: 0040, Batch: 131985, Sample Num: 131985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002719, Log Avg loss: 0.00005958, Global Avg Loss: 0.00105279, Time: 0.0261
Training, Epoch: 0040, Batch: 135985, Sample Num: 135985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002743, Log Avg loss: 0.00003522, Global Avg Loss: 0.00105219, Time: 0.0217
Training, Epoch: 0040, Batch: 139985, Sample Num: 139985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002871, Log Avg loss: 0.00007228, Global Avg Loss: 0.00105162, Time: 0.0220
Training, Epoch: 0040, Batch: 143985, Sample Num: 143985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002918, Log Avg loss: 0.00004567, Global Avg Loss: 0.00105103, Time: 0.0307
Training, Epoch: 0040, Batch: 147985, Sample Num: 147985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002846, Log Avg loss: 0.00000255, Global Avg Loss: 0.00105041, Time: 0.0246
Training, Epoch: 0040, Batch: 151985, Sample Num: 151985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002843, Log Avg loss: 0.00002727, Global Avg Loss: 0.00104981, Time: 0.0209
Training, Epoch: 0040, Batch: 155985, Sample Num: 155985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002785, Log Avg loss: 0.00000568, Global Avg Loss: 0.00104920, Time: 0.0153
Training, Epoch: 0040, Batch: 159985, Sample Num: 159985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002785, Log Avg loss: 0.00002798, Global Avg Loss: 0.00104861, Time: 0.0124
Training, Epoch: 0040, Batch: 163985, Sample Num: 163985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002735, Log Avg loss: 0.00000734, Global Avg Loss: 0.00104800, Time: 0.0149
Training, Epoch: 0040, Batch: 167985, Sample Num: 167985, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002841, Log Avg loss: 0.00007195, Global Avg Loss: 0.00104743, Time: 0.0124
***** Running evaluation checkpoint-6855400 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-6855400 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 4799.154012, Avg time per batch (s): 0.030000
{"eval_avg_loss": 0.266817, "eval_total_loss": 5710.961316, "eval_acc": 0.965147, "eval_prec": 0.978235, "eval_recall": 0.975059, "eval_f1": 0.97662, "eval_top2_acc": 0.99388, "eval_top3_acc": 0.998178, "eval_top5_acc": 0.999766, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999371, "eval_pr_auc": 0.988276, "eval_mcc": 0.961284, "eval_sn": 0.975059, "eval_sp": 0.99866, "update_flag": false, "test_avg_loss": 0.267432, "test_total_loss": 5724.119512, "test_acc": 0.964352, "test_prec": 0.986369, "test_recall": 0.97662, "test_f1": 0.980631, "test_top2_acc": 0.993786, "test_top3_acc": 0.998458, "test_top5_acc": 0.99972, "test_top10_acc": 1.0, "test_roc_auc": 0.999333, "test_pr_auc": 0.993266, "test_mcc": 0.960407, "test_sn": 0.97662, "test_sp": 0.998624, "lr": 4.035329920214267e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0010469158961967286, "train_cur_epoch_loss": 4.8201585672607985, "train_cur_epoch_avg_loss": 2.8124740013774827e-05, "train_cur_epoch_time": 4799.154012203217, "train_cur_epoch_avg_time": 0.02800218229251811, "epoch": 40, "step": 6855400}
##################################################
Training, Epoch: 0041, Batch: 000600, Sample Num: 600, Cur Loss: 0.00000018, Cur Avg Loss: 0.00028743, Log Avg loss: 0.00005496, Global Avg Loss: 0.00104685, Time: 0.0131
Steps: 6857000, Updated lr: 0.000040
Training, Epoch: 0041, Batch: 004600, Sample Num: 4600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00006937, Log Avg loss: 0.00003666, Global Avg Loss: 0.00104626, Time: 0.0282
Steps: 6861000, Updated lr: 0.000040
Training, Epoch: 0041, Batch: 008600, Sample Num: 8600, Cur Loss: 0.00000001, Cur Avg Loss: 0.00004537, Log Avg loss: 0.00001777, Global Avg Loss: 0.00104566, Time: 0.0128
Steps: 6865000, Updated lr: 0.000040
Training, Epoch: 0041, Batch: 012600, Sample Num: 12600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003179, Log Avg loss: 0.00000260, Global Avg Loss: 0.00104505, Time: 0.0187
Steps: 6869000, Updated lr: 0.000040
Training, Epoch: 0041, Batch: 016600, Sample Num: 16600, Cur Loss: 0.00000004, Cur Avg Loss: 0.00002437, Log Avg loss: 0.00000100, Global Avg Loss: 0.00104445, Time: 0.0130
Steps: 6873000, Updated lr: 0.000040
Training, Epoch: 0041, Batch: 020600, Sample Num: 20600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002002, Log Avg loss: 0.00000195, Global Avg Loss: 0.00104384, Time: 0.0479
Steps: 6877000, Updated lr: 0.000040
Training, Epoch: 0041, Batch: 024600, Sample Num: 24600, Cur Loss: 0.00000009, Cur Avg Loss: 0.00002288, Log Avg loss: 0.00003760, Global Avg Loss: 0.00104325, Time: 0.0142
Steps: 6881000, Updated lr: 0.000040
Training, Epoch: 0041, Batch: 028600, Sample Num: 28600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002016, Log Avg loss: 0.00000341, Global Avg Loss: 0.00104265, Time: 0.0301
Steps: 6885000, Updated lr: 0.000040
Training, Epoch: 0041, Batch: 032600, Sample Num: 32600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001918, Log Avg loss: 0.00001224, Global Avg Loss: 0.00104205, Time: 0.0422
Steps: 6889000, Updated lr: 0.000040
Training, Epoch: 0041, Batch: 036600, Sample Num: 36600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001831, Log Avg loss: 0.00001116, Global Avg Loss: 0.00104145, Time: 0.0141
Steps: 6893000, Updated lr: 0.000039
Training, Epoch: 0041, Batch: 040600, Sample Num: 40600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001890, Log Avg loss: 0.00002435, Global Avg Loss: 0.00104086, Time: 0.0124
Steps: 6897000, Updated lr: 0.000039
Training, Epoch: 0041, Batch: 044600, Sample Num: 44600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001750, Log Avg loss: 0.00000322, Global Avg Loss: 0.00104026, Time: 0.0135
Steps: 6901000, Updated lr: 0.000039
Training, Epoch: 0041, Batch: 048600, Sample Num: 48600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001756, Log Avg loss: 0.00001825, Global Avg Loss: 0.00103967, Time: 0.0125
Steps: 6905000, Updated lr: 0.000039
Training, Epoch: 0041, Batch: 052600, Sample Num: 52600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001766, Log Avg loss: 0.00001882, Global Avg Loss: 0.00103908, Time: 0.0127
Steps: 6909000, Updated lr: 0.000039
Training, Epoch: 0041, Batch: 056600, Sample Num: 56600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001675, Log Avg loss: 0.00000489, Global Avg Loss: 0.00103848, Time: 0.0173
Steps: 6913000, Updated lr: 0.000039
Training, Epoch: 0041, Batch: 060600, Sample Num: 60600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001660, Log Avg loss: 0.00001449, Global Avg Loss: 0.00103789, Time: 0.0129
Steps: 6917000, Updated lr: 0.000039
Training, Epoch: 0041, Batch: 064600, Sample Num: 64600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001567, Log Avg loss: 0.00000155, Global Avg Loss: 0.00103729, Time: 0.0122
Steps: 6921000, Updated lr: 0.000039
Training, Epoch: 0041, Batch: 068600, Sample Num: 68600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001495, Log Avg loss: 0.00000333, Global Avg Loss: 0.00103669, Time: 0.0166
Steps: 6925000, Updated lr: 0.000039
Training, Epoch: 0041, Batch: 072600, Sample Num: 72600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001809, Log Avg loss: 0.00007195, Global Avg Loss: 0.00103613, Time: 0.0355
Steps: 6929000, Updated lr: 0.000039
Training, Epoch: 0041, Batch: 076600, Sample Num: 76600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001772, Log Avg loss: 0.00001097, Global Avg Loss: 0.00103554, Time: 0.0211
Steps: 6933000, Updated lr: 0.000039
Training, Epoch: 0041, Batch: 080600, Sample Num: 80600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001710, Log Avg loss: 0.00000522, Global Avg Loss: 0.00103495, Time: 0.0130
Steps: 6937000, Updated lr: 0.000038
Training, Epoch: 0041, Batch: 084600, Sample Num: 84600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001664, Log Avg loss: 0.00000743, Global Avg Loss: 0.00103436, Time: 0.0215
Steps: 6941000, Updated lr: 0.000038
Training, Epoch: 0041, Batch: 088600, Sample Num: 88600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001980, Log Avg loss: 0.00008669, Global Avg Loss: 0.00103381, Time: 0.0458
Steps: 6945000, Updated lr: 0.000038
Training, Epoch: 0041, Batch: 092600, Sample Num: 92600, Cur Loss: 0.00000001, Cur Avg Loss: 0.00001928, Log Avg loss: 0.00000760, Global Avg Loss: 0.00103322, Time: 0.0322
Steps: 6949000, Updated lr: 0.000038
Training, Epoch: 0041, Batch: 096600, Sample Num: 96600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001945, Log Avg loss: 0.00002349, Global Avg Loss: 0.00103264, Time: 0.0188
Steps: 6953000, Updated lr: 0.000038
Training, Epoch: 0041, Batch: 100600, Sample Num: 100600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001939, Log Avg loss: 0.00001795, Global Avg Loss: 0.00103206, Time: 0.0155
Steps: 6957000, Updated lr: 0.000038
Training, Epoch: 0041, Batch: 104600, Sample Num: 104600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001896, Log Avg loss: 0.00000809, Global Avg Loss: 0.00103147, Time: 0.0180
Steps: 6961000, Updated lr: 0.000038
Training, Epoch: 0041, Batch: 108600, Sample Num: 108600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001895, Log Avg loss: 0.00001867, Global Avg Loss: 0.00103089, Time: 0.0183
Steps: 6965000, Updated lr: 0.000038
Training, Epoch: 0041, Batch: 112600, Sample Num: 112600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001843, Log Avg loss: 0.00000443, Global Avg Loss: 0.00103030, Time: 0.0356
Steps: 6969000, Updated lr: 0.000038
Training, Epoch: 0041, Batch: 116600, Sample Num: 116600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002002, Log Avg loss: 0.00006470, Global Avg Loss: 0.00102974, Time: 0.0132
Steps: 6973000, Updated lr: 0.000038
Training, Epoch: 0041, Batch: 120600, Sample Num: 120600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002019, Log Avg loss: 0.00002525, Global Avg Loss: 0.00102917, Time: 0.0126
Steps: 6977000, Updated lr: 0.000037
Training, Epoch: 0041, Batch: 124600, Sample Num: 124600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002195, Log Avg loss: 0.00007490, Global Avg Loss: 0.00102862, Time: 0.0126
Steps: 6981000, Updated lr: 0.000037
Training, Epoch: 0041, Batch: 128600, Sample Num: 128600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002259, Log Avg loss: 0.00004263, Global Avg Loss: 0.00102805, Time: 0.0327
Steps: 6985000, Updated lr: 0.000037
Training, Epoch: 0041, Batch: 132600, Sample Num: 132600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002220, Log Avg loss: 0.00000945, Global Avg Loss: 0.00102747, Time: 0.0251
Steps: 6989000, Updated lr: 0.000037
Training, Epoch: 0041, Batch: 136600, Sample Num: 136600, Cur Loss: 0.00000055, Cur Avg Loss: 0.00002251, Log Avg loss: 0.00003302, Global Avg Loss: 0.00102690, Time: 0.0146
Steps: 6993000, Updated lr: 0.000037
Training, Epoch: 0041, Batch: 140600, Sample Num: 140600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002199, Log Avg loss: 0.00000400, Global Avg Loss: 0.00102632, Time: 0.0129
Steps: 6997000, Updated lr: 0.000037
Training, Epoch: 0041, Batch: 144600, Sample Num: 144600, Cur Loss: 0.00001707, Cur Avg Loss: 0.00002272, Log Avg loss: 0.00004852, Global Avg Loss: 0.00102576, Time: 0.0127
Steps: 7001000, Updated lr: 0.000037
Training, Epoch: 0041, Batch: 148600, Sample Num: 148600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002248, Log Avg loss: 0.00001383, Global Avg Loss: 0.00102518, Time: 0.0151
Steps: 7005000, Updated lr: 0.000037
Training, Epoch: 0041, Batch: 152600, Sample Num: 152600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002202, Log Avg loss: 0.00000482, Global Avg Loss: 0.00102460, Time: 0.0133
Steps: 7009000, Updated lr: 0.000037
Training, Epoch: 0041, Batch: 156600, Sample Num: 156600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002255, Log Avg loss: 0.00004282, Global Avg Loss: 0.00102404, Time: 0.0148
Steps: 7013000, Updated lr: 0.000037
Training, Epoch: 0041, Batch: 160600, Sample Num: 160600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002302, Log Avg loss: 0.00004132, Global Avg Loss: 0.00102348, Time: 0.0137
Steps: 7017000, Updated lr: 0.000037
Training, Epoch: 0041, Batch: 164600, Sample Num: 164600, Cur Loss: 0.00000107, Cur Avg Loss: 0.00002323, Log Avg loss: 0.00003189, Global Avg Loss: 0.00102291, Time: 0.0400
Steps: 7021000, Updated lr: 0.000036
Training, Epoch: 0041, Batch: 168600, Sample Num: 168600, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002279, Log Avg loss: 0.00000448, Global Avg Loss: 0.00102233, Time: 0.0149
Steps: 7025000, Updated lr: 0.000036
***** Running evaluation checkpoint-7026785 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-7026785 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3646.164736, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.273448, "eval_total_loss": 5852.870697, "eval_acc": 0.965193, "eval_prec": 0.9865, "eval_recall": 0.972725, "eval_f1": 0.977634, "eval_top2_acc": 0.994347, "eval_top3_acc": 0.998318, "eval_top5_acc": 0.99986, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999356, "eval_pr_auc": 0.987654, "eval_mcc": 0.961335, "eval_sn": 0.972725, "eval_sp": 0.99866, "update_flag": true, "test_avg_loss": 0.277809, "test_total_loss": 5946.22637, "test_acc": 0.964913, "test_prec": 0.985462, "test_recall": 0.97195, "test_f1": 0.976653, "test_top2_acc": 0.994113, "test_top3_acc": 0.998458, "test_top5_acc": 0.999766, "test_top10_acc": 1.0, "test_roc_auc": 0.999301, "test_pr_auc": 0.990411, "test_mcc": 0.961029, "test_sn": 0.97195, "test_sp": 0.998646, "lr": 3.632451464619297e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0010219604359738147, "train_cur_epoch_loss": 4.0690273065914475, "train_cur_epoch_avg_loss": 2.3742027053659582e-05, "train_cur_epoch_time": 3646.1647362709045, "train_cur_epoch_avg_time": 0.0212747016149074, "epoch": 41, "step": 7026785}
##################################################
Training, Epoch: 0042, Batch: 001215, Sample Num: 1215, Cur Loss: 0.00000002, Cur Avg Loss: 0.00000152, Log Avg loss: 0.00005719, Global Avg Loss: 0.00102178, Time: 0.0362
Training, Epoch: 0042, Batch: 005215, Sample Num: 5215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000847, Log Avg loss: 0.00001058, Global Avg Loss: 0.00102121, Time: 0.0149
Training, Epoch: 0042, Batch: 009215, Sample Num: 9215, Cur Loss: 0.00000463, Cur Avg Loss: 0.00002244, Log Avg loss: 0.00004066, Global Avg Loss: 0.00102065, Time: 0.0133
Training, Epoch: 0042, Batch: 013215, Sample Num: 13215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00003313, Log Avg loss: 0.00005775, Global Avg Loss: 0.00102010, Time: 0.0327
Training, Epoch: 0042, Batch: 017215, Sample Num: 17215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002682, Log Avg loss: 0.00000599, Global Avg Loss: 0.00101953, Time: 0.0190
Training, Epoch: 0042, Batch: 021215, Sample Num: 21215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002323, Log Avg loss: 0.00000774, Global Avg Loss: 0.00101895, Time: 0.0128
Training, Epoch: 0042, Batch: 025215, Sample Num: 25215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002186, Log Avg loss: 0.00001461, Global Avg Loss: 0.00101838, Time: 0.0143
Training, Epoch: 0042, Batch: 029215, Sample Num: 29215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001906, Log Avg loss: 0.00000142, Global Avg Loss: 0.00101781, Time: 0.0377
Training, Epoch: 0042, Batch: 033215, Sample Num: 33215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001708, Log Avg loss: 0.00000265, Global Avg Loss: 0.00101723, Time: 0.0197
Training, Epoch: 0042, Batch: 037215, Sample Num: 37215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001575, Log Avg loss: 0.00000471, Global Avg Loss: 0.00101666, Time: 0.0191
Training, Epoch: 0042, Batch: 041215, Sample Num: 41215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001823, Log Avg loss: 0.00004123, Global Avg Loss: 0.00101611, Time: 0.0365
Training, Epoch: 0042, Batch: 045215, Sample Num: 45215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001702, Log Avg loss: 0.00000460, Global Avg Loss: 0.00101554, Time: 0.0169
Training, Epoch: 0042, Batch: 049215, Sample Num: 49215, Cur Loss: 0.00000001, Cur Avg Loss: 0.00001604, Log Avg loss: 0.00000497, Global Avg Loss: 0.00101496, Time: 0.0256
Training, Epoch: 0042, Batch: 053215, Sample Num: 53215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001879, Log Avg loss: 0.00005267, Global Avg Loss: 0.00101442, Time: 0.0185
Training, Epoch: 0042, Batch: 057215, Sample Num: 57215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001814, Log Avg loss: 0.00000947, Global Avg Loss: 0.00101385, Time: 0.0152
Training, Epoch: 0042, Batch: 061215, Sample Num: 61215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001718, Log Avg loss: 0.00000344, Global Avg Loss: 0.00101328, Time: 0.0424
Training, Epoch: 0042, Batch: 065215, Sample Num: 65215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001697, Log Avg loss: 0.00001369, Global Avg Loss: 0.00101272, Time: 0.0142
Training, Epoch: 0042, Batch: 069215, Sample Num: 69215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001609, Log Avg loss: 0.00000175, Global Avg Loss: 0.00101215, Time: 0.0143
Training, Epoch: 0042, Batch: 073215, Sample Num: 73215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001605, Log Avg loss: 0.00001537, Global Avg Loss: 0.00101159, Time: 0.0153
Training, Epoch: 0042, Batch: 077215, Sample Num: 77215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001574, Log Avg loss: 0.00001017, Global Avg Loss: 0.00101102, Time: 0.0154
Training, Epoch: 0042, Batch: 081215, Sample Num: 81215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001521, Log Avg loss: 0.00000488, Global Avg Loss: 0.00101046, Time: 0.0140
Training, Epoch: 0042, Batch: 085215, Sample Num: 85215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001461, Log Avg loss: 0.00000250, Global Avg Loss: 0.00100989, Time: 0.0125
Training, Epoch: 0042, Batch: 089215, Sample Num: 89215, Cur Loss: 0.00000012, Cur Avg Loss: 0.00001565, Log Avg loss: 0.00003784, Global Avg Loss: 0.00100934, Time: 0.0136
Training, Epoch: 0042, Batch: 093215, Sample Num: 93215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001507, Log Avg loss: 0.00000199, Global Avg Loss: 0.00100878, Time: 0.0130
Training, Epoch: 0042, Batch: 097215, Sample Num: 97215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001451, Log Avg loss: 0.00000154, Global Avg Loss: 0.00100821, Time: 0.0181
Training, Epoch: 0042, Batch: 101215, Sample Num: 101215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001407, Log Avg loss: 0.00000333, Global Avg Loss: 0.00100765, Time: 0.0213
Training, Epoch: 0042, Batch: 105215, Sample Num: 105215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001363, Log Avg loss: 0.00000263, Global Avg Loss: 0.00100709, Time: 0.0266
Training, Epoch: 0042, Batch: 109215, Sample Num: 109215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001464, Log Avg loss: 0.00004116, Global Avg Loss: 0.00100654, Time: 0.0344
Training, Epoch: 0042, Batch: 113215, Sample Num: 113215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001609, Log Avg loss: 0.00005549, Global Avg Loss: 0.00100601, Time: 0.0406
Training, Epoch: 0042, Batch: 117215, Sample Num: 117215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001637, Log Avg loss: 0.00002433, Global Avg Loss: 0.00100546, Time: 0.0197
Training, Epoch: 0042, Batch: 121215, Sample Num: 121215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001645, Log Avg loss: 0.00001895, Global Avg Loss: 0.00100491, Time: 0.0192
Training, Epoch: 0042, Batch: 125215, Sample Num: 125215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001609, Log Avg loss: 0.00000501, Global Avg Loss: 0.00100435, Time: 0.0175
Training, Epoch: 0042, Batch: 129215, Sample Num: 129215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001675, Log Avg loss: 0.00003767, Global Avg Loss: 0.00100381, Time: 0.0130
Training, Epoch: 0042, Batch: 133215, Sample Num: 133215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001917, Log Avg loss: 0.00009718, Global Avg Loss: 0.00100330, Time: 0.0282
Training, Epoch: 0042, Batch: 137215, Sample Num: 137215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002143, Log Avg loss: 0.00009667, Global Avg Loss: 0.00100280, Time: 0.0296
Training, Epoch: 0042, Batch: 141215, Sample Num: 141215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002117, Log Avg loss: 0.00001242, Global Avg Loss: 0.00100224, Time: 0.0151
Training, Epoch: 0042, Batch: 145215, Sample Num: 145215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002062, Log Avg loss: 0.00000095, Global Avg Loss: 0.00100169, Time: 0.0352
Training, Epoch: 0042, Batch: 149215, Sample Num: 149215, Cur Loss: 0.00000560, Cur Avg Loss: 0.00002010, Log Avg loss: 0.00000136, Global Avg Loss: 0.00100113, Time: 0.0128
Training, Epoch: 0042, Batch: 153215, Sample Num: 153215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002061, Log Avg loss: 0.00003963, Global Avg Loss: 0.00100059, Time: 0.0124
Training, Epoch: 0042, Batch: 157215, Sample Num: 157215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002063, Log Avg loss: 0.00002134, Global Avg Loss: 0.00100005, Time: 0.0126
Training, Epoch: 0042, Batch: 161215, Sample Num: 161215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002019, Log Avg loss: 0.00000298, Global Avg Loss: 0.00099949, Time: 0.0127
Training, Epoch: 0042, Batch: 165215, Sample Num: 165215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002011, Log Avg loss: 0.00001688, Global Avg Loss: 0.00099895, Time: 0.0402
Training, Epoch: 0042, Batch: 169215, Sample Num: 169215, Cur Loss: 0.00000000, Cur Avg Loss: 0.00002043, Log Avg loss: 0.00003365, Global Avg Loss: 0.00099841, Time: 0.0253
***** Running evaluation checkpoint-7198170 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-7198170 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3755.852012, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.284377, "eval_total_loss": 6086.813843, "eval_acc": 0.965007, "eval_prec": 0.98627, "eval_recall": 0.971499, "eval_f1": 0.976902, "eval_top2_acc": 0.993926, "eval_top3_acc": 0.998178, "eval_top5_acc": 0.999673, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999334, "eval_pr_auc": 0.986241, "eval_mcc": 0.961147, "eval_sn": 0.971499, "eval_sp": 0.998652, "update_flag": false, "test_avg_loss": 0.285031, "test_total_loss": 6100.804082, "test_acc": 0.965147, "test_prec": 0.986752, "test_recall": 0.976984, "test_f1": 0.980991, "test_top2_acc": 0.993739, "test_top3_acc": 0.998318, "test_top5_acc": 0.99972, "test_top10_acc": 1.0, "test_roc_auc": 0.99929, "test_pr_auc": 0.99158, "test_mcc": 0.961324, "test_sn": 0.976984, "test_sp": 0.998652, "lr": 3.229573009024327e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0009981094419521841, "train_cur_epoch_loss": 3.4651796821752843, "train_cur_epoch_avg_loss": 2.0218687062317496e-05, "train_cur_epoch_time": 3755.852011680603, "train_cur_epoch_avg_time": 0.02191470672276222, "epoch": 42, "step": 7198170}
##################################################
Training, Epoch: 0043, Batch: 001830, Sample Num: 1830, Cur Loss: 0.00000001, Cur Avg Loss: 0.00000316, Log Avg loss: 0.00000344, Global Avg Loss: 0.00099786, Time: 0.0127
Training, Epoch: 0043, Batch: 005830, Sample Num: 5830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000770, Log Avg loss: 0.00000978, Global Avg Loss: 0.00099731, Time: 0.0399
Training, Epoch: 0043, Batch: 009830, Sample Num: 9830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000563, Log Avg loss: 0.00000260, Global Avg Loss: 0.00099676, Time: 0.0518
Training, Epoch: 0043, Batch: 013830, Sample Num: 13830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000472, Log Avg loss: 0.00000250, Global Avg Loss: 0.00099620, Time: 0.0233
Training, Epoch: 0043, Batch: 017830, Sample Num: 17830, Cur Loss: 0.00000054, Cur Avg Loss: 0.00000397, Log Avg loss: 0.00000135, Global Avg Loss: 0.00099565, Time: 0.0215
Training, Epoch: 0043, Batch: 021830, Sample Num: 21830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000378, Log Avg loss: 0.00000294, Global Avg Loss: 0.00099510, Time: 0.0134
Training, Epoch: 0043, Batch: 025830, Sample Num: 25830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000325, Log Avg loss: 0.00000039, Global Avg Loss: 0.00099455, Time: 0.0212
Training, Epoch: 0043, Batch: 029830, Sample Num: 29830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000329, Log Avg loss: 0.00000356, Global Avg Loss: 0.00099400, Time: 0.0284
Training, Epoch: 0043, Batch: 033830, Sample Num: 33830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000301, Log Avg loss: 0.00000088, Global Avg Loss: 0.00099345, Time: 0.0709
Training, Epoch: 0043, Batch: 037830, Sample Num: 37830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000407, Log Avg loss: 0.00001304, Global Avg Loss: 0.00099291, Time: 0.0391
Training, Epoch: 0043, Batch: 041830, Sample Num: 41830, Cur Loss: 0.00000152, Cur Avg Loss: 0.00000533, Log Avg loss: 0.00001728, Global Avg Loss: 0.00099237, Time: 0.0133
Training, Epoch: 0043, Batch: 045830, Sample Num: 45830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000690, Log Avg loss: 0.00002328, Global Avg Loss: 0.00099184, Time: 0.0392
Training, Epoch: 0043, Batch: 049830, Sample Num: 49830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000643, Log Avg loss: 0.00000104, Global Avg Loss: 0.00099129, Time: 0.0208
Training, Epoch: 0043, Batch: 053830, Sample Num: 53830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000613, Log Avg loss: 0.00000244, Global Avg Loss: 0.00099075, Time: 0.0126
Training, Epoch: 0043, Batch: 057830, Sample Num: 57830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000574, Log Avg loss: 0.00000046, Global Avg Loss: 0.00099020, Time: 0.0232
Training, Epoch: 0043, Batch: 061830, Sample Num: 61830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000591, Log Avg loss: 0.00000837, Global Avg Loss: 0.00098966, Time: 0.0157
Training, Epoch: 0043, Batch: 065830, Sample Num: 65830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000753, Log Avg loss: 0.00003263, Global Avg Loss: 0.00098913, Time: 0.0150
Training, Epoch: 0043, Batch: 069830, Sample Num: 69830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000736, Log Avg loss: 0.00000444, Global Avg Loss: 0.00098859, Time: 0.0675
Training, Epoch: 0043, Batch: 073830, Sample Num: 73830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000706, Log Avg loss: 0.00000181, Global Avg Loss: 0.00098805, Time: 0.0364
Training, Epoch: 0043, Batch: 077830, Sample Num: 77830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000950, Log Avg loss: 0.00005456, Global Avg Loss: 0.00098753, Time: 0.0392
Training, Epoch: 0043, Batch: 081830, Sample Num: 81830, Cur Loss: 0.00000047, Cur Avg Loss: 0.00000939, Log Avg loss: 0.00000727, Global Avg Loss: 0.00098700, Time: 0.0505
Training, Epoch: 0043, Batch: 085830, Sample Num: 85830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001024, Log Avg loss: 0.00002757, Global Avg Loss: 0.00098647, Time: 0.0348
Training, Epoch: 0043, Batch: 089830, Sample Num: 89830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000992, Log Avg loss: 0.00000324, Global Avg Loss: 0.00098593, Time: 0.0172
Training, Epoch: 0043, Batch: 093830, Sample Num: 93830, Cur Loss: 0.00000001, Cur Avg Loss: 0.00001063, Log Avg loss: 0.00002635, Global Avg Loss: 0.00098540, Time: 0.0175
Training, Epoch: 0043, Batch: 097830, Sample Num: 97830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001080, Log Avg loss: 0.00001488, Global Avg Loss: 0.00098487, Time: 0.0145
Training, Epoch: 0043, Batch: 101830, Sample Num: 101830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001045, Log Avg loss: 0.00000180, Global Avg Loss: 0.00098433, Time: 0.0133
Training, Epoch: 0043, Batch: 105830, Sample Num: 105830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001486, Log Avg loss: 0.00012714, Global Avg Loss: 0.00098386, Time: 0.0126
Training, Epoch: 0043, Batch: 109830, Sample Num: 109830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001611, Log Avg loss: 0.00004924, Global Avg Loss: 0.00098335, Time: 0.0125
Training, Epoch: 0043, Batch: 113830, Sample Num: 113830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001560, Log Avg loss: 0.00000175, Global Avg Loss: 0.00098281, Time: 0.0159
Training, Epoch: 0043, Batch: 117830, Sample Num: 117830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001510, Log Avg loss: 0.00000072, Global Avg Loss: 0.00098228, Time: 0.0147
Training, Epoch: 0043, Batch: 121830, Sample Num: 121830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001594, Log Avg loss: 0.00004073, Global Avg Loss: 0.00098176, Time: 0.0476
Training, Epoch: 0043, Batch: 125830, Sample Num: 125830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001556, Log Avg loss: 0.00000384, Global Avg Loss: 0.00098123, Time: 0.0126
Training, Epoch: 0043, Batch: 129830, Sample Num: 129830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001612, Log Avg loss: 0.00003382, Global Avg Loss: 0.00098071, Time: 0.0243
Training, Epoch: 0043, Batch: 133830, Sample Num: 133830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001627, Log Avg loss: 0.00002105, Global Avg Loss: 0.00098019, Time: 0.0148
Training, Epoch: 0043, Batch: 137830, Sample Num: 137830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001614, Log Avg loss: 0.00001201, Global Avg Loss: 0.00097966, Time: 0.0122
Training, Epoch: 0043, Batch: 141830, Sample Num: 141830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001603, Log Avg loss: 0.00001226, Global Avg Loss: 0.00097913, Time: 0.0186
Training, Epoch: 0043, Batch: 145830, Sample Num: 145830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001567, Log Avg loss: 0.00000265, Global Avg Loss: 0.00097860, Time: 0.0214
Training, Epoch: 0043, Batch: 149830, Sample Num: 149830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001606, Log Avg loss: 0.00003031, Global Avg Loss: 0.00097808, Time: 0.0478
Training, Epoch: 0043, Batch: 153830, Sample Num: 153830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001569, Log Avg loss: 0.00000189, Global Avg Loss: 0.00097755, Time: 0.0205
Training, Epoch: 0043, Batch: 157830, Sample Num: 157830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001590, Log Avg loss: 0.00002414, Global Avg Loss: 0.00097704, Time: 0.0131
Training, Epoch: 0043, Batch: 161830, Sample Num: 161830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001553, Log Avg loss: 0.00000069, Global Avg Loss: 0.00097650, Time: 0.0192
Training, Epoch: 0043, Batch: 165830, Sample Num: 165830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001529, Log Avg loss: 0.00000582, Global Avg Loss: 0.00097598, Time: 0.0146
Training, Epoch: 0043, Batch: 169830, Sample Num: 169830, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001502, Log Avg loss: 0.00000369, Global Avg Loss: 0.00097545, Time: 0.0443
***** Running evaluation checkpoint-7369555 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-7369555 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 4310.136799, Avg time per batch (s): 0.030000
{"eval_avg_loss": 0.286806, "eval_total_loss": 6138.79889, "eval_acc": 0.965147, "eval_prec": 0.986766, "eval_recall": 0.972768, "eval_f1": 0.97779, "eval_top2_acc": 0.99444, "eval_top3_acc": 0.998365, "eval_top5_acc": 0.99972, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999329, "eval_pr_auc": 0.986682, "eval_mcc": 0.96128, "eval_sn": 0.972768, "eval_sp": 0.998656, "update_flag": true, "test_avg_loss": 0.284667, "test_total_loss": 6093.012645, "test_acc": 0.964493, "test_prec": 0.986727, "test_recall": 0.976321, "test_f1": 0.980644, "test_top2_acc": 0.994207, "test_top3_acc": 0.998365, "test_top5_acc": 0.999813, "test_top10_acc": 1.0, "test_roc_auc": 0.999292, "test_pr_auc": 0.993475, "test_mcc": 0.960561, "test_sn": 0.976321, "test_sp": 0.998626, "lr": 2.8266945534293572e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0009752450609752767, "train_cur_epoch_loss": 2.560673558173197, "train_cur_epoch_avg_loss": 1.4941059942078927e-05, "train_cur_epoch_time": 4310.136798858643, "train_cur_epoch_avg_time": 0.02514885666107677, "epoch": 43, "step": 7369555}
##################################################
Training, Epoch: 0044, Batch: 002445, Sample Num: 2445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000595, Log Avg loss: 0.00000613, Global Avg Loss: 0.00097492, Time: 0.0149
Training, Epoch: 0044, Batch: 006445, Sample Num: 6445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000313, Log Avg loss: 0.00000140, Global Avg Loss: 0.00097440, Time: 0.0130
Training, Epoch: 0044, Batch: 010445, Sample Num: 10445, Cur Loss: 0.00001533, Cur Avg Loss: 0.00000375, Log Avg loss: 0.00000474, Global Avg Loss: 0.00097387, Time: 0.0138
Training, Epoch: 0044, Batch: 014445, Sample Num: 14445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000291, Log Avg loss: 0.00000072, Global Avg Loss: 0.00097334, Time: 0.0131
Training, Epoch: 0044, Batch: 018445, Sample Num: 18445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000248, Log Avg loss: 0.00000092, Global Avg Loss: 0.00097282, Time: 0.0136
Training, Epoch: 0044, Batch: 022445, Sample Num: 22445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000215, Log Avg loss: 0.00000062, Global Avg Loss: 0.00097229, Time: 0.0150
Training, Epoch: 0044, Batch: 026445, Sample Num: 26445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000504, Log Avg loss: 0.00002129, Global Avg Loss: 0.00097178, Time: 0.0195
Training, Epoch: 0044, Batch: 030445, Sample Num: 30445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000474, Log Avg loss: 0.00000271, Global Avg Loss: 0.00097125, Time: 0.0219
Training, Epoch: 0044, Batch: 034445, Sample Num: 34445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000661, Log Avg loss: 0.00002085, Global Avg Loss: 0.00097074, Time: 0.0194
Training, Epoch: 0044, Batch: 038445, Sample Num: 38445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000616, Log Avg loss: 0.00000226, Global Avg Loss: 0.00097022, Time: 0.0356
Training, Epoch: 0044, Batch: 042445, Sample Num: 42445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000594, Log Avg loss: 0.00000388, Global Avg Loss: 0.00096969, Time: 0.0290
Training, Epoch: 0044, Batch: 046445, Sample Num: 46445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000740, Log Avg loss: 0.00002290, Global Avg Loss: 0.00096918, Time: 0.0146
Training, Epoch: 0044, Batch: 050445, Sample Num: 50445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000685, Log Avg loss: 0.00000049, Global Avg Loss: 0.00096866, Time: 0.0376
Training, Epoch: 0044, Batch: 054445, Sample Num: 54445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000716, Log Avg loss: 0.00001100, Global Avg Loss: 0.00096815, Time: 0.0167
Training, Epoch: 0044, Batch: 058445, Sample Num: 58445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000672, Log Avg loss: 0.00000075, Global Avg Loss: 0.00096762, Time: 0.0361
Training, Epoch: 0044, Batch: 062445, Sample Num: 62445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000648, Log Avg loss: 0.00000304, Global Avg Loss: 0.00096711, Time: 0.0160
Training, Epoch: 0044, Batch: 066445, Sample Num: 66445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000739, Log Avg loss: 0.00002154, Global Avg Loss: 0.00096660, Time: 0.0141
Training, Epoch: 0044, Batch: 070445, Sample Num: 70445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000869, Log Avg loss: 0.00003035, Global Avg Loss: 0.00096609, Time: 0.0143
Training, Epoch: 0044, Batch: 074445, Sample Num: 74445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000963, Log Avg loss: 0.00002618, Global Avg Loss: 0.00096559, Time: 0.0142
Training, Epoch: 0044, Batch: 078445, Sample Num: 78445, Cur Loss: 0.00000001, Cur Avg Loss: 0.00000924, Log Avg loss: 0.00000198, Global Avg Loss: 0.00096507, Time: 0.0124
Training, Epoch: 0044, Batch: 082445, Sample Num: 82445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000961, Log Avg loss: 0.00001673, Global Avg Loss: 0.00096456, Time: 0.0130
Training, Epoch: 0044, Batch: 086445, Sample Num: 86445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000924, Log Avg loss: 0.00000178, Global Avg Loss: 0.00096405, Time: 0.0192
Training, Epoch: 0044, Batch: 090445, Sample Num: 90445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000895, Log Avg loss: 0.00000247, Global Avg Loss: 0.00096353, Time: 0.0138
Training, Epoch: 0044, Batch: 094445, Sample Num: 94445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001027, Log Avg loss: 0.00004031, Global Avg Loss: 0.00096303, Time: 0.0183
Training, Epoch: 0044, Batch: 098445, Sample Num: 98445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000990, Log Avg loss: 0.00000099, Global Avg Loss: 0.00096252, Time: 0.0157
Training, Epoch: 0044, Batch: 102445, Sample Num: 102445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000966, Log Avg loss: 0.00000383, Global Avg Loss: 0.00096201, Time: 0.0125
Training, Epoch: 0044, Batch: 106445, Sample Num: 106445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000939, Log Avg loss: 0.00000245, Global Avg Loss: 0.00096149, Time: 0.0129
Training, Epoch: 0044, Batch: 110445, Sample Num: 110445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001003, Log Avg loss: 0.00002701, Global Avg Loss: 0.00096099, Time: 0.0179
Training, Epoch: 0044, Batch: 114445, Sample Num: 114445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001003, Log Avg loss: 0.00001015, Global Avg Loss: 0.00096049, Time: 0.0193
Training, Epoch: 0044, Batch: 118445, Sample Num: 118445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000976, Log Avg loss: 0.00000191, Global Avg Loss: 0.00095997, Time: 0.0173
Training, Epoch: 0044, Batch: 122445, Sample Num: 122445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000949, Log Avg loss: 0.00000165, Global Avg Loss: 0.00095946, Time: 0.0201
Training, Epoch: 0044, Batch: 126445, Sample Num: 126445, Cur Loss: 0.00000004, Cur Avg Loss: 0.00000922, Log Avg loss: 0.00000080, Global Avg Loss: 0.00095895, Time: 0.0166
Training, Epoch: 0044, Batch: 130445, Sample Num: 130445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000894, Log Avg loss: 0.00000030, Global Avg Loss: 0.00095844, Time: 0.0392
Training, Epoch: 0044, Batch: 134445, Sample Num: 134445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000869, Log Avg loss: 0.00000050, Global Avg Loss: 0.00095793, Time: 0.0346
Training, Epoch: 0044, Batch: 138445, Sample Num: 138445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000917, Log Avg loss: 0.00002534, Global Avg Loss: 0.00095743, Time: 0.0345
Training, Epoch: 0044, Batch: 142445, Sample Num: 142445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000945, Log Avg loss: 0.00001900, Global Avg Loss: 0.00095693, Time: 0.0146
Training, Epoch: 0044, Batch: 146445, Sample Num: 146445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000951, Log Avg loss: 0.00001182, Global Avg Loss: 0.00095643, Time: 0.0125
Training, Epoch: 0044, Batch: 150445, Sample Num: 150445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000928, Log Avg loss: 0.00000064, Global Avg Loss: 0.00095592, Time: 0.0148
Training, Epoch: 0044, Batch: 154445, Sample Num: 154445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000959, Log Avg loss: 0.00002125, Global Avg Loss: 0.00095542, Time: 0.0125
Training, Epoch: 0044, Batch: 158445, Sample Num: 158445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001001, Log Avg loss: 0.00002645, Global Avg Loss: 0.00095493, Time: 0.0146
Training, Epoch: 0044, Batch: 162445, Sample Num: 162445, Cur Loss: 0.00000017, Cur Avg Loss: 0.00000990, Log Avg loss: 0.00000543, Global Avg Loss: 0.00095443, Time: 0.0285
Training, Epoch: 0044, Batch: 166445, Sample Num: 166445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001045, Log Avg loss: 0.00003252, Global Avg Loss: 0.00095394, Time: 0.0375
Training, Epoch: 0044, Batch: 170445, Sample Num: 170445, Cur Loss: 0.00000000, Cur Avg Loss: 0.00001057, Log Avg loss: 0.00001556, Global Avg Loss: 0.00095344, Time: 0.0172
***** Running evaluation checkpoint-7540940 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-7540940 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3613.432228, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.288941, "eval_total_loss": 6184.487928, "eval_acc": 0.966034, "eval_prec": 0.980325, "eval_recall": 0.971921, "eval_f1": 0.97547, "eval_top2_acc": 0.99444, "eval_top3_acc": 0.998458, "eval_top5_acc": 0.99972, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999346, "eval_pr_auc": 0.987488, "eval_mcc": 0.962268, "eval_sn": 0.971921, "eval_sp": 0.998692, "update_flag": false, "test_avg_loss": 0.291296, "test_total_loss": 6234.891602, "test_acc": 0.964493, "test_prec": 0.986606, "test_recall": 0.976431, "test_f1": 0.980647, "test_top2_acc": 0.993973, "test_top3_acc": 0.998365, "test_top5_acc": 0.99972, "test_top10_acc": 1.0, "test_roc_auc": 0.999301, "test_pr_auc": 0.993755, "test_mcc": 0.960559, "test_sn": 0.976431, "test_sp": 0.998628, "lr": 2.4238160978343872e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0009533194213508503, "train_cur_epoch_loss": 1.8024419053569047, "train_cur_epoch_avg_loss": 1.0516917497779297e-05, "train_cur_epoch_time": 3613.432228088379, "train_cur_epoch_avg_time": 0.021083713441015135, "epoch": 44, "step": 7540940}
##################################################
Training, Epoch: 0045, Batch: 003060, Sample Num: 3060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000074, Log Avg loss: 0.00000098, Global Avg Loss: 0.00095293, Time: 0.0154
Training, Epoch: 0045, Batch: 007060, Sample Num: 7060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000239, Log Avg loss: 0.00000365, Global Avg Loss: 0.00095243, Time: 0.0157
Training, Epoch: 0045, Batch: 011060, Sample Num: 11060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000166, Log Avg loss: 0.00000037, Global Avg Loss: 0.00095193, Time: 0.0136
Training, Epoch: 0045, Batch: 015060, Sample Num: 15060, Cur Loss: 0.00000002, Cur Avg Loss: 0.00000513, Log Avg loss: 0.00001474, Global Avg Loss: 0.00095143, Time: 0.0127
Training, Epoch: 0045, Batch: 019060, Sample Num: 19060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000429, Log Avg loss: 0.00000113, Global Avg Loss: 0.00095093, Time: 0.0126
Training, Epoch: 0045, Batch: 023060, Sample Num: 23060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000435, Log Avg loss: 0.00000464, Global Avg Loss: 0.00095043, Time: 0.0122
Training, Epoch: 0045, Batch: 027060, Sample Num: 27060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000402, Log Avg loss: 0.00000206, Global Avg Loss: 0.00094993, Time: 0.0292
Training, Epoch: 0045, Batch: 031060, Sample Num: 31060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000366, Log Avg loss: 0.00000124, Global Avg Loss: 0.00094942, Time: 0.0260
Training, Epoch: 0045, Batch: 035060, Sample Num: 35060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000342, Log Avg loss: 0.00000155, Global Avg Loss: 0.00094892, Time: 0.0192
Training, Epoch: 0045, Batch: 039060, Sample Num: 39060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000516, Log Avg loss: 0.00002040, Global Avg Loss: 0.00094843, Time: 0.0135
Training, Epoch: 0045, Batch: 043060, Sample Num: 43060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000666, Log Avg loss: 0.00002138, Global Avg Loss: 0.00094794, Time: 0.0124
Training, Epoch: 0045, Batch: 047060, Sample Num: 47060, Cur Loss: 0.00000025, Cur Avg Loss: 0.00001016, Log Avg loss: 0.00004780, Global Avg Loss: 0.00094747, Time: 0.0135
Training, Epoch: 0045, Batch: 051060, Sample Num: 51060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000949, Log Avg loss: 0.00000159, Global Avg Loss: 0.00094697, Time: 0.0462
Training, Epoch: 0045, Batch: 055060, Sample Num: 55060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000885, Log Avg loss: 0.00000073, Global Avg Loss: 0.00094647, Time: 0.0156
Training, Epoch: 0045, Batch: 059060, Sample Num: 59060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000841, Log Avg loss: 0.00000241, Global Avg Loss: 0.00094598, Time: 0.0138
Training, Epoch: 0045, Batch: 063060, Sample Num: 63060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000800, Log Avg loss: 0.00000190, Global Avg Loss: 0.00094548, Time: 0.0124
Training, Epoch: 0045, Batch: 067060, Sample Num: 67060, Cur Loss: 0.00000002, Cur Avg Loss: 0.00000755, Log Avg loss: 0.00000052, Global Avg Loss: 0.00094498, Time: 0.0134
Training, Epoch: 0045, Batch: 071060, Sample Num: 71060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000772, Log Avg loss: 0.00001054, Global Avg Loss: 0.00094449, Time: 0.0152
Training, Epoch: 0045, Batch: 075060, Sample Num: 75060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000740, Log Avg loss: 0.00000174, Global Avg Loss: 0.00094400, Time: 0.0131
Training, Epoch: 0045, Batch: 079060, Sample Num: 79060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000715, Log Avg loss: 0.00000230, Global Avg Loss: 0.00094350, Time: 0.0240
Training, Epoch: 0045, Batch: 083060, Sample Num: 83060, Cur Loss: 0.00000001, Cur Avg Loss: 0.00000699, Log Avg loss: 0.00000386, Global Avg Loss: 0.00094301, Time: 0.0168
Training, Epoch: 0045, Batch: 087060, Sample Num: 87060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000669, Log Avg loss: 0.00000060, Global Avg Loss: 0.00094252, Time: 0.0151
Training, Epoch: 0045, Batch: 091060, Sample Num: 91060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000645, Log Avg loss: 0.00000112, Global Avg Loss: 0.00094202, Time: 0.0319
Training, Epoch: 0045, Batch: 095060, Sample Num: 95060, Cur Loss: 0.00000001, Cur Avg Loss: 0.00000635, Log Avg loss: 0.00000405, Global Avg Loss: 0.00094153, Time: 0.0133
Training, Epoch: 0045, Batch: 099060, Sample Num: 99060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000613, Log Avg loss: 0.00000101, Global Avg Loss: 0.00094104, Time: 0.0452
Training, Epoch: 0045, Batch: 103060, Sample Num: 103060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000694, Log Avg loss: 0.00002703, Global Avg Loss: 0.00094056, Time: 0.0143
Training, Epoch: 0045, Batch: 107060, Sample Num: 107060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000670, Log Avg loss: 0.00000041, Global Avg Loss: 0.00094007, Time: 0.0127
Training, Epoch: 0045, Batch: 111060, Sample Num: 111060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000660, Log Avg loss: 0.00000383, Global Avg Loss: 0.00093958, Time: 0.0210
Training, Epoch: 0045, Batch: 115060, Sample Num: 115060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000655, Log Avg loss: 0.00000528, Global Avg Loss: 0.00093909, Time: 0.0383
Training, Epoch: 0045, Batch: 119060, Sample Num: 119060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000700, Log Avg loss: 0.00001993, Global Avg Loss: 0.00093861, Time: 0.0142
Training, Epoch: 0045, Batch: 123060, Sample Num: 123060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000716, Log Avg loss: 0.00001188, Global Avg Loss: 0.00093813, Time: 0.0319
Training, Epoch: 0045, Batch: 127060, Sample Num: 127060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000706, Log Avg loss: 0.00000406, Global Avg Loss: 0.00093764, Time: 0.0142
Training, Epoch: 0045, Batch: 131060, Sample Num: 131060, Cur Loss: 0.00000002, Cur Avg Loss: 0.00000727, Log Avg loss: 0.00001392, Global Avg Loss: 0.00093716, Time: 0.0124
Training, Epoch: 0045, Batch: 135060, Sample Num: 135060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000708, Log Avg loss: 0.00000066, Global Avg Loss: 0.00093667, Time: 0.0128
Training, Epoch: 0045, Batch: 139060, Sample Num: 139060, Cur Loss: 0.00000011, Cur Avg Loss: 0.00000701, Log Avg loss: 0.00000483, Global Avg Loss: 0.00093618, Time: 0.0123
Training, Epoch: 0045, Batch: 143060, Sample Num: 143060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000688, Log Avg loss: 0.00000225, Global Avg Loss: 0.00093570, Time: 0.0143
Training, Epoch: 0045, Batch: 147060, Sample Num: 147060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000745, Log Avg loss: 0.00002807, Global Avg Loss: 0.00093523, Time: 0.0234
Training, Epoch: 0045, Batch: 151060, Sample Num: 151060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000884, Log Avg loss: 0.00005995, Global Avg Loss: 0.00093477, Time: 0.0152
Training, Epoch: 0045, Batch: 155060, Sample Num: 155060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000864, Log Avg loss: 0.00000095, Global Avg Loss: 0.00093429, Time: 0.0181
Training, Epoch: 0045, Batch: 159060, Sample Num: 159060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000855, Log Avg loss: 0.00000487, Global Avg Loss: 0.00093380, Time: 0.0423
Training, Epoch: 0045, Batch: 163060, Sample Num: 163060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000871, Log Avg loss: 0.00001528, Global Avg Loss: 0.00093333, Time: 0.0130
Training, Epoch: 0045, Batch: 167060, Sample Num: 167060, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000854, Log Avg loss: 0.00000136, Global Avg Loss: 0.00093284, Time: 0.0350
Training, Epoch: 0045, Batch: 171060, Sample Num: 171060, Cur Loss: 0.00000270, Cur Avg Loss: 0.00000858, Log Avg loss: 0.00001051, Global Avg Loss: 0.00093236, Time: 0.0199
***** Running evaluation checkpoint-7712325 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-7712325 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3558.689066, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.299427, "eval_total_loss": 6408.928838, "eval_acc": 0.965147, "eval_prec": 0.980301, "eval_recall": 0.971506, "eval_f1": 0.975235, "eval_top2_acc": 0.9943, "eval_top3_acc": 0.998318, "eval_top5_acc": 0.99972, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999319, "eval_pr_auc": 0.986987, "eval_mcc": 0.961297, "eval_sn": 0.971506, "eval_sp": 0.998653, "update_flag": false, "test_avg_loss": 0.30082, "test_total_loss": 6438.75176, "test_acc": 0.964773, "test_prec": 0.986987, "test_recall": 0.976254, "test_f1": 0.980715, "test_top2_acc": 0.993926, "test_top3_acc": 0.998458, "test_top5_acc": 0.999673, "test_top10_acc": 1.0, "test_roc_auc": 0.999269, "test_pr_auc": 0.992909, "test_mcc": 0.960908, "test_sn": 0.976254, "test_sp": 0.998633, "lr": 2.0209376422394173e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0009323248824997678, "train_cur_epoch_loss": 1.4679421830166448, "train_cur_epoch_avg_loss": 8.565173049080403e-06, "train_cur_epoch_time": 3558.6890664100647, "train_cur_epoch_avg_time": 0.020764297146250048, "epoch": 45, "step": 7712325}
##################################################
Training, Epoch: 0046, Batch: 003675, Sample Num: 3675, Cur Loss: 0.00000113, Cur Avg Loss: 0.00000041, Log Avg loss: 0.00000038, Global Avg Loss: 0.00093188, Time: 0.0129
Training, Epoch: 0046, Batch: 007675, Sample Num: 7675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000510, Log Avg loss: 0.00000941, Global Avg Loss: 0.00093140, Time: 0.0193
Training, Epoch: 0046, Batch: 011675, Sample Num: 11675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000341, Log Avg loss: 0.00000016, Global Avg Loss: 0.00093092, Time: 0.0288
Training, Epoch: 0046, Batch: 015675, Sample Num: 15675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000262, Log Avg loss: 0.00000031, Global Avg Loss: 0.00093044, Time: 0.0252
Training, Epoch: 0046, Batch: 019675, Sample Num: 19675, Cur Loss: 0.00000001, Cur Avg Loss: 0.00000378, Log Avg loss: 0.00000832, Global Avg Loss: 0.00092996, Time: 0.0124
Training, Epoch: 0046, Batch: 023675, Sample Num: 23675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000322, Log Avg loss: 0.00000049, Global Avg Loss: 0.00092948, Time: 0.0141
Training, Epoch: 0046, Batch: 027675, Sample Num: 27675, Cur Loss: 0.00000003, Cur Avg Loss: 0.00000282, Log Avg loss: 0.00000048, Global Avg Loss: 0.00092900, Time: 0.0273
Training, Epoch: 0046, Batch: 031675, Sample Num: 31675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000258, Log Avg loss: 0.00000087, Global Avg Loss: 0.00092852, Time: 0.0343
Training, Epoch: 0046, Batch: 035675, Sample Num: 35675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000268, Log Avg loss: 0.00000352, Global Avg Loss: 0.00092804, Time: 0.0217
Training, Epoch: 0046, Batch: 039675, Sample Num: 39675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000458, Log Avg loss: 0.00002151, Global Avg Loss: 0.00092758, Time: 0.0138
Training, Epoch: 0046, Batch: 043675, Sample Num: 43675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000420, Log Avg loss: 0.00000044, Global Avg Loss: 0.00092710, Time: 0.0138
Training, Epoch: 0046, Batch: 047675, Sample Num: 47675, Cur Loss: 0.00000003, Cur Avg Loss: 0.00000661, Log Avg loss: 0.00003295, Global Avg Loss: 0.00092664, Time: 0.0835
Training, Epoch: 0046, Batch: 051675, Sample Num: 51675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000637, Log Avg loss: 0.00000349, Global Avg Loss: 0.00092616, Time: 0.0360
Training, Epoch: 0046, Batch: 055675, Sample Num: 55675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000594, Log Avg loss: 0.00000039, Global Avg Loss: 0.00092569, Time: 0.0364
Training, Epoch: 0046, Batch: 059675, Sample Num: 59675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000570, Log Avg loss: 0.00000230, Global Avg Loss: 0.00092521, Time: 0.0225
Training, Epoch: 0046, Batch: 063675, Sample Num: 63675, Cur Loss: 0.00000006, Cur Avg Loss: 0.00000541, Log Avg loss: 0.00000106, Global Avg Loss: 0.00092473, Time: 0.0433
Training, Epoch: 0046, Batch: 067675, Sample Num: 67675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000512, Log Avg loss: 0.00000057, Global Avg Loss: 0.00092426, Time: 0.0154
Training, Epoch: 0046, Batch: 071675, Sample Num: 71675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000559, Log Avg loss: 0.00001359, Global Avg Loss: 0.00092379, Time: 0.0126
Training, Epoch: 0046, Batch: 075675, Sample Num: 75675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000540, Log Avg loss: 0.00000198, Global Avg Loss: 0.00092332, Time: 0.0142
Training, Epoch: 0046, Batch: 079675, Sample Num: 79675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000520, Log Avg loss: 0.00000135, Global Avg Loss: 0.00092284, Time: 0.0150
Training, Epoch: 0046, Batch: 083675, Sample Num: 83675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000497, Log Avg loss: 0.00000048, Global Avg Loss: 0.00092237, Time: 0.0157
Training, Epoch: 0046, Batch: 087675, Sample Num: 87675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000486, Log Avg loss: 0.00000241, Global Avg Loss: 0.00092190, Time: 0.0448
Training, Epoch: 0046, Batch: 091675, Sample Num: 91675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000500, Log Avg loss: 0.00000813, Global Avg Loss: 0.00092143, Time: 0.0452
Training, Epoch: 0046, Batch: 095675, Sample Num: 95675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000482, Log Avg loss: 0.00000077, Global Avg Loss: 0.00092096, Time: 0.0139
Training, Epoch: 0046, Batch: 099675, Sample Num: 99675, Cur Loss: 0.00000004, Cur Avg Loss: 0.00000474, Log Avg loss: 0.00000288, Global Avg Loss: 0.00092049, Time: 0.0123
Training, Epoch: 0046, Batch: 103675, Sample Num: 103675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000458, Log Avg loss: 0.00000049, Global Avg Loss: 0.00092002, Time: 0.0405
Training, Epoch: 0046, Batch: 107675, Sample Num: 107675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000447, Log Avg loss: 0.00000168, Global Avg Loss: 0.00091955, Time: 0.0261
Training, Epoch: 0046, Batch: 111675, Sample Num: 111675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000468, Log Avg loss: 0.00001032, Global Avg Loss: 0.00091908, Time: 0.0325
Training, Epoch: 0046, Batch: 115675, Sample Num: 115675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000482, Log Avg loss: 0.00000860, Global Avg Loss: 0.00091862, Time: 0.0197
Training, Epoch: 0046, Batch: 119675, Sample Num: 119675, Cur Loss: 0.00000002, Cur Avg Loss: 0.00000467, Log Avg loss: 0.00000036, Global Avg Loss: 0.00091815, Time: 0.0176
Training, Epoch: 0046, Batch: 123675, Sample Num: 123675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000466, Log Avg loss: 0.00000449, Global Avg Loss: 0.00091768, Time: 0.0202
Training, Epoch: 0046, Batch: 127675, Sample Num: 127675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000456, Log Avg loss: 0.00000140, Global Avg Loss: 0.00091722, Time: 0.0153
Training, Epoch: 0046, Batch: 131675, Sample Num: 131675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000453, Log Avg loss: 0.00000361, Global Avg Loss: 0.00091675, Time: 0.0380
Training, Epoch: 0046, Batch: 135675, Sample Num: 135675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000443, Log Avg loss: 0.00000107, Global Avg Loss: 0.00091628, Time: 0.0289
Training, Epoch: 0046, Batch: 139675, Sample Num: 139675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000431, Log Avg loss: 0.00000037, Global Avg Loss: 0.00091582, Time: 0.0345
Training, Epoch: 0046, Batch: 143675, Sample Num: 143675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000430, Log Avg loss: 0.00000371, Global Avg Loss: 0.00091535, Time: 0.0127
Training, Epoch: 0046, Batch: 147675, Sample Num: 147675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000456, Log Avg loss: 0.00001419, Global Avg Loss: 0.00091489, Time: 0.0124
Training, Epoch: 0046, Batch: 151675, Sample Num: 151675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000446, Log Avg loss: 0.00000045, Global Avg Loss: 0.00091443, Time: 0.0613
Training, Epoch: 0046, Batch: 155675, Sample Num: 155675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000436, Log Avg loss: 0.00000058, Global Avg Loss: 0.00091396, Time: 0.0373
Training, Epoch: 0046, Batch: 159675, Sample Num: 159675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000426, Log Avg loss: 0.00000053, Global Avg Loss: 0.00091350, Time: 0.0194
Training, Epoch: 0046, Batch: 163675, Sample Num: 163675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000439, Log Avg loss: 0.00000969, Global Avg Loss: 0.00091304, Time: 0.0728
Training, Epoch: 0046, Batch: 167675, Sample Num: 167675, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000434, Log Avg loss: 0.00000207, Global Avg Loss: 0.00091258, Time: 0.0125
***** Running evaluation checkpoint-7883710 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-7883710 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3577.431065, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.299475, "eval_total_loss": 6409.958529, "eval_acc": 0.965707, "eval_prec": 0.986601, "eval_recall": 0.973281, "eval_f1": 0.977976, "eval_top2_acc": 0.994113, "eval_top3_acc": 0.998225, "eval_top5_acc": 0.999766, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999324, "eval_pr_auc": 0.986992, "eval_mcc": 0.961915, "eval_sn": 0.973281, "eval_sp": 0.998678, "update_flag": true, "test_avg_loss": 0.301891, "test_total_loss": 6461.672196, "test_acc": 0.964306, "test_prec": 0.986603, "test_recall": 0.976237, "test_f1": 0.980532, "test_top2_acc": 0.993833, "test_top3_acc": 0.998365, "test_top5_acc": 0.999626, "test_top10_acc": 1.0, "test_roc_auc": 0.999266, "test_pr_auc": 0.991335, "test_mcc": 0.960376, "test_sn": 0.976237, "test_sp": 0.998618, "lr": 1.6180591866444473e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0009121556589373077, "train_cur_epoch_loss": 0.7781904951048302, "train_cur_epoch_avg_loss": 4.540598623595007e-06, "train_cur_epoch_time": 3577.431064605713, "train_cur_epoch_avg_time": 0.020873653263737857, "epoch": 46, "step": 7883710}
##################################################
Training, Epoch: 0047, Batch: 000290, Sample Num: 290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000003, Log Avg loss: 0.00001272, Global Avg Loss: 0.00091212, Time: 0.0266
Training, Epoch: 0047, Batch: 004290, Sample Num: 4290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000073, Log Avg loss: 0.00000078, Global Avg Loss: 0.00091166, Time: 0.0134
Training, Epoch: 0047, Batch: 008290, Sample Num: 8290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000109, Log Avg loss: 0.00000147, Global Avg Loss: 0.00091120, Time: 0.0196
Training, Epoch: 0047, Batch: 012290, Sample Num: 12290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000249, Log Avg loss: 0.00000539, Global Avg Loss: 0.00091074, Time: 0.0276
Training, Epoch: 0047, Batch: 016290, Sample Num: 16290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000204, Log Avg loss: 0.00000066, Global Avg Loss: 0.00091028, Time: 0.0205
Training, Epoch: 0047, Batch: 020290, Sample Num: 20290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000192, Log Avg loss: 0.00000144, Global Avg Loss: 0.00090982, Time: 0.0191
Training, Epoch: 0047, Batch: 024290, Sample Num: 24290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000177, Log Avg loss: 0.00000100, Global Avg Loss: 0.00090936, Time: 0.0332
Training, Epoch: 0047, Batch: 028290, Sample Num: 28290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000157, Log Avg loss: 0.00000037, Global Avg Loss: 0.00090890, Time: 0.0363
Training, Epoch: 0047, Batch: 032290, Sample Num: 32290, Cur Loss: 0.00000424, Cur Avg Loss: 0.00000603, Log Avg loss: 0.00003757, Global Avg Loss: 0.00090846, Time: 0.0285
Training, Epoch: 0047, Batch: 036290, Sample Num: 36290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000539, Log Avg loss: 0.00000018, Global Avg Loss: 0.00090800, Time: 0.0151
Training, Epoch: 0047, Batch: 040290, Sample Num: 40290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000488, Log Avg loss: 0.00000035, Global Avg Loss: 0.00090754, Time: 0.0189
Training, Epoch: 0047, Batch: 044290, Sample Num: 44290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000451, Log Avg loss: 0.00000072, Global Avg Loss: 0.00090709, Time: 0.0148
Training, Epoch: 0047, Batch: 048290, Sample Num: 48290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000426, Log Avg loss: 0.00000149, Global Avg Loss: 0.00090663, Time: 0.0128
Training, Epoch: 0047, Batch: 052290, Sample Num: 52290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000419, Log Avg loss: 0.00000337, Global Avg Loss: 0.00090617, Time: 0.0155
Training, Epoch: 0047, Batch: 056290, Sample Num: 56290, Cur Loss: 0.00000004, Cur Avg Loss: 0.00000392, Log Avg loss: 0.00000035, Global Avg Loss: 0.00090572, Time: 0.0326
Training, Epoch: 0047, Batch: 060290, Sample Num: 60290, Cur Loss: 0.00000001, Cur Avg Loss: 0.00000368, Log Avg loss: 0.00000036, Global Avg Loss: 0.00090526, Time: 0.0320
Training, Epoch: 0047, Batch: 064290, Sample Num: 64290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000348, Log Avg loss: 0.00000040, Global Avg Loss: 0.00090481, Time: 0.0139
Training, Epoch: 0047, Batch: 068290, Sample Num: 68290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000332, Log Avg loss: 0.00000071, Global Avg Loss: 0.00090435, Time: 0.0145
Training, Epoch: 0047, Batch: 072290, Sample Num: 72290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000316, Log Avg loss: 0.00000057, Global Avg Loss: 0.00090390, Time: 0.0264
Training, Epoch: 0047, Batch: 076290, Sample Num: 76290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000397, Log Avg loss: 0.00001850, Global Avg Loss: 0.00090345, Time: 0.0129
Training, Epoch: 0047, Batch: 080290, Sample Num: 80290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000492, Log Avg loss: 0.00002308, Global Avg Loss: 0.00090301, Time: 0.0137
Training, Epoch: 0047, Batch: 084290, Sample Num: 84290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000481, Log Avg loss: 0.00000267, Global Avg Loss: 0.00090256, Time: 0.0135
Training, Epoch: 0047, Batch: 088290, Sample Num: 88290, Cur Loss: 0.00000021, Cur Avg Loss: 0.00000461, Log Avg loss: 0.00000026, Global Avg Loss: 0.00090210, Time: 0.0194
Training, Epoch: 0047, Batch: 092290, Sample Num: 92290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000442, Log Avg loss: 0.00000027, Global Avg Loss: 0.00090165, Time: 0.0220
Training, Epoch: 0047, Batch: 096290, Sample Num: 96290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000425, Log Avg loss: 0.00000047, Global Avg Loss: 0.00090120, Time: 0.0139
Training, Epoch: 0047, Batch: 100290, Sample Num: 100290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000427, Log Avg loss: 0.00000455, Global Avg Loss: 0.00090075, Time: 0.0182
Training, Epoch: 0047, Batch: 104290, Sample Num: 104290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000416, Log Avg loss: 0.00000137, Global Avg Loss: 0.00090030, Time: 0.0175
Training, Epoch: 0047, Batch: 108290, Sample Num: 108290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000401, Log Avg loss: 0.00000034, Global Avg Loss: 0.00089985, Time: 0.0258
Training, Epoch: 0047, Batch: 112290, Sample Num: 112290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000393, Log Avg loss: 0.00000151, Global Avg Loss: 0.00089940, Time: 0.0181
Training, Epoch: 0047, Batch: 116290, Sample Num: 116290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000392, Log Avg loss: 0.00000384, Global Avg Loss: 0.00089895, Time: 0.0251
Training, Epoch: 0047, Batch: 120290, Sample Num: 120290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000381, Log Avg loss: 0.00000051, Global Avg Loss: 0.00089850, Time: 0.0372
Training, Epoch: 0047, Batch: 124290, Sample Num: 124290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000402, Log Avg loss: 0.00001023, Global Avg Loss: 0.00089806, Time: 0.0332
Training, Epoch: 0047, Batch: 128290, Sample Num: 128290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000434, Log Avg loss: 0.00001435, Global Avg Loss: 0.00089762, Time: 0.0174
Training, Epoch: 0047, Batch: 132290, Sample Num: 132290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000421, Log Avg loss: 0.00000025, Global Avg Loss: 0.00089717, Time: 0.0348
Training, Epoch: 0047, Batch: 136290, Sample Num: 136290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000411, Log Avg loss: 0.00000059, Global Avg Loss: 0.00089672, Time: 0.0127
Training, Epoch: 0047, Batch: 140290, Sample Num: 140290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000408, Log Avg loss: 0.00000306, Global Avg Loss: 0.00089628, Time: 0.0124
Training, Epoch: 0047, Batch: 144290, Sample Num: 144290, Cur Loss: 0.00000594, Cur Avg Loss: 0.00000430, Log Avg loss: 0.00001216, Global Avg Loss: 0.00089584, Time: 0.0146
Training, Epoch: 0047, Batch: 148290, Sample Num: 148290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000420, Log Avg loss: 0.00000064, Global Avg Loss: 0.00089539, Time: 0.0126
Training, Epoch: 0047, Batch: 152290, Sample Num: 152290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000410, Log Avg loss: 0.00000031, Global Avg Loss: 0.00089495, Time: 0.0128
Training, Epoch: 0047, Batch: 156290, Sample Num: 156290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000485, Log Avg loss: 0.00003328, Global Avg Loss: 0.00089452, Time: 0.0162
Training, Epoch: 0047, Batch: 160290, Sample Num: 160290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000473, Log Avg loss: 0.00000026, Global Avg Loss: 0.00089407, Time: 0.0238
Training, Epoch: 0047, Batch: 164290, Sample Num: 164290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000465, Log Avg loss: 0.00000133, Global Avg Loss: 0.00089363, Time: 0.0269
Training, Epoch: 0047, Batch: 168290, Sample Num: 168290, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000512, Log Avg loss: 0.00002425, Global Avg Loss: 0.00089320, Time: 0.0348
***** Running evaluation checkpoint-8055095 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-8055095 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3791.393061, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.299311, "eval_total_loss": 6406.449995, "eval_acc": 0.965474, "eval_prec": 0.979741, "eval_recall": 0.972115, "eval_f1": 0.97529, "eval_top2_acc": 0.994067, "eval_top3_acc": 0.998318, "eval_top5_acc": 0.999766, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999338, "eval_pr_auc": 0.987037, "eval_mcc": 0.961649, "eval_sn": 0.972115, "eval_sp": 0.998673, "update_flag": false, "test_avg_loss": 0.301605, "test_total_loss": 6455.558754, "test_acc": 0.964446, "test_prec": 0.986312, "test_recall": 0.976562, "test_f1": 0.980573, "test_top2_acc": 0.993739, "test_top3_acc": 0.998458, "test_top5_acc": 0.999626, "test_top10_acc": 1.0, "test_roc_auc": 0.999286, "test_pr_auc": 0.992942, "test_mcc": 0.960517, "test_sn": 0.976562, "test_sp": 0.998627, "lr": 1.2151807310494777e-05, "cur_epoch_step": 171385, "train_global_avg_loss": 0.000892855131541151, "train_cur_epoch_loss": 0.862215880330667, "train_cur_epoch_avg_loss": 5.0308713150548e-06, "train_cur_epoch_time": 3791.3930611610413, "train_cur_epoch_avg_time": 0.02212208221933682, "epoch": 47, "step": 8055095}
##################################################
Training, Epoch: 0048, Batch: 000905, Sample Num: 905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000047, Log Avg loss: 0.00000042, Global Avg Loss: 0.00089275, Time: 0.0126
Training, Epoch: 0048, Batch: 004905, Sample Num: 4905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000253, Log Avg loss: 0.00000299, Global Avg Loss: 0.00089231, Time: 0.0233
Training, Epoch: 0048, Batch: 008905, Sample Num: 8905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000158, Log Avg loss: 0.00000043, Global Avg Loss: 0.00089187, Time: 0.0696
Training, Epoch: 0048, Batch: 012905, Sample Num: 12905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000144, Log Avg loss: 0.00000112, Global Avg Loss: 0.00089143, Time: 0.0414
Training, Epoch: 0048, Batch: 016905, Sample Num: 16905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000139, Log Avg loss: 0.00000122, Global Avg Loss: 0.00089099, Time: 0.0213
Training, Epoch: 0048, Batch: 020905, Sample Num: 20905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000124, Log Avg loss: 0.00000063, Global Avg Loss: 0.00089055, Time: 0.0314
Training, Epoch: 0048, Batch: 024905, Sample Num: 24905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000121, Log Avg loss: 0.00000104, Global Avg Loss: 0.00089011, Time: 0.0506
Training, Epoch: 0048, Batch: 028905, Sample Num: 28905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000109, Log Avg loss: 0.00000031, Global Avg Loss: 0.00088967, Time: 0.0228
Training, Epoch: 0048, Batch: 032905, Sample Num: 32905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000106, Log Avg loss: 0.00000089, Global Avg Loss: 0.00088923, Time: 0.0145
Training, Epoch: 0048, Batch: 036905, Sample Num: 36905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000099, Log Avg loss: 0.00000038, Global Avg Loss: 0.00088879, Time: 0.0485
Training, Epoch: 0048, Batch: 040905, Sample Num: 40905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000194, Log Avg loss: 0.00001076, Global Avg Loss: 0.00088835, Time: 0.0530
Training, Epoch: 0048, Batch: 044905, Sample Num: 44905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000181, Log Avg loss: 0.00000045, Global Avg Loss: 0.00088792, Time: 0.0392
Training, Epoch: 0048, Batch: 048905, Sample Num: 48905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000178, Log Avg loss: 0.00000144, Global Avg Loss: 0.00088748, Time: 0.0199
Training, Epoch: 0048, Batch: 052905, Sample Num: 52905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000173, Log Avg loss: 0.00000117, Global Avg Loss: 0.00088704, Time: 0.0153
Training, Epoch: 0048, Batch: 056905, Sample Num: 56905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000165, Log Avg loss: 0.00000052, Global Avg Loss: 0.00088660, Time: 0.0270
Training, Epoch: 0048, Batch: 060905, Sample Num: 60905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000157, Log Avg loss: 0.00000044, Global Avg Loss: 0.00088617, Time: 0.0421
Training, Epoch: 0048, Batch: 064905, Sample Num: 64905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000160, Log Avg loss: 0.00000211, Global Avg Loss: 0.00088573, Time: 0.0125
Training, Epoch: 0048, Batch: 068905, Sample Num: 68905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000154, Log Avg loss: 0.00000056, Global Avg Loss: 0.00088530, Time: 0.0300
Training, Epoch: 0048, Batch: 072905, Sample Num: 72905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000149, Log Avg loss: 0.00000058, Global Avg Loss: 0.00088486, Time: 0.0127
Training, Epoch: 0048, Batch: 076905, Sample Num: 76905, Cur Loss: 0.00000040, Cur Avg Loss: 0.00000143, Log Avg loss: 0.00000041, Global Avg Loss: 0.00088442, Time: 0.0130
Training, Epoch: 0048, Batch: 080905, Sample Num: 80905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000157, Log Avg loss: 0.00000427, Global Avg Loss: 0.00088399, Time: 0.0367
Training, Epoch: 0048, Batch: 084905, Sample Num: 84905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000189, Log Avg loss: 0.00000824, Global Avg Loss: 0.00088356, Time: 0.0141
Training, Epoch: 0048, Batch: 088905, Sample Num: 88905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000194, Log Avg loss: 0.00000299, Global Avg Loss: 0.00088313, Time: 0.0597
Training, Epoch: 0048, Batch: 092905, Sample Num: 92905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000196, Log Avg loss: 0.00000248, Global Avg Loss: 0.00088270, Time: 0.0125
Training, Epoch: 0048, Batch: 096905, Sample Num: 96905, Cur Loss: 0.00000013, Cur Avg Loss: 0.00000195, Log Avg loss: 0.00000180, Global Avg Loss: 0.00088226, Time: 0.0315
Training, Epoch: 0048, Batch: 100905, Sample Num: 100905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000192, Log Avg loss: 0.00000109, Global Avg Loss: 0.00088183, Time: 0.0154
Training, Epoch: 0048, Batch: 104905, Sample Num: 104905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000241, Log Avg loss: 0.00001478, Global Avg Loss: 0.00088141, Time: 0.0209
Training, Epoch: 0048, Batch: 108905, Sample Num: 108905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000236, Log Avg loss: 0.00000120, Global Avg Loss: 0.00088098, Time: 0.0128
Training, Epoch: 0048, Batch: 112905, Sample Num: 112905, Cur Loss: 0.00000001, Cur Avg Loss: 0.00000231, Log Avg loss: 0.00000092, Global Avg Loss: 0.00088055, Time: 0.0160
Training, Epoch: 0048, Batch: 116905, Sample Num: 116905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000224, Log Avg loss: 0.00000022, Global Avg Loss: 0.00088011, Time: 0.0130
Training, Epoch: 0048, Batch: 120905, Sample Num: 120905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000217, Log Avg loss: 0.00000013, Global Avg Loss: 0.00087968, Time: 0.0139
Training, Epoch: 0048, Batch: 124905, Sample Num: 124905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000211, Log Avg loss: 0.00000011, Global Avg Loss: 0.00087925, Time: 0.0212
Training, Epoch: 0048, Batch: 128905, Sample Num: 128905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000205, Log Avg loss: 0.00000026, Global Avg Loss: 0.00087882, Time: 0.0356
Training, Epoch: 0048, Batch: 132905, Sample Num: 132905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000199, Log Avg loss: 0.00000018, Global Avg Loss: 0.00087839, Time: 0.0248
Training, Epoch: 0048, Batch: 136905, Sample Num: 136905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000194, Log Avg loss: 0.00000016, Global Avg Loss: 0.00087797, Time: 0.0137
Training, Epoch: 0048, Batch: 140905, Sample Num: 140905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000190, Log Avg loss: 0.00000069, Global Avg Loss: 0.00087754, Time: 0.0134
Training, Epoch: 0048, Batch: 144905, Sample Num: 144905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000187, Log Avg loss: 0.00000077, Global Avg Loss: 0.00087711, Time: 0.0158
Training, Epoch: 0048, Batch: 148905, Sample Num: 148905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000184, Log Avg loss: 0.00000050, Global Avg Loss: 0.00087668, Time: 0.0206
Training, Epoch: 0048, Batch: 152905, Sample Num: 152905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000179, Log Avg loss: 0.00000023, Global Avg Loss: 0.00087626, Time: 0.0273
Training, Epoch: 0048, Batch: 156905, Sample Num: 156905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000205, Log Avg loss: 0.00001183, Global Avg Loss: 0.00087583, Time: 0.0125
Training, Epoch: 0048, Batch: 160905, Sample Num: 160905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000204, Log Avg loss: 0.00000169, Global Avg Loss: 0.00087541, Time: 0.0152
Training, Epoch: 0048, Batch: 164905, Sample Num: 164905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000244, Log Avg loss: 0.00001869, Global Avg Loss: 0.00087499, Time: 0.0422
Training, Epoch: 0048, Batch: 168905, Sample Num: 168905, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000246, Log Avg loss: 0.00000299, Global Avg Loss: 0.00087457, Time: 0.0574
***** Running evaluation checkpoint-8226480 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-8226480 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 4240.549813, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.303369, "eval_total_loss": 6493.299722, "eval_acc": 0.965707, "eval_prec": 0.986297, "eval_recall": 0.973589, "eval_f1": 0.977996, "eval_top2_acc": 0.994253, "eval_top3_acc": 0.998365, "eval_top5_acc": 0.999766, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999321, "eval_pr_auc": 0.98695, "eval_mcc": 0.961909, "eval_sn": 0.973589, "eval_sp": 0.998681, "update_flag": true, "test_avg_loss": 0.305244, "test_total_loss": 6533.43363, "test_acc": 0.964913, "test_prec": 0.986596, "test_recall": 0.976685, "test_f1": 0.980774, "test_top2_acc": 0.993739, "test_top3_acc": 0.998458, "test_top5_acc": 0.999673, "test_top10_acc": 1.0, "test_roc_auc": 0.999268, "test_pr_auc": 0.992506, "test_mcc": 0.961036, "test_sn": 0.976685, "test_sp": 0.998644, "lr": 8.12302275454508e-06, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0008743130468692116, "train_cur_epoch_loss": 0.4858880067139979, "train_cur_epoch_avg_loss": 2.8350672854333687e-06, "train_cur_epoch_time": 4240.549813270569, "train_cur_epoch_avg_time": 0.02474282937987904, "epoch": 48, "step": 8226480}
##################################################
Training, Epoch: 0049, Batch: 001520, Sample Num: 1520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000145, Log Avg loss: 0.00001825, Global Avg Loss: 0.00087415, Time: 0.0141
Steps: 8230000, Updated lr: 0.000008
Training, Epoch: 0049, Batch: 005520, Sample Num: 5520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000247, Log Avg loss: 0.00000285, Global Avg Loss: 0.00087373, Time: 0.0128
Steps: 8234000, Updated lr: 0.000008
Training, Epoch: 0049, Batch: 009520, Sample Num: 9520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000505, Log Avg loss: 0.00000860, Global Avg Loss: 0.00087331, Time: 0.0148
Steps: 8238000, Updated lr: 0.000008
Training, Epoch: 0049, Batch: 013520, Sample Num: 13520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000386, Log Avg loss: 0.00000103, Global Avg Loss: 0.00087288, Time: 0.0153
Steps: 8242000, Updated lr: 0.000008
Training, Epoch: 0049, Batch: 017520, Sample Num: 17520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000304, Log Avg loss: 0.00000029, Global Avg Loss: 0.00087246, Time: 0.0128
Steps: 8246000, Updated lr: 0.000008
Training, Epoch: 0049, Batch: 021520, Sample Num: 21520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000256, Log Avg loss: 0.00000042, Global Avg Loss: 0.00087204, Time: 0.0141
Steps: 8250000, Updated lr: 0.000008
Training, Epoch: 0049, Batch: 025520, Sample Num: 25520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000220, Log Avg loss: 0.00000029, Global Avg Loss: 0.00087162, Time: 0.0422
Steps: 8254000, Updated lr: 0.000007
Training, Epoch: 0049, Batch: 029520, Sample Num: 29520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000197, Log Avg loss: 0.00000053, Global Avg Loss: 0.00087119, Time: 0.0148
Steps: 8258000, Updated lr: 0.000007
Training, Epoch: 0049, Batch: 033520, Sample Num: 33520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000176, Log Avg loss: 0.00000018, Global Avg Loss: 0.00087077, Time: 0.0131
Steps: 8262000, Updated lr: 0.000007
Training, Epoch: 0049, Batch: 037520, Sample Num: 37520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000171, Log Avg loss: 0.00000126, Global Avg Loss: 0.00087035, Time: 0.0352
Steps: 8266000, Updated lr: 0.000007
Training, Epoch: 0049, Batch: 041520, Sample Num: 41520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000158, Log Avg loss: 0.00000038, Global Avg Loss: 0.00086993, Time: 0.0280
Steps: 8270000, Updated lr: 0.000007
Training, Epoch: 0049, Batch: 045520, Sample Num: 45520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000146, Log Avg loss: 0.00000025, Global Avg Loss: 0.00086951, Time: 0.0291
Steps: 8274000, Updated lr: 0.000007
Training, Epoch: 0049, Batch: 049520, Sample Num: 49520, Cur Loss: 0.00000058, Cur Avg Loss: 0.00000174, Log Avg loss: 0.00000485, Global Avg Loss: 0.00086909, Time: 0.0164
Steps: 8278000, Updated lr: 0.000007
Training, Epoch: 0049, Batch: 053520, Sample Num: 53520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000162, Log Avg loss: 0.00000018, Global Avg Loss: 0.00086867, Time: 0.0346
Steps: 8282000, Updated lr: 0.000007
Training, Epoch: 0049, Batch: 057520, Sample Num: 57520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000153, Log Avg loss: 0.00000037, Global Avg Loss: 0.00086825, Time: 0.0422
Steps: 8286000, Updated lr: 0.000007
Training, Epoch: 0049, Batch: 061520, Sample Num: 61520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000145, Log Avg loss: 0.00000029, Global Avg Loss: 0.00086783, Time: 0.0137
Steps: 8290000, Updated lr: 0.000007
Training, Epoch: 0049, Batch: 065520, Sample Num: 65520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000137, Log Avg loss: 0.00000018, Global Avg Loss: 0.00086742, Time: 0.0171
Steps: 8294000, Updated lr: 0.000007
Training, Epoch: 0049, Batch: 069520, Sample Num: 69520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000172, Log Avg loss: 0.00000735, Global Avg Loss: 0.00086700, Time: 0.0147
Steps: 8298000, Updated lr: 0.000006
Training, Epoch: 0049, Batch: 073520, Sample Num: 73520, Cur Loss: 0.00000006, Cur Avg Loss: 0.00000170, Log Avg loss: 0.00000142, Global Avg Loss: 0.00086658, Time: 0.0144
Steps: 8302000, Updated lr: 0.000006
Training, Epoch: 0049, Batch: 077520, Sample Num: 77520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000164, Log Avg loss: 0.00000045, Global Avg Loss: 0.00086617, Time: 0.0192
Steps: 8306000, Updated lr: 0.000006
Training, Epoch: 0049, Batch: 081520, Sample Num: 81520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000174, Log Avg loss: 0.00000363, Global Avg Loss: 0.00086575, Time: 0.0152
Steps: 8310000, Updated lr: 0.000006
Training, Epoch: 0049, Batch: 085520, Sample Num: 85520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000180, Log Avg loss: 0.00000302, Global Avg Loss: 0.00086534, Time: 0.0142
Steps: 8314000, Updated lr: 0.000006
Training, Epoch: 0049, Batch: 089520, Sample Num: 89520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000173, Log Avg loss: 0.00000024, Global Avg Loss: 0.00086492, Time: 0.0147
Steps: 8318000, Updated lr: 0.000006
Training, Epoch: 0049, Batch: 093520, Sample Num: 93520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000167, Log Avg loss: 0.00000049, Global Avg Loss: 0.00086450, Time: 0.0125
Steps: 8322000, Updated lr: 0.000006
Training, Epoch: 0049, Batch: 097520, Sample Num: 97520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000161, Log Avg loss: 0.00000019, Global Avg Loss: 0.00086409, Time: 0.0144
Steps: 8326000, Updated lr: 0.000006
Training, Epoch: 0049, Batch: 101520, Sample Num: 101520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000156, Log Avg loss: 0.00000023, Global Avg Loss: 0.00086367, Time: 0.0179
Steps: 8330000, Updated lr: 0.000006
Training, Epoch: 0049, Batch: 105520, Sample Num: 105520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000151, Log Avg loss: 0.00000029, Global Avg Loss: 0.00086326, Time: 0.0124
Steps: 8334000, Updated lr: 0.000006
Training, Epoch: 0049, Batch: 109520, Sample Num: 109520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000146, Log Avg loss: 0.00000014, Global Avg Loss: 0.00086285, Time: 0.0352
Steps: 8338000, Updated lr: 0.000006
Training, Epoch: 0049, Batch: 113520, Sample Num: 113520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000141, Log Avg loss: 0.00000011, Global Avg Loss: 0.00086243, Time: 0.0235
Steps: 8342000, Updated lr: 0.000005
Training, Epoch: 0049, Batch: 117520, Sample Num: 117520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000138, Log Avg loss: 0.00000033, Global Avg Loss: 0.00086202, Time: 0.0492
Steps: 8346000, Updated lr: 0.000005
Training, Epoch: 0049, Batch: 121520, Sample Num: 121520, Cur Loss: 0.00000181, Cur Avg Loss: 0.00000161, Log Avg loss: 0.00000842, Global Avg Loss: 0.00086161, Time: 0.0173
Steps: 8350000, Updated lr: 0.000005
Training, Epoch: 0049, Batch: 125520, Sample Num: 125520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000157, Log Avg loss: 0.00000035, Global Avg Loss: 0.00086120, Time: 0.0177
Steps: 8354000, Updated lr: 0.000005
Training, Epoch: 0049, Batch: 129520, Sample Num: 129520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000152, Log Avg loss: 0.00000014, Global Avg Loss: 0.00086078, Time: 0.0179
Steps: 8358000, Updated lr: 0.000005
Training, Epoch: 0049, Batch: 133520, Sample Num: 133520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000148, Log Avg loss: 0.00000013, Global Avg Loss: 0.00086037, Time: 0.0338
Steps: 8362000, Updated lr: 0.000005
Training, Epoch: 0049, Batch: 137520, Sample Num: 137520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000145, Log Avg loss: 0.00000045, Global Avg Loss: 0.00085996, Time: 0.0202
Steps: 8366000, Updated lr: 0.000005
Training, Epoch: 0049, Batch: 141520, Sample Num: 141520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000154, Log Avg loss: 0.00000445, Global Avg Loss: 0.00085955, Time: 0.0326
Steps: 8370000, Updated lr: 0.000005
Training, Epoch: 0049, Batch: 145520, Sample Num: 145520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000154, Log Avg loss: 0.00000163, Global Avg Loss: 0.00085914, Time: 0.0178
Steps: 8374000, Updated lr: 0.000005
Training, Epoch: 0049, Batch: 149520, Sample Num: 149520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000151, Log Avg loss: 0.00000060, Global Avg Loss: 0.00085873, Time: 0.0181
Steps: 8378000, Updated lr: 0.000005
Training, Epoch: 0049, Batch: 153520, Sample Num: 153520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000149, Log Avg loss: 0.00000047, Global Avg Loss: 0.00085832, Time: 0.0670
Steps: 8382000, Updated lr: 0.000004
Training, Epoch: 0049, Batch: 157520, Sample Num: 157520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000147, Log Avg loss: 0.00000072, Global Avg Loss: 0.00085791, Time: 0.0786
Steps: 8386000, Updated lr: 0.000004
Training, Epoch: 0049, Batch: 161520, Sample Num: 161520, Cur Loss: 0.00000002, Cur Avg Loss: 0.00000144, Log Avg loss: 0.00000037, Global Avg Loss: 0.00085750, Time: 0.0375
Steps: 8390000, Updated lr: 0.000004
Training, Epoch: 0049, Batch: 165520, Sample Num: 165520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000141, Log Avg loss: 0.00000019, Global Avg Loss: 0.00085710, Time: 0.0321
Steps: 8394000, Updated lr: 0.000004
Training, Epoch: 0049, Batch: 169520, Sample Num: 169520, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000153, Log Avg loss: 0.00000656, Global Avg Loss: 0.00085669, Time: 0.0197
***** Running evaluation checkpoint-8397865 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-8397865 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 4098.446500, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.305278, "eval_total_loss": 6534.172205, "eval_acc": 0.965427, "eval_prec": 0.986304, "eval_recall": 0.973516, "eval_f1": 0.977962, "eval_top2_acc": 0.994207, "eval_top3_acc": 0.998318, "eval_top5_acc": 0.999766, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.99932, "eval_pr_auc": 0.986911, "eval_mcc": 0.961596, "eval_sn": 0.973516, "eval_sp": 0.998671, "update_flag": false, "test_avg_loss": 0.306309, "test_total_loss": 6556.24319, "test_acc": 0.964679, "test_prec": 0.986516, "test_recall": 0.97661, "test_f1": 0.980698, "test_top2_acc": 0.99402, "test_top3_acc": 0.998412, "test_top5_acc": 0.999673, "test_top10_acc": 1.0, "test_roc_auc": 0.999272, "test_pr_auc": 0.99253, "test_mcc": 0.960768, "test_sn": 0.97661, "test_sp": 0.998637, "lr": 4.09423819859538e-06, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0008565009794213433, "train_cur_epoch_loss": 0.26080373916880184, "train_cur_epoch_avg_loss": 1.5217419212229883e-06, "train_cur_epoch_time": 4098.446499824524, "train_cur_epoch_avg_time": 0.0239136826433149, "epoch": 49, "step": 8397865}
##################################################
Training, Epoch: 0050, Batch: 002135, Sample Num: 2135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000095, Log Avg loss: 0.00000080, Global Avg Loss: 0.00085628, Time: 0.0164
Training, Epoch: 0050, Batch: 006135, Sample Num: 6135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000042, Log Avg loss: 0.00000014, Global Avg Loss: 0.00085588, Time: 0.0193
Training, Epoch: 0050, Batch: 010135, Sample Num: 10135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000029, Log Avg loss: 0.00000009, Global Avg Loss: 0.00085547, Time: 0.0142
Training, Epoch: 0050, Batch: 014135, Sample Num: 14135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000035, Log Avg loss: 0.00000052, Global Avg Loss: 0.00085506, Time: 0.0147
Training, Epoch: 0050, Batch: 018135, Sample Num: 18135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000032, Log Avg loss: 0.00000019, Global Avg Loss: 0.00085466, Time: 0.0148
Training, Epoch: 0050, Batch: 022135, Sample Num: 22135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000032, Log Avg loss: 0.00000032, Global Avg Loss: 0.00085425, Time: 0.0125
Training, Epoch: 0050, Batch: 026135, Sample Num: 26135, Cur Loss: 0.00000001, Cur Avg Loss: 0.00000033, Log Avg loss: 0.00000039, Global Avg Loss: 0.00085384, Time: 0.0272
Training, Epoch: 0050, Batch: 030135, Sample Num: 30135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000036, Log Avg loss: 0.00000054, Global Avg Loss: 0.00085344, Time: 0.0126
Training, Epoch: 0050, Batch: 034135, Sample Num: 34135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000164, Log Avg loss: 0.00001129, Global Avg Loss: 0.00085304, Time: 0.0190
Training, Epoch: 0050, Batch: 038135, Sample Num: 38135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000148, Log Avg loss: 0.00000017, Global Avg Loss: 0.00085264, Time: 0.0445
Training, Epoch: 0050, Batch: 042135, Sample Num: 42135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000241, Log Avg loss: 0.00001122, Global Avg Loss: 0.00085224, Time: 0.0420
Training, Epoch: 0050, Batch: 046135, Sample Num: 46135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000235, Log Avg loss: 0.00000169, Global Avg Loss: 0.00085183, Time: 0.0126
Training, Epoch: 0050, Batch: 050135, Sample Num: 50135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000217, Log Avg loss: 0.00000016, Global Avg Loss: 0.00085143, Time: 0.0139
Training, Epoch: 0050, Batch: 054135, Sample Num: 54135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000202, Log Avg loss: 0.00000014, Global Avg Loss: 0.00085103, Time: 0.0302
Training, Epoch: 0050, Batch: 058135, Sample Num: 58135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000201, Log Avg loss: 0.00000191, Global Avg Loss: 0.00085063, Time: 0.0126
Training, Epoch: 0050, Batch: 062135, Sample Num: 62135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000214, Log Avg loss: 0.00000403, Global Avg Loss: 0.00085023, Time: 0.0259
Training, Epoch: 0050, Batch: 066135, Sample Num: 66135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000237, Log Avg loss: 0.00000591, Global Avg Loss: 0.00084983, Time: 0.0440
Training, Epoch: 0050, Batch: 070135, Sample Num: 70135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000227, Log Avg loss: 0.00000062, Global Avg Loss: 0.00084943, Time: 0.0202
Training, Epoch: 0050, Batch: 074135, Sample Num: 74135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000216, Log Avg loss: 0.00000027, Global Avg Loss: 0.00084903, Time: 0.0343
Training, Epoch: 0050, Batch: 078135, Sample Num: 78135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000206, Log Avg loss: 0.00000023, Global Avg Loss: 0.00084862, Time: 0.0155
Training, Epoch: 0050, Batch: 082135, Sample Num: 82135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000212, Log Avg loss: 0.00000315, Global Avg Loss: 0.00084823, Time: 0.0276
Training, Epoch: 0050, Batch: 086135, Sample Num: 86135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000205, Log Avg loss: 0.00000057, Global Avg Loss: 0.00084783, Time: 0.0120
Training, Epoch: 0050, Batch: 090135, Sample Num: 90135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000197, Log Avg loss: 0.00000035, Global Avg Loss: 0.00084743, Time: 0.0170
Training, Epoch: 0050, Batch: 094135, Sample Num: 94135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000190, Log Avg loss: 0.00000039, Global Avg Loss: 0.00084703, Time: 0.0168
Training, Epoch: 0050, Batch: 098135, Sample Num: 98135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000185, Log Avg loss: 0.00000067, Global Avg Loss: 0.00084663, Time: 0.0147
Training, Epoch: 0050, Batch: 102135, Sample Num: 102135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000179, Log Avg loss: 0.00000023, Global Avg Loss: 0.00084623, Time: 0.0238
Training, Epoch: 0050, Batch: 106135, Sample Num: 106135, Cur Loss: 0.00000057, Cur Avg Loss: 0.00000173, Log Avg loss: 0.00000017, Global Avg Loss: 0.00084583, Time: 0.0148
Training, Epoch: 0050, Batch: 110135, Sample Num: 110135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000168, Log Avg loss: 0.00000031, Global Avg Loss: 0.00084544, Time: 0.0418
Training, Epoch: 0050, Batch: 114135, Sample Num: 114135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000163, Log Avg loss: 0.00000040, Global Avg Loss: 0.00084504, Time: 0.0139
Training, Epoch: 0050, Batch: 118135, Sample Num: 118135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000159, Log Avg loss: 0.00000040, Global Avg Loss: 0.00084464, Time: 0.0211
Training, Epoch: 0050, Batch: 122135, Sample Num: 122135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000175, Log Avg loss: 0.00000643, Global Avg Loss: 0.00084425, Time: 0.0398
Training, Epoch: 0050, Batch: 126135, Sample Num: 126135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000182, Log Avg loss: 0.00000395, Global Avg Loss: 0.00084385, Time: 0.0172
Training, Epoch: 0050, Batch: 130135, Sample Num: 130135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000178, Log Avg loss: 0.00000047, Global Avg Loss: 0.00084346, Time: 0.0455
Training, Epoch: 0050, Batch: 134135, Sample Num: 134135, Cur Loss: 0.00000005, Cur Avg Loss: 0.00000175, Log Avg loss: 0.00000103, Global Avg Loss: 0.00084306, Time: 0.0474
Training, Epoch: 0050, Batch: 138135, Sample Num: 138135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000171, Log Avg loss: 0.00000017, Global Avg Loss: 0.00084267, Time: 0.0149
Training, Epoch: 0050, Batch: 142135, Sample Num: 142135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000173, Log Avg loss: 0.00000244, Global Avg Loss: 0.00084227, Time: 0.0154
Training, Epoch: 0050, Batch: 146135, Sample Num: 146135, Cur Loss: 0.00000005, Cur Avg Loss: 0.00000170, Log Avg loss: 0.00000053, Global Avg Loss: 0.00084188, Time: 0.0413
Training, Epoch: 0050, Batch: 150135, Sample Num: 150135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000165, Log Avg loss: 0.00000009, Global Avg Loss: 0.00084149, Time: 0.0143
Training, Epoch: 0050, Batch: 154135, Sample Num: 154135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000161, Log Avg loss: 0.00000014, Global Avg Loss: 0.00084109, Time: 0.0142
Training, Epoch: 0050, Batch: 158135, Sample Num: 158135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000159, Log Avg loss: 0.00000075, Global Avg Loss: 0.00084070, Time: 0.0124
Training, Epoch: 0050, Batch: 162135, Sample Num: 162135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000179, Log Avg loss: 0.00000969, Global Avg Loss: 0.00084031, Time: 0.0267
Training, Epoch: 0050, Batch: 166135, Sample Num: 166135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000177, Log Avg loss: 0.00000076, Global Avg Loss: 0.00083992, Time: 0.0129
Training, Epoch: 0050, Batch: 170135, Sample Num: 170135, Cur Loss: 0.00000000, Cur Avg Loss: 0.00000173, Log Avg loss: 0.00000029, Global Avg Loss: 0.00083953, Time: 0.0458
***** Running evaluation checkpoint-8569250 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
***** Running testing checkpoint-8569250 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
Epoch Time: 3763.109892, Avg time per batch (s): 0.020000
{"eval_avg_loss": 0.30602, "eval_total_loss": 6550.049148, "eval_acc": 0.965661, "eval_prec": 0.986526, "eval_recall": 0.973385, "eval_f1": 0.977998, "eval_top2_acc": 0.994207, "eval_top3_acc": 0.998271, "eval_top5_acc": 0.999766, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999315, "eval_pr_auc": 0.986969, "eval_mcc": 0.961854, "eval_sn": 0.973385, "eval_sp": 0.998678, "update_flag": true, "test_avg_loss": 0.307944, "test_total_loss": 6591.228792, "test_acc": 0.964866, "test_prec": 0.986689, "test_recall": 0.976578, "test_f1": 0.980762, "test_top2_acc": 0.99402, "test_top3_acc": 0.998412, "test_top5_acc": 0.999673, "test_top10_acc": 1.0, "test_roc_auc": 0.999261, "test_pr_auc": 0.992485, "test_mcc": 0.960977, "test_sn": 0.976578, "test_sp": 0.998642, "lr": 6.545364264568138e-08, "cur_epoch_step": 171385, "train_global_avg_loss": 0.0008394053828738623, "train_cur_epoch_loss": 0.29497964317419934, "train_cur_epoch_avg_loss": 1.7211520446608475e-06, "train_cur_epoch_time": 3763.1098923683167, "train_cur_epoch_avg_time": 0.02195705512365911, "epoch": 50, "step": 8569250}
##################################################
#########################Best Metric#########################
{"epoch": 50, "global_step": 8569250, "eval_avg_loss": 0.30602, "eval_total_loss": 6550.049148, "eval_acc": 0.965661, "eval_prec": 0.986526, "eval_recall": 0.973385, "eval_f1": 0.977998, "eval_top2_acc": 0.994207, "eval_top3_acc": 0.998271, "eval_top5_acc": 0.999766, "eval_top10_acc": 0.999907, "eval_roc_auc": 0.999315, "eval_pr_auc": 0.986969, "eval_mcc": 0.961854, "eval_sn": 0.973385, "eval_sp": 0.998678, "update_flag": true, "test_avg_loss": 0.307944, "test_total_loss": 6591.228792, "test_acc": 0.964866, "test_prec": 0.986689, "test_recall": 0.976578, "test_f1": 0.980762, "test_top2_acc": 0.99402, "test_top3_acc": 0.998412, "test_top5_acc": 0.999673, "test_top10_acc": 1.0, "test_roc_auc": 0.999261, "test_pr_auc": 0.992485, "test_mcc": 0.960977, "test_sn": 0.976578, "test_sp": 0.998642}
##################################################
Total Time: 862148.105918, Avg time per epoch(50 epochs): 17242.960000
++++++++++++Validation+++++++++++++
best f1 global step: 8569250
checkpoint path: ../models/extra_p_31_class_v3/protein/multi_class/lucaprot/seq_matrix/20240923094428/checkpoint-8569250
***** Running evaluation checkpoint-8569250 *****
Dev Dataset Instantaneous batch size per GPU = 1
Dev Dataset Num examples = 21404
##################################################
{"evaluation_avg_loss_8569250": 0.30602, "evaluation_total_loss_8569250": 6550.049148, "evaluation_acc_8569250": 0.965661, "evaluation_prec_8569250": 0.986526, "evaluation_recall_8569250": 0.973385, "evaluation_f1_8569250": 0.977998, "evaluation_top2_acc_8569250": 0.994207, "evaluation_top3_acc_8569250": 0.998271, "evaluation_top5_acc_8569250": 0.999766, "evaluation_top10_acc_8569250": 0.999907, "evaluation_roc_auc_8569250": 0.999315, "evaluation_pr_auc_8569250": 0.986969, "evaluation_mcc_8569250": 0.961854, "evaluation_sn_8569250": 0.973385, "evaluation_sp_8569250": 0.998678}
++++++++++++Testing+++++++++++++
best f1 global step: 8569250
checkpoint path: ../models/extra_p_31_class_v3/protein/multi_class/lucaprot/seq_matrix/20240923094428/checkpoint-8569250
***** Running testing checkpoint-8569250 *****
Test Dataset Instantaneous batch size per GPU = 1
Test Dataset Num examples = [21404]
##################################################
{"evaluation_avg_loss_8569250": 0.307944, "evaluation_total_loss_8569250": 6591.228792, "evaluation_acc_8569250": 0.964866, "evaluation_prec_8569250": 0.986689, "evaluation_recall_8569250": 0.976578, "evaluation_f1_8569250": 0.980762, "evaluation_top2_acc_8569250": 0.99402, "evaluation_top3_acc_8569250": 0.998412, "evaluation_top5_acc_8569250": 0.999673, "evaluation_top10_acc_8569250": 1.0, "evaluation_roc_auc_8569250": 0.999261, "evaluation_pr_auc_8569250": 0.992485, "evaluation_mcc_8569250": 0.960977, "evaluation_sn_8569250": 0.976578, "evaluation_sp_8569250": 0.998642}
