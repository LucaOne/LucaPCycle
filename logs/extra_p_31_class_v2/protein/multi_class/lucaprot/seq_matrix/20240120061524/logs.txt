{"adam_epsilon": 1e-08, "alphabet": null, "append_eos": false, "asl_gamma_neg": 4.0, "asl_gamma_pos": 1.0, "atom_embedding_input_size": null, "atom_matrix_max_length": null, "atom_seq_max_length": null, "best_metric_type": "f1", "beta1": 0.9, "beta2": 0.98, "buffer_size": 10240, "cache_dir": null, "classifier_activate_func": "gelu", "classifier_size": 1024, "codes_file": "../subword/extra_p_31_class_v2/extra_p_31_class_codes_20000.txt", "config_path": "../config/lucaprot/lucaprot_config.json", "cross_atten": false, "dataset_name": "extra_p_31_class_v2", "dataset_type": "protein", "delete_old": true, "dev_data_dir": "../dataset/extra_p_31_class_v2/protein/multi_class/dev/", "device": "cuda", "do_eval": true, "do_lower_case": false, "do_metrics": true, "do_predict": true, "do_train": true, "dropout_prob": 0.1, "emb_activate_func": "gelu", "embedding_input_size": 2560, "embedding_input_size_a": null, "embedding_input_size_b": null, "eval_all_checkpoints": false, "evaluate_during_training": true, "fc_activate_func": "gelu", "focal_loss_alpha": 0.7, "focal_loss_gamma": 2.0, "focal_loss_reduce": false, "fp16": false, "fp16_opt_level": "O1", "fusion_type": "concat", "gradient_accumulation_steps": 2, "hidden_size": 2560, "ignore_index": -100, "input_mode": "single", "input_type": "seq_matrix", "kernel_size": null, "label_filepath": "../dataset/extra_p_31_class_v2/protein/multi_class/label.txt", "label_size": 31, "label_type": "extra_p_31_class_v2", "learning_rate": 0.0001, "llm_dir": null, "llm_dirpath": null, "llm_step": null, "llm_task_level": "token_level,span_level,seq_level,structure_level", "llm_time_str": null, "llm_type": "esm", "llm_version": "esm2", "local_rank": -1, "log_dir": "../logs/extra_p_31_class_v2/protein/multi_class/lucaprot/seq_matrix/20240120061524", "logging_steps": 1000, "loss_reduction": "mean", "loss_type": "cce", "lr_decay_rate": 0.95, "lr_update_strategy": "step", "matrix_dirpath": "../matrices/extra_p_31_class_v2/protein/multi_class/lucaprot/esm2/esm/3B", "matrix_encoder": false, "matrix_encoder_act": false, "matrix_fc_size": "null", "matrix_max_length": 3072, "matrix_max_length_a": null, "matrix_max_length_b": null, "matrix_pooling_type": "value_attention", "max_grad_norm": 1.0, "max_sentence_length": null, "max_sentences": null, "max_steps": -1, "model_dirpath": null, "model_type": "lucaprot", "molecule": false, "n_gpu": 1, "no_cuda": false, "no_position_embeddings": true, "no_token_embeddings": false, "no_token_type_embeddings": true, "non_ignore": false, "not_append_eos": true, "not_matrix_encoder_shared": false, "not_prepend_bos": true, "not_seq_encoder_shared": false, "num_attention_heads": 8, "num_hidden_layers": 4, "num_train_epochs": 50, "output_dir": "../models/extra_p_31_class_v2/protein/multi_class/lucaprot/seq_matrix/20240120061524", "output_mode": "multi_class", "overwrite_cache": false, "overwrite_output_dir": true, "per_gpu_eval_batch_size": 32, "per_gpu_train_batch_size": 32, "pos_weight": 1.0, "position_embedding_type": "absolute", "prepend_bos": false, "save_all": false, "save_steps": 1000000000, "seed": 1221, "self_atten": false, "seq_fc_size": "null", "seq_max_length": 3072, "seq_max_length_a": null, "seq_max_length_b": null, "seq_pooling_type": "value_attention", "seq_subword": true, "seq_vocab_path": "../vocab/extra_p_31_class_v2/extra_p_31_class_subword_vocab_20000.txt", "sigmoid": false, "task_level_type": "seq_level", "task_type": "multi_class", "tb_log_dir": "../tb-logs/extra_p_31_class_v2/protein/multi_class/lucaprot/seq_matrix/20240120061524", "test_data_dir": "../dataset/extra_p_31_class_v2/protein/multi_class/test/", "train_data_dir": "../dataset/extra_p_31_class_v2/protein/multi_class/train/", "trunc_type": "right", "vector_dirpath": "../vectors/extra_p_31_class_v2/protein/multi_class/lucaprot/esm2/esm/3B", "vector_fc_size": "null", "vocab_size": 19978, "warmup_steps": 2000, "weight": "20,14,30,2,600,20,1,13,10,18,6,1,8,32,4,4,11,64,7,3,4,180,30,12,3,110,27,28,369,37,215", "weight_decay": 0.01, "worker_num": 1}
##################################################
n_gpu: 1
##################################################
Inputs:
Input Name List: protein,seq,embedding_matrix
##################################################
Encoder Config:
 {'llm_type': 'esm', 'llm_version': 'esm2', 'llm_step': None, 'llm_dirpath': None, 'input_type': 'seq_matrix', 'trunc_type': 'right', 'seq_max_length': 3072, 'atom_seq_max_length': None, 'vector_dirpath': '../vectors/extra_p_31_class_v2/protein/multi_class/lucaprot/esm2/esm/3B', 'matrix_dirpath': '../matrices/extra_p_31_class_v2/protein/multi_class/lucaprot/esm2/esm/3B', 'local_rank': -1}
##################################################
Model Config:
 LucaConfig {
  "activate_func": "tanh",
  "alphabet": null,
  "attention_probs_dropout_prob": 0.1,
  "classifier_activate_func": "gelu",
  "classifier_dropout_prob": 0.1,
  "classifier_size": 1024,
  "cls_token_id": 2,
  "cross_atten": false,
  "directionality": "bidi",
  "emb_activate_func": "gelu",
  "embedding_fc_size": [
    1280
  ],
  "embedding_input_size": 2560,
  "embedding_pooling_type": "value_attention",
  "embedding_weight": null,
  "fc_activate_func": "gelu",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 2560,
  "id2label": {},
  "ignore_index": -100,
  "initializer_range": 0.02,
  "intermediate_size": 5120,
  "kernel_size": 7,
  "label2id": {},
  "layer_norm_eps": 1e-12,
  "loss_reduction": "mean",
  "matrix_fc_size": 1024,
  "matrix_max_length": 3072,
  "matrix_pooling_type": "value_attention",
  "max_position_embeddings": 3072,
  "no_position_embeddings": true,
  "no_token_embeddings": false,
  "no_token_type_embeddings": true,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "pooler_fc_size": 1280,
  "pooler_num_attention_heads": 4,
  "pooler_num_fc_layers": 1,
  "pooler_size_per_head": 1280,
  "pooler_type": "first_token_transform",
  "pos_weight": 1.0,
  "position_embedding_type": "absolute",
  "self_atten": false,
  "sep_token_id": 3,
  "seq_fc_size": [
    1280
  ],
  "seq_max_length": 3072,
  "seq_pooling_type": "value_attention",
  "seq_weight": null,
  "struct_alpha": 0.2,
  "struct_embed_size": 1280,
  "struct_fc_size": null,
  "struct_hidden_size": null,
  "struct_nb_heads": 4,
  "struct_output_size": null,
  "struct_vocab_size": null,
  "struct_weight": null,
  "token_dropout": true,
  "transformers_version": "4.29.0",
  "type_vocab_size": 2,
  "use_luca_layer_norm_v2": true,
  "vector_fc_size": 1024,
  "vocab_size": 19978,
  "weight": [
    20.0,
    14.0,
    30.0,
    2.0,
    600.0,
    20.0,
    1.0,
    13.0,
    10.0,
    18.0,
    6.0,
    1.0,
    8.0,
    32.0,
    4.0,
    4.0,
    11.0,
    64.0,
    7.0,
    3.0,
    4.0,
    180.0,
    30.0,
    12.0,
    3.0,
    110.0,
    27.0,
    28.0,
    369.0,
    37.0,
    215.0
  ]
}

##################################################
Mode Architecture:
 LucaProt(
  (seq_encoder): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(19978, 2560, padding_idx=0)
      (LayerNorm): LayerNorm((2560,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=2560, out_features=2560, bias=True)
              (key): Linear(in_features=2560, out_features=2560, bias=True)
              (value): Linear(in_features=2560, out_features=2560, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=2560, out_features=2560, bias=True)
              (LayerNorm): LayerNorm((2560,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=2560, out_features=5120, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=5120, out_features=2560, bias=True)
            (LayerNorm): LayerNorm((2560,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=2560, out_features=2560, bias=True)
              (key): Linear(in_features=2560, out_features=2560, bias=True)
              (value): Linear(in_features=2560, out_features=2560, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=2560, out_features=2560, bias=True)
              (LayerNorm): LayerNorm((2560,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=2560, out_features=5120, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=5120, out_features=2560, bias=True)
            (LayerNorm): LayerNorm((2560,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=2560, out_features=2560, bias=True)
              (key): Linear(in_features=2560, out_features=2560, bias=True)
              (value): Linear(in_features=2560, out_features=2560, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=2560, out_features=2560, bias=True)
              (LayerNorm): LayerNorm((2560,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=2560, out_features=5120, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=5120, out_features=2560, bias=True)
            (LayerNorm): LayerNorm((2560,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=2560, out_features=2560, bias=True)
              (key): Linear(in_features=2560, out_features=2560, bias=True)
              (value): Linear(in_features=2560, out_features=2560, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=2560, out_features=2560, bias=True)
              (LayerNorm): LayerNorm((2560,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=2560, out_features=5120, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=5120, out_features=2560, bias=True)
            (LayerNorm): LayerNorm((2560,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=2560, out_features=2560, bias=True)
      (activation): Tanh()
    )
  )
  (seq_pooler): GlobalMaskValueAttentionPooling1D (2560 -> 2560)
  (seq_linear): ModuleList(
    (0): Linear(in_features=2560, out_features=1280, bias=True)
    (1): Tanh()
  )
  (embedding_pooler): GlobalMaskValueAttentionPooling1D (2560 -> 2560)
  (embedding_linear): ModuleList(
    (0): Linear(in_features=2560, out_features=1280, bias=True)
    (1): Tanh()
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=2560, out_features=31, bias=True)
  (output): Softmax(dim=-1)
  (loss_fct): CrossEntropyLoss()
)
##################################################
Model parameters: 313489951 
##################################################
{"total_num": "298.970000M", "total_size": "1195.870000MB", "param_sum": "298.970000M", "param_size": "1195.870000MB", "buffer_sum": "0.000000M", "buffer_size": "0.000000MB", "trainable_num": "298.970000M", "trainable_size": "1195.870000MB"}
##################################################
